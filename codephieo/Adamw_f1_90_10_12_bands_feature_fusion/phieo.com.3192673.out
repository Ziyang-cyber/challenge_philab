2.2.0
test
DataLoaders created successfully!
FloodPredictorHSL(
  (foundation_spectral): FloodPredictorSpectral(
    (foundation): Foundation(
      (activation): LeakyReLU(negative_slope=0.01)
      (stem): CNNBlock(
        (activation): LeakyReLU(negative_slope=0.01)
        (activation_out): LeakyReLU(negative_slope=0.01)
        (squeeze): SE_Block(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Linear(in_features=32, out_features=2, bias=False)
            (1): GELU(approximate='none')
            (2): Linear(in_features=2, out_features=32, bias=False)
            (3): Sigmoid()
          )
        )
        (matcher): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (conv1): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (scaler): ScaleSkip2D()
      )
      (encoder): FoundationEncoder(
        (activation): LeakyReLU(negative_slope=0.01)
        (downsample): ModuleList(
          (0-1): 2 x Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          )
          (2): Sequential(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          )
          (3): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          )
        )
        (block_scalers): ModuleList(
          (0-4): 5 x ScaleSkip2D()
        )
        (blocks_down): ModuleList(
          (0): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (1): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (2): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (3): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (4): ModuleList(
            (0-4): 5 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
        )
        (prelinear_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
        (linear_encode): Sequential(
          (0): LeakyReLU(negative_slope=0.01)
          (1): Linear(in_features=4096, out_features=1024, bias=True)
          (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (head_clouds): Sequential(
          (0): Linear(in_features=1024, out_features=4, bias=True)
        )
        (head_landcover): Sequential(
          (0): Linear(in_features=1024, out_features=11, bias=True)
        )
        (head_buildings): Sequential(
          (0): Linear(in_features=1024, out_features=1, bias=True)
          (1): Sigmoid()
        )
        (head_water): Sequential(
          (0): Linear(in_features=1024, out_features=1, bias=True)
          (1): Sigmoid()
        )
        (head_coords): Sequential(
          (0): Linear(in_features=1024, out_features=4, bias=True)
          (1): Sigmoid()
        )
      )
      (decoder): FoundationDecoder(
        (activation): LeakyReLU(negative_slope=0.01)
        (linear_decode): Linear(in_features=1024, out_features=4096, bias=True)
        (latent_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
        (prehead_norm): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (skip_scalers): ModuleList(
          (0-4): 5 x ScaleSkip2D()
        )
        (block_scalers): ModuleList(
          (0-4): 5 x ScaleSkip2D()
        )
        (blocks_up): ModuleList(
          (0): ModuleList(
            (0-4): 5 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (1): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (2): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (3): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (4): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
        )
        (upsamplers): ModuleList(
          (0): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
          (1): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
          (2): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
          (3): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (head): CNNBlock(
        (activation): LeakyReLU(negative_slope=0.01)
        (activation_out): Sigmoid()
        (squeeze): SE_Block(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Linear(in_features=10, out_features=1, bias=False)
            (1): GELU(approximate='none')
            (2): Linear(in_features=1, out_features=10, bias=False)
            (3): Sigmoid()
          )
        )
        (matcher): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): LayerNorm((10, 128, 128), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((10, 128, 128), eps=1e-05, elementwise_affine=True)
        (conv1): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (conv3): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))
        (scaler): ScaleSkip2D()
      )
    )
    (encoder): FoundationEncoder(
      (activation): LeakyReLU(negative_slope=0.01)
      (downsample): ModuleList(
        (0-1): 2 x Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (2): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (3): Sequential(
          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (block_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (blocks_down): ModuleList(
        (0): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (1): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (2): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (3): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (4): ModuleList(
          (0-4): 5 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
      )
      (prelinear_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
      (linear_encode): Sequential(
        (0): LeakyReLU(negative_slope=0.01)
        (1): Linear(in_features=4096, out_features=1024, bias=True)
        (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (head_clouds): Sequential(
        (0): Linear(in_features=1024, out_features=4, bias=True)
      )
      (head_landcover): Sequential(
        (0): Linear(in_features=1024, out_features=11, bias=True)
      )
      (head_buildings): Sequential(
        (0): Linear(in_features=1024, out_features=1, bias=True)
        (1): Sigmoid()
      )
      (head_water): Sequential(
        (0): Linear(in_features=1024, out_features=1, bias=True)
        (1): Sigmoid()
      )
      (head_coords): Sequential(
        (0): Linear(in_features=1024, out_features=4, bias=True)
        (1): Sigmoid()
      )
    )
    (decoder): FoundationDecoder(
      (activation): LeakyReLU(negative_slope=0.01)
      (linear_decode): Linear(in_features=1024, out_features=4096, bias=True)
      (latent_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
      (prehead_norm): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (skip_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (block_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (blocks_up): ModuleList(
        (0): ModuleList(
          (0-4): 5 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (1): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (2): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (3): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (4): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
      )
      (upsamplers): ModuleList(
        (0): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (1): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (2): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (3): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (stem): CNNBlock(
      (activation): LeakyReLU(negative_slope=0.01)
      (activation_out): LeakyReLU(negative_slope=0.01)
      (squeeze): SE_Block(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Linear(in_features=32, out_features=2, bias=False)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2, out_features=32, bias=False)
          (3): Sigmoid()
        )
      )
      (matcher): Conv2d(7, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(7, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (foundation_static): FloodPredictorStatic(
    (foundation): Foundation(
      (activation): LeakyReLU(negative_slope=0.01)
      (stem): CNNBlock(
        (activation): LeakyReLU(negative_slope=0.01)
        (activation_out): LeakyReLU(negative_slope=0.01)
        (squeeze): SE_Block(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Linear(in_features=32, out_features=2, bias=False)
            (1): GELU(approximate='none')
            (2): Linear(in_features=2, out_features=32, bias=False)
            (3): Sigmoid()
          )
        )
        (matcher): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (conv1): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (scaler): ScaleSkip2D()
      )
      (encoder): FoundationEncoder(
        (activation): LeakyReLU(negative_slope=0.01)
        (downsample): ModuleList(
          (0-1): 2 x Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          )
          (2): Sequential(
            (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          )
          (3): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          )
        )
        (block_scalers): ModuleList(
          (0-4): 5 x ScaleSkip2D()
        )
        (blocks_down): ModuleList(
          (0): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (1): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (2): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (3): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (4): ModuleList(
            (0-4): 5 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
        )
        (prelinear_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
        (linear_encode): Sequential(
          (0): LeakyReLU(negative_slope=0.01)
          (1): Linear(in_features=4096, out_features=1024, bias=True)
          (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (head_clouds): Sequential(
          (0): Linear(in_features=1024, out_features=4, bias=True)
        )
        (head_landcover): Sequential(
          (0): Linear(in_features=1024, out_features=11, bias=True)
        )
        (head_buildings): Sequential(
          (0): Linear(in_features=1024, out_features=1, bias=True)
          (1): Sigmoid()
        )
        (head_water): Sequential(
          (0): Linear(in_features=1024, out_features=1, bias=True)
          (1): Sigmoid()
        )
        (head_coords): Sequential(
          (0): Linear(in_features=1024, out_features=4, bias=True)
          (1): Sigmoid()
        )
      )
      (decoder): FoundationDecoder(
        (activation): LeakyReLU(negative_slope=0.01)
        (linear_decode): Linear(in_features=1024, out_features=4096, bias=True)
        (latent_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
        (prehead_norm): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (skip_scalers): ModuleList(
          (0-4): 5 x ScaleSkip2D()
        )
        (block_scalers): ModuleList(
          (0-4): 5 x ScaleSkip2D()
        )
        (blocks_up): ModuleList(
          (0): ModuleList(
            (0-4): 5 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (1): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=64, out_features=4, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=4, out_features=64, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (2): ModuleList(
            (0-3): 4 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (3): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
          (4): ModuleList(
            (0-2): 3 x CNNBlock(
              (activation): LeakyReLU(negative_slope=0.01)
              (activation_out): LeakyReLU(negative_slope=0.01)
              (squeeze): SE_Block(
                (squeeze): AdaptiveAvgPool2d(output_size=1)
                (excitation): Sequential(
                  (0): Linear(in_features=32, out_features=2, bias=False)
                  (1): GELU(approximate='none')
                  (2): Linear(in_features=2, out_features=32, bias=False)
                  (3): Sigmoid()
                )
              )
              (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
              (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
              (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
              (scaler): ScaleSkip2D()
            )
          )
        )
        (upsamplers): ModuleList(
          (0): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
          (1): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
          (2): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
          (3): Sequential(
            (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
            (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (3): LeakyReLU(negative_slope=0.01)
          )
        )
      )
      (head): CNNBlock(
        (activation): LeakyReLU(negative_slope=0.01)
        (activation_out): Sigmoid()
        (squeeze): SE_Block(
          (squeeze): AdaptiveAvgPool2d(output_size=1)
          (excitation): Sequential(
            (0): Linear(in_features=10, out_features=1, bias=False)
            (1): GELU(approximate='none')
            (2): Linear(in_features=1, out_features=10, bias=False)
            (3): Sigmoid()
          )
        )
        (matcher): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm1): LayerNorm((10, 128, 128), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((10, 128, 128), eps=1e-05, elementwise_affine=True)
        (conv1): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (conv3): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))
        (scaler): ScaleSkip2D()
      )
    )
    (encoder): FoundationEncoder(
      (activation): LeakyReLU(negative_slope=0.01)
      (downsample): ModuleList(
        (0-1): 2 x Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (2): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (3): Sequential(
          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (block_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (blocks_down): ModuleList(
        (0): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (1): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (2): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (3): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (4): ModuleList(
          (0-4): 5 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
      )
      (prelinear_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
      (linear_encode): Sequential(
        (0): LeakyReLU(negative_slope=0.01)
        (1): Linear(in_features=4096, out_features=1024, bias=True)
        (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (head_clouds): Sequential(
        (0): Linear(in_features=1024, out_features=4, bias=True)
      )
      (head_landcover): Sequential(
        (0): Linear(in_features=1024, out_features=11, bias=True)
      )
      (head_buildings): Sequential(
        (0): Linear(in_features=1024, out_features=1, bias=True)
        (1): Sigmoid()
      )
      (head_water): Sequential(
        (0): Linear(in_features=1024, out_features=1, bias=True)
        (1): Sigmoid()
      )
      (head_coords): Sequential(
        (0): Linear(in_features=1024, out_features=4, bias=True)
        (1): Sigmoid()
      )
    )
    (decoder): FoundationDecoder(
      (activation): LeakyReLU(negative_slope=0.01)
      (linear_decode): Linear(in_features=1024, out_features=4096, bias=True)
      (latent_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
      (prehead_norm): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (skip_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (block_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (blocks_up): ModuleList(
        (0): ModuleList(
          (0-4): 5 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (1): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (2): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (3): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (4): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
      )
      (upsamplers): ModuleList(
        (0): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (1): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (2): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (3): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (stem): CNNBlock(
      (activation): LeakyReLU(negative_slope=0.01)
      (activation_out): LeakyReLU(negative_slope=0.01)
      (squeeze): SE_Block(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Linear(in_features=32, out_features=2, bias=False)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2, out_features=32, bias=False)
          (3): Sigmoid()
        )
      )
      (matcher): Conv2d(5, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(5, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (head): Sequential(
    (0): Dropout2d(p=0.2, inplace=False)
    (1): CNNBlock(
      (activation): LeakyReLU(negative_slope=0.01)
      (activation_out): LeakyReLU(negative_slope=0.01)
      (squeeze): SE_Block(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Linear(in_features=32, out_features=2, bias=False)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2, out_features=32, bias=False)
          (3): Sigmoid()
        )
      )
      (matcher): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    )
    (2): CNNBlock(
      (activation): LeakyReLU(negative_slope=0.01)
      (activation_out): Sigmoid()
      (squeeze): SE_Block(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Linear(in_features=2, out_features=1, bias=False)
          (1): GELU(approximate='none')
          (2): Linear(in_features=1, out_features=2, bias=False)
          (3): Sigmoid()
        )
      )
      (matcher): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): LayerNorm((2, 128, 128), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((2, 128, 128), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
      (conv3): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))
      (scaler): ScaleSkip2D()
    )
  )
)
Epoch 0/199
----------
train Loss: 0.1651 Acc: 0.7239 F1: 0.7291
val Loss: 0.1363 Acc: 0.8565 F1: 0.8480

Epoch 1/199
----------
train Loss: 0.1256 Acc: 0.8750 F1: 0.8660
val Loss: 0.1046 Acc: 0.9202 F1: 0.9177

Epoch 2/199
----------
train Loss: 0.1064 Acc: 0.8996 F1: 0.8937
val Loss: 0.0943 Acc: 0.9227 F1: 0.9201

Epoch 3/199
----------
train Loss: 0.0998 Acc: 0.8969 F1: 0.8909
val Loss: 0.0916 Acc: 0.9112 F1: 0.9065

Epoch 4/199
----------
train Loss: 0.0921 Acc: 0.9035 F1: 0.8983
val Loss: 0.0839 Acc: 0.9229 F1: 0.9234

Epoch 5/199
----------
train Loss: 0.0851 Acc: 0.9069 F1: 0.9029
val Loss: 0.0734 Acc: 0.9341 F1: 0.9331

Epoch 6/199
----------
train Loss: 0.0797 Acc: 0.9106 F1: 0.9073
val Loss: 0.0691 Acc: 0.9317 F1: 0.9302

Epoch 7/199
----------
train Loss: 0.0741 Acc: 0.9137 F1: 0.9112
val Loss: 0.0654 Acc: 0.9272 F1: 0.9265

Epoch 8/199
----------
train Loss: 0.0677 Acc: 0.9209 F1: 0.9191
val Loss: 0.0616 Acc: 0.9291 F1: 0.9284

Epoch 9/199
----------
train Loss: 0.0622 Acc: 0.9255 F1: 0.9240
val Loss: 0.0631 Acc: 0.9194 F1: 0.9199

Epoch 10/199
----------
train Loss: 0.0581 Acc: 0.9297 F1: 0.9285
val Loss: 0.0553 Acc: 0.9309 F1: 0.9308

Epoch 11/199
----------
train Loss: 0.0526 Acc: 0.9348 F1: 0.9339
val Loss: 0.0565 Acc: 0.9242 F1: 0.9251

Epoch 12/199
----------
train Loss: 0.0481 Acc: 0.9400 F1: 0.9392
val Loss: 0.0533 Acc: 0.9302 F1: 0.9306

Epoch 13/199
----------
train Loss: 0.0437 Acc: 0.9443 F1: 0.9437
val Loss: 0.0506 Acc: 0.9304 F1: 0.9307

Epoch 14/199
----------
train Loss: 0.0429 Acc: 0.9436 F1: 0.9431
val Loss: 0.0499 Acc: 0.9325 F1: 0.9330

Epoch 15/199
----------
train Loss: 0.0379 Acc: 0.9499 F1: 0.9494
val Loss: 0.0508 Acc: 0.9275 F1: 0.9284

Epoch 16/199
----------
train Loss: 0.0381 Acc: 0.9489 F1: 0.9484
val Loss: 0.0514 Acc: 0.9266 F1: 0.9280

Epoch 17/199
----------
train Loss: 0.0376 Acc: 0.9481 F1: 0.9476
val Loss: 0.0541 Acc: 0.9251 F1: 0.9265

Epoch 18/199
----------
train Loss: 0.0333 Acc: 0.9527 F1: 0.9524
val Loss: 0.0472 Acc: 0.9315 F1: 0.9320

Epoch 19/199
----------
train Loss: 0.0297 Acc: 0.9589 F1: 0.9587
val Loss: 0.0478 Acc: 0.9326 F1: 0.9333

Epoch 20/199
----------
train Loss: 0.0278 Acc: 0.9610 F1: 0.9608
val Loss: 0.0440 Acc: 0.9414 F1: 0.9413

Epoch 21/199
----------
train Loss: 0.0263 Acc: 0.9624 F1: 0.9622
val Loss: 0.0488 Acc: 0.9368 F1: 0.9372

Epoch 22/199
----------
train Loss: 0.0251 Acc: 0.9638 F1: 0.9637
val Loss: 0.0505 Acc: 0.9362 F1: 0.9370

Epoch 23/199
----------
train Loss: 0.0240 Acc: 0.9656 F1: 0.9654
val Loss: 0.0528 Acc: 0.9344 F1: 0.9354

Epoch 24/199
----------
train Loss: 0.0229 Acc: 0.9669 F1: 0.9667
val Loss: 0.0514 Acc: 0.9374 F1: 0.9382

Epoch 25/199
----------
train Loss: 0.0224 Acc: 0.9672 F1: 0.9671
val Loss: 0.0481 Acc: 0.9408 F1: 0.9410

Epoch 26/199
----------
train Loss: 0.0216 Acc: 0.9678 F1: 0.9677
val Loss: 0.0526 Acc: 0.9398 F1: 0.9405

Epoch 27/199
----------
train Loss: 0.0226 Acc: 0.9656 F1: 0.9655
val Loss: 0.0476 Acc: 0.9391 F1: 0.9396

Epoch 28/199
----------
train Loss: 0.0207 Acc: 0.9694 F1: 0.9693
val Loss: 0.0564 Acc: 0.9358 F1: 0.9370

Epoch 29/199
----------
train Loss: 0.0203 Acc: 0.9691 F1: 0.9690
val Loss: 0.0539 Acc: 0.9396 F1: 0.9404

Epoch 30/199
----------
train Loss: 0.0190 Acc: 0.9712 F1: 0.9711
val Loss: 0.0483 Acc: 0.9427 F1: 0.9431

Epoch 31/199
----------
train Loss: 0.0352 Acc: 0.9497 F1: 0.9494
val Loss: 0.0539 Acc: 0.9155 F1: 0.9181

Epoch 32/199
----------
train Loss: 0.0313 Acc: 0.9529 F1: 0.9527
val Loss: 0.0487 Acc: 0.9324 F1: 0.9326

Epoch 33/199
----------
train Loss: 0.0241 Acc: 0.9647 F1: 0.9645
val Loss: 0.0554 Acc: 0.9341 F1: 0.9351

Epoch 34/199
----------
train Loss: 0.0200 Acc: 0.9703 F1: 0.9702
val Loss: 0.0470 Acc: 0.9423 F1: 0.9428

Epoch 35/199
----------
train Loss: 0.0184 Acc: 0.9723 F1: 0.9722
val Loss: 0.0523 Acc: 0.9393 F1: 0.9398

Epoch 36/199
----------
train Loss: 0.0174 Acc: 0.9739 F1: 0.9738
val Loss: 0.0553 Acc: 0.9383 F1: 0.9393

Epoch 37/199
----------
train Loss: 0.0166 Acc: 0.9749 F1: 0.9748
val Loss: 0.0564 Acc: 0.9400 F1: 0.9409

Epoch 38/199
----------
train Loss: 0.0160 Acc: 0.9757 F1: 0.9756
val Loss: 0.0521 Acc: 0.9422 F1: 0.9425

Epoch 39/199
----------
train Loss: 0.0153 Acc: 0.9768 F1: 0.9768
val Loss: 0.0574 Acc: 0.9423 F1: 0.9431

Epoch 40/199
----------
train Loss: 0.0150 Acc: 0.9771 F1: 0.9770
val Loss: 0.0522 Acc: 0.9415 F1: 0.9419

Epoch 41/199
----------
train Loss: 0.0146 Acc: 0.9775 F1: 0.9774
val Loss: 0.0640 Acc: 0.9380 F1: 0.9391

Epoch 42/199
----------
train Loss: 0.0155 Acc: 0.9762 F1: 0.9761
val Loss: 0.0625 Acc: 0.9335 F1: 0.9348

Epoch 43/199
----------
train Loss: 0.0144 Acc: 0.9780 F1: 0.9780
val Loss: 0.0572 Acc: 0.9419 F1: 0.9427

Epoch 44/199
----------
train Loss: 0.0127 Acc: 0.9803 F1: 0.9803
val Loss: 0.0586 Acc: 0.9422 F1: 0.9428

Epoch 45/199
----------
train Loss: 0.0138 Acc: 0.9791 F1: 0.9791
val Loss: 0.0562 Acc: 0.9454 F1: 0.9459

Epoch 46/199
----------
train Loss: 0.0133 Acc: 0.9799 F1: 0.9798
val Loss: 0.0639 Acc: 0.9403 F1: 0.9413

Epoch 47/199
----------
train Loss: 0.0122 Acc: 0.9817 F1: 0.9816
val Loss: 0.0646 Acc: 0.9407 F1: 0.9418

Epoch 48/199
----------
train Loss: 0.0129 Acc: 0.9806 F1: 0.9806
val Loss: 0.0573 Acc: 0.9463 F1: 0.9467

Epoch 49/199
----------
train Loss: 0.0314 Acc: 0.9521 F1: 0.9519
val Loss: 0.0467 Acc: 0.9274 F1: 0.9277

Epoch 50/199
----------
train Loss: 0.0308 Acc: 0.9532 F1: 0.9531
val Loss: 0.0517 Acc: 0.9292 F1: 0.9301

Epoch 51/199
----------
train Loss: 0.0265 Acc: 0.9578 F1: 0.9577
val Loss: 0.0479 Acc: 0.9299 F1: 0.9303

Epoch 52/199
----------
train Loss: 0.0224 Acc: 0.9633 F1: 0.9632
val Loss: 0.0502 Acc: 0.9288 F1: 0.9285

Epoch 53/199
----------
train Loss: 0.0196 Acc: 0.9684 F1: 0.9683
val Loss: 0.0525 Acc: 0.9238 F1: 0.9236

Epoch 54/199
----------
train Loss: 0.0185 Acc: 0.9699 F1: 0.9698
val Loss: 0.0611 Acc: 0.9232 F1: 0.9249

Epoch 55/199
----------
train Loss: 0.0172 Acc: 0.9724 F1: 0.9723
val Loss: 0.0558 Acc: 0.9361 F1: 0.9364

Epoch 56/199
----------
train Loss: 0.0160 Acc: 0.9750 F1: 0.9749
val Loss: 0.0514 Acc: 0.9375 F1: 0.9378

Epoch 57/199
----------
train Loss: 0.0149 Acc: 0.9763 F1: 0.9763
val Loss: 0.0555 Acc: 0.9321 F1: 0.9325

Epoch 58/199
----------
train Loss: 0.0128 Acc: 0.9804 F1: 0.9804
val Loss: 0.0644 Acc: 0.9355 F1: 0.9364

Epoch 59/199
----------
train Loss: 0.0109 Acc: 0.9830 F1: 0.9830
val Loss: 0.0667 Acc: 0.9350 F1: 0.9359

Epoch 60/199
----------
train Loss: 0.0096 Acc: 0.9858 F1: 0.9858
val Loss: 0.0676 Acc: 0.9373 F1: 0.9378

Epoch 61/199
----------
train Loss: 0.0090 Acc: 0.9866 F1: 0.9866
val Loss: 0.0705 Acc: 0.9413 F1: 0.9420

Epoch 62/199
----------
train Loss: 0.0090 Acc: 0.9866 F1: 0.9866
val Loss: 0.0725 Acc: 0.9398 F1: 0.9405

Epoch 63/199
----------
train Loss: 0.0078 Acc: 0.9883 F1: 0.9883
val Loss: 0.0726 Acc: 0.9417 F1: 0.9423

Epoch 64/199
----------
train Loss: 0.0074 Acc: 0.9891 F1: 0.9891
val Loss: 0.0742 Acc: 0.9420 F1: 0.9425

Epoch 65/199
----------
train Loss: 0.0069 Acc: 0.9899 F1: 0.9899
val Loss: 0.0834 Acc: 0.9381 F1: 0.9388

Epoch 66/199
----------
train Loss: 0.0070 Acc: 0.9896 F1: 0.9896
val Loss: 0.0751 Acc: 0.9414 F1: 0.9421

Epoch 67/199
----------
train Loss: 0.0071 Acc: 0.9896 F1: 0.9896
val Loss: 0.0768 Acc: 0.9406 F1: 0.9413

Epoch 68/199
----------
train Loss: 0.0072 Acc: 0.9894 F1: 0.9894
val Loss: 0.0720 Acc: 0.9414 F1: 0.9415

Epoch 69/199
----------
train Loss: 0.0073 Acc: 0.9892 F1: 0.9892
val Loss: 0.0764 Acc: 0.9407 F1: 0.9410

Epoch 70/199
----------
train Loss: 0.0067 Acc: 0.9903 F1: 0.9903
val Loss: 0.0797 Acc: 0.9414 F1: 0.9419

Epoch 71/199
----------
train Loss: 0.0064 Acc: 0.9905 F1: 0.9905
val Loss: 0.0709 Acc: 0.9411 F1: 0.9412

Epoch 72/199
----------
train Loss: 0.0064 Acc: 0.9905 F1: 0.9905
val Loss: 0.0836 Acc: 0.9421 F1: 0.9428

Epoch 73/199
----------
train Loss: 0.0058 Acc: 0.9914 F1: 0.9914
val Loss: 0.0791 Acc: 0.9428 F1: 0.9433

Epoch 74/199
----------
train Loss: 0.0053 Acc: 0.9924 F1: 0.9924
val Loss: 0.0839 Acc: 0.9422 F1: 0.9428

Epoch 75/199
----------
train Loss: 0.0068 Acc: 0.9899 F1: 0.9899
val Loss: 0.0774 Acc: 0.9450 F1: 0.9453

Epoch 76/199
----------
train Loss: 0.0059 Acc: 0.9914 F1: 0.9914
val Loss: 0.0808 Acc: 0.9448 F1: 0.9453

Epoch 77/199
----------
train Loss: 0.0050 Acc: 0.9929 F1: 0.9929
val Loss: 0.0900 Acc: 0.9432 F1: 0.9440

Epoch 78/199
----------
train Loss: 0.0046 Acc: 0.9935 F1: 0.9935
val Loss: 0.0879 Acc: 0.9456 F1: 0.9461

Epoch 79/199
----------
train Loss: 0.0042 Acc: 0.9941 F1: 0.9941
val Loss: 0.0842 Acc: 0.9457 F1: 0.9463

Epoch 80/199
----------
train Loss: 0.0038 Acc: 0.9947 F1: 0.9947
val Loss: 0.0864 Acc: 0.9476 F1: 0.9481

Epoch 81/199
----------
train Loss: 0.0037 Acc: 0.9948 F1: 0.9948
val Loss: 0.0941 Acc: 0.9429 F1: 0.9436

Epoch 82/199
----------
train Loss: 0.0037 Acc: 0.9948 F1: 0.9948
val Loss: 0.0994 Acc: 0.9416 F1: 0.9419

Epoch 83/199
----------
train Loss: 0.0035 Acc: 0.9952 F1: 0.9952
val Loss: 0.0963 Acc: 0.9434 F1: 0.9441

Epoch 84/199
----------
train Loss: 0.0033 Acc: 0.9955 F1: 0.9955
val Loss: 0.0918 Acc: 0.9462 F1: 0.9466

Epoch 85/199
----------
train Loss: 0.0033 Acc: 0.9954 F1: 0.9954
val Loss: 0.1058 Acc: 0.9421 F1: 0.9428

Epoch 86/199
----------
train Loss: 0.0033 Acc: 0.9954 F1: 0.9954
val Loss: 0.0976 Acc: 0.9451 F1: 0.9457

Epoch 87/199
----------
train Loss: 0.0036 Acc: 0.9950 F1: 0.9950
val Loss: 0.1099 Acc: 0.9384 F1: 0.9393

Epoch 88/199
----------
train Loss: 0.0034 Acc: 0.9952 F1: 0.9952
val Loss: 0.0942 Acc: 0.9452 F1: 0.9458

Epoch 89/199
----------
train Loss: 0.0031 Acc: 0.9958 F1: 0.9958
val Loss: 0.0947 Acc: 0.9483 F1: 0.9489

Epoch 90/199
----------
train Loss: 0.0025 Acc: 0.9967 F1: 0.9967
val Loss: 0.1081 Acc: 0.9423 F1: 0.9425

Epoch 91/199
----------
train Loss: 0.0027 Acc: 0.9964 F1: 0.9964
val Loss: 0.1013 Acc: 0.9473 F1: 0.9479

Epoch 92/199
----------
train Loss: 0.0026 Acc: 0.9966 F1: 0.9966
val Loss: 0.1019 Acc: 0.9467 F1: 0.9473

Epoch 93/199
----------
train Loss: 0.0022 Acc: 0.9972 F1: 0.9972
val Loss: 0.1072 Acc: 0.9441 F1: 0.9447

Epoch 94/199
----------
train Loss: 0.0020 Acc: 0.9975 F1: 0.9975
val Loss: 0.1035 Acc: 0.9480 F1: 0.9485

Epoch 95/199
----------
train Loss: 0.0020 Acc: 0.9973 F1: 0.9973
val Loss: 0.1105 Acc: 0.9468 F1: 0.9474

Epoch 96/199
----------
train Loss: 0.0018 Acc: 0.9978 F1: 0.9978
val Loss: 0.1135 Acc: 0.9471 F1: 0.9476

Epoch 97/199
----------
train Loss: 0.0016 Acc: 0.9982 F1: 0.9982
val Loss: 0.1048 Acc: 0.9493 F1: 0.9496

Epoch 98/199
----------
train Loss: 0.0016 Acc: 0.9981 F1: 0.9981
val Loss: 0.1143 Acc: 0.9470 F1: 0.9474

Epoch 99/199
----------
train Loss: 0.0013 Acc: 0.9986 F1: 0.9986
val Loss: 0.1188 Acc: 0.9473 F1: 0.9480

Epoch 100/199
----------
train Loss: 0.0012 Acc: 0.9987 F1: 0.9987
val Loss: 0.1140 Acc: 0.9498 F1: 0.9503

Epoch 101/199
----------
train Loss: 0.0011 Acc: 0.9988 F1: 0.9988
val Loss: 0.1251 Acc: 0.9475 F1: 0.9481

Epoch 102/199
----------
train Loss: 0.0009 Acc: 0.9991 F1: 0.9991
val Loss: 0.1194 Acc: 0.9493 F1: 0.9497

Epoch 103/199
----------
train Loss: 0.0009 Acc: 0.9990 F1: 0.9990
val Loss: 0.1318 Acc: 0.9467 F1: 0.9473

Epoch 104/199
----------
train Loss: 0.0010 Acc: 0.9989 F1: 0.9989
val Loss: 0.1211 Acc: 0.9475 F1: 0.9481

Epoch 105/199
----------
train Loss: 0.0028 Acc: 0.9965 F1: 0.9965
val Loss: 0.1046 Acc: 0.9506 F1: 0.9510

Epoch 106/199
----------
train Loss: 0.0029 Acc: 0.9961 F1: 0.9961
val Loss: 0.1073 Acc: 0.9494 F1: 0.9499

Epoch 107/199
----------
train Loss: 0.0020 Acc: 0.9975 F1: 0.9975
val Loss: 0.1131 Acc: 0.9476 F1: 0.9480

Epoch 108/199
----------
train Loss: 0.0039 Acc: 0.9951 F1: 0.9951
val Loss: 0.0998 Acc: 0.9402 F1: 0.9405

Epoch 109/199
----------
train Loss: 0.0113 Acc: 0.9847 F1: 0.9847
val Loss: 0.0560 Acc: 0.9457 F1: 0.9460

Epoch 110/199
----------
train Loss: 0.0292 Acc: 0.9639 F1: 0.9637
val Loss: 0.0462 Acc: 0.9285 F1: 0.9275

Epoch 111/199
----------
train Loss: 0.0176 Acc: 0.9744 F1: 0.9743
val Loss: 0.0538 Acc: 0.9438 F1: 0.9441

Epoch 112/199
----------
train Loss: 0.0077 Acc: 0.9892 F1: 0.9892
val Loss: 0.0672 Acc: 0.9482 F1: 0.9487

Epoch 113/199
----------
train Loss: 0.0049 Acc: 0.9935 F1: 0.9935
val Loss: 0.0750 Acc: 0.9497 F1: 0.9501

Epoch 114/199
----------
train Loss: 0.0058 Acc: 0.9925 F1: 0.9925
val Loss: 0.0770 Acc: 0.9411 F1: 0.9422

Epoch 115/199
----------
train Loss: 0.0042 Acc: 0.9944 F1: 0.9944
val Loss: 0.0886 Acc: 0.9470 F1: 0.9475

Epoch 116/199
----------
train Loss: 0.0021 Acc: 0.9975 F1: 0.9975
val Loss: 0.0971 Acc: 0.9497 F1: 0.9503

Epoch 117/199
----------
train Loss: 0.0014 Acc: 0.9985 F1: 0.9985
val Loss: 0.1033 Acc: 0.9491 F1: 0.9496

Epoch 118/199
----------
train Loss: 0.0010 Acc: 0.9991 F1: 0.9991
val Loss: 0.1074 Acc: 0.9506 F1: 0.9511

Epoch 119/199
----------
train Loss: 0.0009 Acc: 0.9991 F1: 0.9991
val Loss: 0.1111 Acc: 0.9498 F1: 0.9503

Epoch 120/199
----------
train Loss: 0.0008 Acc: 0.9993 F1: 0.9993
val Loss: 0.1132 Acc: 0.9504 F1: 0.9508

Epoch 121/199
----------
train Loss: 0.0007 Acc: 0.9994 F1: 0.9994
val Loss: 0.1173 Acc: 0.9505 F1: 0.9510

Epoch 122/199
----------
train Loss: 0.0008 Acc: 0.9992 F1: 0.9992
val Loss: 0.1107 Acc: 0.9514 F1: 0.9518

Epoch 123/199
----------
train Loss: 0.0008 Acc: 0.9992 F1: 0.9992
val Loss: 0.1207 Acc: 0.9490 F1: 0.9496

Epoch 124/199
----------
train Loss: 0.0006 Acc: 0.9995 F1: 0.9995
val Loss: 0.1209 Acc: 0.9494 F1: 0.9498

Epoch 125/199
----------
train Loss: 0.0005 Acc: 0.9996 F1: 0.9996
val Loss: 0.1236 Acc: 0.9495 F1: 0.9500

Epoch 126/199
----------
train Loss: 0.0005 Acc: 0.9997 F1: 0.9997
val Loss: 0.1243 Acc: 0.9495 F1: 0.9500

Epoch 127/199
----------
train Loss: 0.0004 Acc: 0.9997 F1: 0.9997
val Loss: 0.1239 Acc: 0.9504 F1: 0.9508

Epoch 128/199
----------
train Loss: 0.0004 Acc: 0.9998 F1: 0.9998
val Loss: 0.1230 Acc: 0.9516 F1: 0.9520

Epoch 129/199
----------
train Loss: 0.0004 Acc: 0.9998 F1: 0.9998
val Loss: 0.1271 Acc: 0.9502 F1: 0.9506

Epoch 130/199
----------
train Loss: 0.0003 Acc: 0.9998 F1: 0.9998
val Loss: 0.1287 Acc: 0.9505 F1: 0.9510

Epoch 131/199
----------
train Loss: 0.0005 Acc: 0.9995 F1: 0.9995
val Loss: 0.1248 Acc: 0.9499 F1: 0.9503

Epoch 132/199
----------
train Loss: 0.0005 Acc: 0.9996 F1: 0.9996
val Loss: 0.1259 Acc: 0.9513 F1: 0.9517

Epoch 133/199
----------
train Loss: 0.0003 Acc: 0.9997 F1: 0.9997
val Loss: 0.1318 Acc: 0.9499 F1: 0.9503

Epoch 134/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1338 Acc: 0.9506 F1: 0.9511

Epoch 135/199
----------
train Loss: 0.0002 Acc: 0.9998 F1: 0.9998
val Loss: 0.1354 Acc: 0.9505 F1: 0.9510

Epoch 136/199
----------
train Loss: 0.0002 Acc: 0.9998 F1: 0.9998
val Loss: 0.1342 Acc: 0.9494 F1: 0.9499

Epoch 137/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1389 Acc: 0.9493 F1: 0.9499

Epoch 138/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1352 Acc: 0.9510 F1: 0.9514

Epoch 139/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1390 Acc: 0.9503 F1: 0.9509

Epoch 140/199
----------
train Loss: 0.0002 Acc: 0.9998 F1: 0.9998
val Loss: 0.1337 Acc: 0.9514 F1: 0.9518

Epoch 141/199
----------
train Loss: 0.0004 Acc: 0.9997 F1: 0.9997
val Loss: 0.1324 Acc: 0.9510 F1: 0.9514

Epoch 142/199
----------
train Loss: 0.0004 Acc: 0.9997 F1: 0.9997
val Loss: 0.1321 Acc: 0.9502 F1: 0.9505

Epoch 143/199
----------
train Loss: 0.0006 Acc: 0.9995 F1: 0.9995
val Loss: 0.1339 Acc: 0.9513 F1: 0.9518

Epoch 144/199
----------
train Loss: 0.0010 Acc: 0.9988 F1: 0.9988
val Loss: 0.1353 Acc: 0.9450 F1: 0.9455

Epoch 145/199
----------
train Loss: 0.0027 Acc: 0.9969 F1: 0.9969
val Loss: 0.1109 Acc: 0.9424 F1: 0.9427

Epoch 146/199
----------
train Loss: 0.0023 Acc: 0.9971 F1: 0.9971
val Loss: 0.1267 Acc: 0.9434 F1: 0.9442

Epoch 147/199
----------
train Loss: 0.0018 Acc: 0.9979 F1: 0.9979
val Loss: 0.1086 Acc: 0.9492 F1: 0.9495

Epoch 148/199
----------
train Loss: 0.0015 Acc: 0.9983 F1: 0.9983
val Loss: 0.1234 Acc: 0.9487 F1: 0.9493

Epoch 149/199
----------
train Loss: 0.0010 Acc: 0.9989 F1: 0.9989
val Loss: 0.1282 Acc: 0.9475 F1: 0.9481

Epoch 150/199
----------
train Loss: 0.0005 Acc: 0.9995 F1: 0.9995
val Loss: 0.1300 Acc: 0.9503 F1: 0.9509

Epoch 151/199
----------
train Loss: 0.0003 Acc: 0.9997 F1: 0.9997
val Loss: 0.1316 Acc: 0.9508 F1: 0.9512

Epoch 152/199
----------
train Loss: 0.0003 Acc: 0.9998 F1: 0.9998
val Loss: 0.1418 Acc: 0.9488 F1: 0.9494

Epoch 153/199
----------
train Loss: 0.0002 Acc: 0.9998 F1: 0.9998
val Loss: 0.1404 Acc: 0.9495 F1: 0.9500

Epoch 154/199
----------
train Loss: 0.0004 Acc: 0.9997 F1: 0.9997
val Loss: 0.1480 Acc: 0.9483 F1: 0.9488

Epoch 155/199
----------
train Loss: 0.0007 Acc: 0.9993 F1: 0.9993
val Loss: 0.1435 Acc: 0.9476 F1: 0.9483

Epoch 156/199
----------
train Loss: 0.0010 Acc: 0.9989 F1: 0.9989
val Loss: 0.1253 Acc: 0.9497 F1: 0.9500

Epoch 157/199
----------
train Loss: 0.0009 Acc: 0.9990 F1: 0.9990
val Loss: 0.1333 Acc: 0.9500 F1: 0.9505

Epoch 158/199
----------
train Loss: 0.0004 Acc: 0.9996 F1: 0.9996
val Loss: 0.1350 Acc: 0.9495 F1: 0.9500

Epoch 159/199
----------
train Loss: 0.0002 Acc: 0.9998 F1: 0.9998
val Loss: 0.1423 Acc: 0.9501 F1: 0.9506

Epoch 160/199
----------
train Loss: 0.0001 Acc: 0.9999 F1: 0.9999
val Loss: 0.1426 Acc: 0.9510 F1: 0.9515

Epoch 161/199
----------
train Loss: 0.0001 Acc: 0.9999 F1: 0.9999
val Loss: 0.1459 Acc: 0.9505 F1: 0.9510

Epoch 162/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1456 Acc: 0.9510 F1: 0.9515

Epoch 163/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1507 Acc: 0.9495 F1: 0.9500

Epoch 164/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1484 Acc: 0.9509 F1: 0.9514

Epoch 165/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1517 Acc: 0.9505 F1: 0.9510

Epoch 166/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1508 Acc: 0.9509 F1: 0.9514

Epoch 167/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1545 Acc: 0.9503 F1: 0.9509

Epoch 168/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1519 Acc: 0.9511 F1: 0.9516

Epoch 169/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1531 Acc: 0.9512 F1: 0.9517

Epoch 170/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1569 Acc: 0.9506 F1: 0.9511

Epoch 171/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1552 Acc: 0.9509 F1: 0.9514

Epoch 172/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1546 Acc: 0.9513 F1: 0.9518

Epoch 173/199
----------
train Loss: 0.0000 Acc: 1.0000 F1: 1.0000
val Loss: 0.1583 Acc: 0.9507 F1: 0.9512

Epoch 174/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1555 Acc: 0.9504 F1: 0.9508

Epoch 175/199
----------
train Loss: 0.0004 Acc: 0.9996 F1: 0.9996
val Loss: 0.1457 Acc: 0.9499 F1: 0.9503

Epoch 176/199
----------
train Loss: 0.0043 Acc: 0.9947 F1: 0.9947
val Loss: 0.0864 Acc: 0.9307 F1: 0.9314

Epoch 177/199
----------
train Loss: 0.0196 Acc: 0.9751 F1: 0.9750
val Loss: 0.0726 Acc: 0.9413 F1: 0.9417

Epoch 178/199
----------
train Loss: 0.0124 Acc: 0.9846 F1: 0.9845
val Loss: 0.0693 Acc: 0.9475 F1: 0.9481

Epoch 179/199
----------
train Loss: 0.0067 Acc: 0.9914 F1: 0.9914
val Loss: 0.0713 Acc: 0.9442 F1: 0.9446

Epoch 180/199
----------
train Loss: 0.0177 Acc: 0.9764 F1: 0.9764
val Loss: 0.0491 Acc: 0.9432 F1: 0.9437

Epoch 181/199
----------
train Loss: 0.0107 Acc: 0.9859 F1: 0.9859
val Loss: 0.0662 Acc: 0.9444 F1: 0.9446

Epoch 182/199
----------
train Loss: 0.0047 Acc: 0.9939 F1: 0.9939
val Loss: 0.0901 Acc: 0.9473 F1: 0.9479

Epoch 183/199
----------
train Loss: 0.0028 Acc: 0.9970 F1: 0.9970
val Loss: 0.0974 Acc: 0.9486 F1: 0.9490

Epoch 184/199
----------
train Loss: 0.0011 Acc: 0.9990 F1: 0.9990
val Loss: 0.1108 Acc: 0.9499 F1: 0.9504

Epoch 185/199
----------
train Loss: 0.0007 Acc: 0.9995 F1: 0.9995
val Loss: 0.1238 Acc: 0.9491 F1: 0.9495

Epoch 186/199
----------
train Loss: 0.0004 Acc: 0.9997 F1: 0.9997
val Loss: 0.1320 Acc: 0.9494 F1: 0.9498

Epoch 187/199
----------
train Loss: 0.0003 Acc: 0.9998 F1: 0.9998
val Loss: 0.1355 Acc: 0.9501 F1: 0.9506

Epoch 188/199
----------
train Loss: 0.0003 Acc: 0.9998 F1: 0.9998
val Loss: 0.1408 Acc: 0.9495 F1: 0.9499

Epoch 189/199
----------
train Loss: 0.0003 Acc: 0.9998 F1: 0.9998
val Loss: 0.1426 Acc: 0.9506 F1: 0.9511

Epoch 190/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1456 Acc: 0.9501 F1: 0.9505

Epoch 191/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1483 Acc: 0.9499 F1: 0.9504

Epoch 192/199
----------
train Loss: 0.0002 Acc: 0.9999 F1: 0.9999
val Loss: 0.1507 Acc: 0.9497 F1: 0.9501

Epoch 193/199
----------
train Loss: 0.0001 Acc: 0.9999 F1: 0.9999
val Loss: 0.1490 Acc: 0.9506 F1: 0.9510

Epoch 194/199
----------
train Loss: 0.0001 Acc: 0.9999 F1: 0.9999
val Loss: 0.1519 Acc: 0.9502 F1: 0.9506

Epoch 195/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1545 Acc: 0.9500 F1: 0.9504

Epoch 196/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1559 Acc: 0.9500 F1: 0.9505

Epoch 197/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1566 Acc: 0.9503 F1: 0.9507

Epoch 198/199
----------
train Loss: 0.0001 Acc: 1.0000 F1: 1.0000
val Loss: 0.1590 Acc: 0.9502 F1: 0.9506

Epoch 199/199
----------
train Loss: 0.0001 Acc: 0.9999 F1: 0.9999
val Loss: 0.1592 Acc: 0.9505 F1: 0.9511

Training complete in 54m 17s
Best val F1: 0.9520
Saving..
Successfully retrieved statistics for job: zhangz65-gpu06-3192673_. 
+------------------------------------------------------------------------------+
| GPU ID: 0                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Fri Mar  1 17:11:15 2024                |
| End Time                           | Fri Mar  1 18:06:14 2024                |
| Total Execution Time (sec)         | 3299.68                                 |
| No. of Processes                   | 1                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 224144                                  |
| Power Usage (Watts)                | Avg: 68.7108, Max: 97.126, Min: 23.22   |
| Max GPU Memory Used (bytes)        | 3999268864                              |
| SM Clock (MHz)                     | Avg: 1368, Max: 1380, Min: 135          |
| Memory Clock (MHz)                 | Avg: 877, Max: 877, Min: 877            |
| SM Utilization (%)                 | Avg: 79, Max: 98, Min: 0                |
| Memory Utilization (%)             | Avg: 15, Max: 20, Min: 0                |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 1570743                                 |
|     Avg SM Utilization (%)         | 80                                      |
|     Avg Memory Utilization (%)     | 15                                      |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+

