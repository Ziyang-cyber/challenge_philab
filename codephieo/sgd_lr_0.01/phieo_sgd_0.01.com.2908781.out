2.2.0
test
DataLoaders created successfully!
FloodPredictorHSL(
  (foundation): Foundation(
    (activation): LeakyReLU(negative_slope=0.01)
    (stem): CNNBlock(
      (activation): LeakyReLU(negative_slope=0.01)
      (activation_out): LeakyReLU(negative_slope=0.01)
      (squeeze): SE_Block(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Linear(in_features=32, out_features=2, bias=False)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2, out_features=32, bias=False)
          (3): Sigmoid()
        )
      )
      (matcher): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
      (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (scaler): ScaleSkip2D()
    )
    (encoder): FoundationEncoder(
      (activation): LeakyReLU(negative_slope=0.01)
      (downsample): ModuleList(
        (0-1): 2 x Sequential(
          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (2): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
        (3): Sequential(
          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
      (block_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (blocks_down): ModuleList(
        (0): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (1): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (2): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (3): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (4): ModuleList(
          (0-4): 5 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
      )
      (prelinear_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
      (linear_encode): Sequential(
        (0): LeakyReLU(negative_slope=0.01)
        (1): Linear(in_features=4096, out_features=1024, bias=True)
        (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (head_clouds): Sequential(
        (0): Linear(in_features=1024, out_features=4, bias=True)
      )
      (head_landcover): Sequential(
        (0): Linear(in_features=1024, out_features=11, bias=True)
      )
      (head_buildings): Sequential(
        (0): Linear(in_features=1024, out_features=1, bias=True)
        (1): Sigmoid()
      )
      (head_water): Sequential(
        (0): Linear(in_features=1024, out_features=1, bias=True)
        (1): Sigmoid()
      )
      (head_coords): Sequential(
        (0): Linear(in_features=1024, out_features=4, bias=True)
        (1): Sigmoid()
      )
    )
    (decoder): FoundationDecoder(
      (activation): LeakyReLU(negative_slope=0.01)
      (linear_decode): Linear(in_features=1024, out_features=4096, bias=True)
      (latent_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
      (prehead_norm): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
      (skip_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (block_scalers): ModuleList(
        (0-4): 5 x ScaleSkip2D()
      )
      (blocks_up): ModuleList(
        (0): ModuleList(
          (0-4): 5 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (1): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=64, out_features=4, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=4, out_features=64, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (2): ModuleList(
          (0-3): 4 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (3): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
        (4): ModuleList(
          (0-2): 3 x CNNBlock(
            (activation): LeakyReLU(negative_slope=0.01)
            (activation_out): LeakyReLU(negative_slope=0.01)
            (squeeze): SE_Block(
              (squeeze): AdaptiveAvgPool2d(output_size=1)
              (excitation): Sequential(
                (0): Linear(in_features=32, out_features=2, bias=False)
                (1): GELU(approximate='none')
                (2): Linear(in_features=2, out_features=32, bias=False)
                (3): Sigmoid()
              )
            )
            (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
            (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
            (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
            (scaler): ScaleSkip2D()
          )
        )
      )
      (upsamplers): ModuleList(
        (0): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (1): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (2): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
        (3): Sequential(
          (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
          (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (3): LeakyReLU(negative_slope=0.01)
        )
      )
    )
    (head): CNNBlock(
      (activation): LeakyReLU(negative_slope=0.01)
      (activation_out): Sigmoid()
      (squeeze): SE_Block(
        (squeeze): AdaptiveAvgPool2d(output_size=1)
        (excitation): Sequential(
          (0): Linear(in_features=10, out_features=1, bias=False)
          (1): GELU(approximate='none')
          (2): Linear(in_features=1, out_features=10, bias=False)
          (3): Sigmoid()
        )
      )
      (matcher): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (norm1): LayerNorm((10, 128, 128), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((10, 128, 128), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
      (conv3): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))
      (scaler): ScaleSkip2D()
    )
  )
  (encoder): FoundationEncoder(
    (activation): LeakyReLU(negative_slope=0.01)
    (downsample): ModuleList(
      (0-1): 2 x Sequential(
        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
      (2): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
      (3): Sequential(
        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
    )
    (block_scalers): ModuleList(
      (0-4): 5 x ScaleSkip2D()
    )
    (blocks_down): ModuleList(
      (0): ModuleList(
        (0-2): 3 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (1): ModuleList(
        (0-2): 3 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (2): ModuleList(
        (0-3): 4 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (3): ModuleList(
        (0-3): 4 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=64, out_features=4, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=4, out_features=64, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (4): ModuleList(
        (0-4): 5 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=64, out_features=4, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=4, out_features=64, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
    )
    (prelinear_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
    (linear_encode): Sequential(
      (0): LeakyReLU(negative_slope=0.01)
      (1): Linear(in_features=4096, out_features=1024, bias=True)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (head_clouds): Sequential(
      (0): Linear(in_features=1024, out_features=4, bias=True)
    )
    (head_landcover): Sequential(
      (0): Linear(in_features=1024, out_features=11, bias=True)
    )
    (head_buildings): Sequential(
      (0): Linear(in_features=1024, out_features=1, bias=True)
      (1): Sigmoid()
    )
    (head_water): Sequential(
      (0): Linear(in_features=1024, out_features=1, bias=True)
      (1): Sigmoid()
    )
    (head_coords): Sequential(
      (0): Linear(in_features=1024, out_features=4, bias=True)
      (1): Sigmoid()
    )
  )
  (decoder): FoundationDecoder(
    (activation): LeakyReLU(negative_slope=0.01)
    (linear_decode): Linear(in_features=1024, out_features=4096, bias=True)
    (latent_norm): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
    (prehead_norm): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
    (skip_scalers): ModuleList(
      (0-4): 5 x ScaleSkip2D()
    )
    (block_scalers): ModuleList(
      (0-4): 5 x ScaleSkip2D()
    )
    (blocks_up): ModuleList(
      (0): ModuleList(
        (0-4): 5 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=64, out_features=4, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=4, out_features=64, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64, 8, 8), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (1): ModuleList(
        (0-3): 4 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=64, out_features=4, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=4, out_features=64, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (2): ModuleList(
        (0-3): 4 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (3): ModuleList(
        (0-2): 3 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
      (4): ModuleList(
        (0-2): 3 x CNNBlock(
          (activation): LeakyReLU(negative_slope=0.01)
          (activation_out): LeakyReLU(negative_slope=0.01)
          (squeeze): SE_Block(
            (squeeze): AdaptiveAvgPool2d(output_size=1)
            (excitation): Sequential(
              (0): Linear(in_features=32, out_features=2, bias=False)
              (1): GELU(approximate='none')
              (2): Linear(in_features=2, out_features=32, bias=False)
              (3): Sigmoid()
            )
          )
          (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
          (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
          (scaler): ScaleSkip2D()
        )
      )
    )
    (upsamplers): ModuleList(
      (0): Sequential(
        (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (2): LayerNorm((64, 16, 16), eps=1e-05, elementwise_affine=True)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (1): Sequential(
        (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
        (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (2): LayerNorm((32, 32, 32), eps=1e-05, elementwise_affine=True)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (2): Sequential(
        (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (2): LayerNorm((32, 64, 64), eps=1e-05, elementwise_affine=True)
        (3): LeakyReLU(negative_slope=0.01)
      )
      (3): Sequential(
        (0): UpsamplingBilinear2d(scale_factor=2.0, mode='bilinear')
        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
        (2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
        (3): LeakyReLU(negative_slope=0.01)
      )
    )
  )
  (stem): CNNBlock(
    (activation): LeakyReLU(negative_slope=0.01)
    (activation_out): LeakyReLU(negative_slope=0.01)
    (squeeze): SE_Block(
      (squeeze): AdaptiveAvgPool2d(output_size=1)
      (excitation): Sequential(
        (0): Linear(in_features=32, out_features=2, bias=False)
        (1): GELU(approximate='none')
        (2): Linear(in_features=2, out_features=32, bias=False)
        (3): Sigmoid()
      )
    )
    (matcher): Conv2d(7, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (norm1): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((32, 128, 128), eps=1e-05, elementwise_affine=True)
    (conv1): Conv2d(7, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
    (conv3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (head): CNNBlock(
    (activation): LeakyReLU(negative_slope=0.01)
    (activation_out): Sigmoid()
    (squeeze): SE_Block(
      (squeeze): AdaptiveAvgPool2d(output_size=1)
      (excitation): Sequential(
        (0): Linear(in_features=2, out_features=1, bias=False)
        (1): GELU(approximate='none')
        (2): Linear(in_features=1, out_features=2, bias=False)
        (3): Sigmoid()
      )
    )
    (matcher): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (norm1): LayerNorm((2, 128, 128), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((2, 128, 128), eps=1e-05, elementwise_affine=True)
    (conv1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=replicate)
    (conv3): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))
    (scaler): ScaleSkip2D()
  )
)
Epoch 0/199
----------
train Loss: 0.6464 acc: 0.0000
val Loss: 0.5778 acc: 0.0000

Epoch 1/199
----------
train Loss: 0.5625 acc: 0.0000
val Loss: 0.5210 acc: 0.0000

Epoch 2/199
----------
train Loss: 0.5117 acc: 0.0000
val Loss: 0.4780 acc: 0.0000

Epoch 3/199
----------
train Loss: 0.4770 acc: 0.0000
val Loss: 0.4715 acc: 0.0000

Epoch 4/199
----------
train Loss: 0.4904 acc: 0.0000
val Loss: 0.4675 acc: 0.0000

Epoch 5/199
----------
train Loss: 0.4619 acc: 0.0000
val Loss: 0.4574 acc: 0.0000

Epoch 6/199
----------
train Loss: 0.4561 acc: 0.0000
val Loss: 0.4588 acc: 0.0000

Epoch 7/199
----------
train Loss: 0.4549 acc: 0.0000
val Loss: 0.4490 acc: 0.0000

Epoch 8/199
----------
train Loss: 0.4467 acc: 0.0000
val Loss: 0.4490 acc: 0.0000

Epoch 9/199
----------
train Loss: 0.4476 acc: 0.0000
val Loss: 0.4477 acc: 0.0000

Epoch 10/199
----------
train Loss: 0.4409 acc: 0.0000
val Loss: 0.4499 acc: 0.0000

Epoch 11/199
----------
train Loss: 0.4425 acc: 0.0000
val Loss: 0.4448 acc: 0.0000

Epoch 12/199
----------
train Loss: 0.4396 acc: 0.0000
val Loss: 0.4437 acc: 0.0000

Epoch 13/199
----------
train Loss: 0.4410 acc: 0.0000
val Loss: 0.4379 acc: 0.0000

Epoch 14/199
----------
train Loss: 0.4508 acc: 0.0000
val Loss: 0.4483 acc: 0.0000

Epoch 15/199
----------
train Loss: 0.4465 acc: 0.0000
val Loss: 0.4413 acc: 0.0000

Epoch 16/199
----------
train Loss: 0.4351 acc: 0.0000
val Loss: 0.4330 acc: 0.0000

Epoch 17/199
----------
train Loss: 0.4353 acc: 0.0000
val Loss: 0.4349 acc: 0.0000

Epoch 18/199
----------
train Loss: 0.4376 acc: 0.0000
val Loss: 0.4264 acc: 0.0000

Epoch 19/199
----------
train Loss: 0.4317 acc: 0.0000
val Loss: 0.4344 acc: 0.0000

Epoch 20/199
----------
train Loss: 0.4304 acc: 0.0000
val Loss: 0.4268 acc: 0.0000

Epoch 21/199
----------
train Loss: 0.4226 acc: 0.0000
val Loss: 0.4297 acc: 0.0000

Epoch 22/199
----------
train Loss: 0.4218 acc: 0.0000
val Loss: 0.4270 acc: 0.0000

Epoch 23/199
----------
train Loss: 0.4294 acc: 0.0000
val Loss: 0.4209 acc: 0.0000

Epoch 24/199
----------
train Loss: 0.4145 acc: 0.0000
val Loss: 0.4186 acc: 0.0000

Epoch 25/199
----------
train Loss: 0.4079 acc: 0.0000
val Loss: 0.4154 acc: 0.0000

Epoch 26/199
----------
train Loss: 0.4065 acc: 0.0000
val Loss: 0.4263 acc: 0.0000

Epoch 27/199
----------
train Loss: 0.4027 acc: 0.0000
val Loss: 0.4071 acc: 0.0000

Epoch 28/199
----------
train Loss: 0.3954 acc: 0.0001
val Loss: 0.4045 acc: 0.0000

Epoch 29/199
----------
train Loss: 0.3907 acc: 0.0001
val Loss: 0.4052 acc: 0.0000

Epoch 30/199
----------
train Loss: 0.3889 acc: 0.0001
val Loss: 0.4058 acc: 0.0000

Epoch 31/199
----------
train Loss: 0.3836 acc: 0.0001
val Loss: 0.4041 acc: 0.0000

Epoch 32/199
----------
train Loss: 0.3816 acc: 0.0001
val Loss: 0.4025 acc: 0.0000

Epoch 33/199
----------
train Loss: 0.3776 acc: 0.0001
val Loss: 0.4052 acc: 0.0000

Epoch 34/199
----------
train Loss: 0.3758 acc: 0.0001
val Loss: 0.4011 acc: 0.0000

Epoch 35/199
----------
train Loss: 0.3735 acc: 0.0000
val Loss: 0.4112 acc: 0.0000

Epoch 36/199
----------
train Loss: 0.3754 acc: 0.0000
val Loss: 0.4033 acc: 0.0000

Epoch 37/199
----------
train Loss: 0.3766 acc: 0.0000
val Loss: 0.4137 acc: 0.0000

Epoch 38/199
----------
train Loss: 0.3800 acc: 0.0000
val Loss: 0.4073 acc: 0.0000

Epoch 39/199
----------
train Loss: 0.3730 acc: 0.0000
val Loss: 0.3993 acc: 0.0001

Epoch 40/199
----------
train Loss: 0.3674 acc: 0.0000
val Loss: 0.3989 acc: 0.0000

Epoch 41/199
----------
train Loss: 0.3640 acc: 0.0000
val Loss: 0.3978 acc: 0.0000

Epoch 42/199
----------
train Loss: 0.3621 acc: 0.0000
val Loss: 0.3984 acc: 0.0000

Epoch 43/199
----------
train Loss: 0.3609 acc: 0.0000
val Loss: 0.3958 acc: 0.0000

Epoch 44/199
----------
train Loss: 0.3600 acc: 0.0000
val Loss: 0.3974 acc: 0.0000

Epoch 45/199
----------
train Loss: 0.3577 acc: 0.0000
val Loss: 0.3940 acc: 0.0000

Epoch 46/199
----------
train Loss: 0.3570 acc: 0.0000
val Loss: 0.3958 acc: 0.0000

Epoch 47/199
----------
train Loss: 0.3580 acc: 0.0000
val Loss: 0.3962 acc: 0.0000

Epoch 48/199
----------
train Loss: 0.3604 acc: 0.0000
val Loss: 0.3971 acc: 0.0000

Epoch 49/199
----------
train Loss: 0.3568 acc: 0.0000
val Loss: 0.3934 acc: 0.0000

Epoch 50/199
----------
train Loss: 0.3552 acc: 0.0000
val Loss: 0.3944 acc: 0.0000

Epoch 51/199
----------
train Loss: 0.3544 acc: 0.0000
val Loss: 0.3916 acc: 0.0000

Epoch 52/199
----------
train Loss: 0.3536 acc: 0.0000
val Loss: 0.3907 acc: 0.0000

Epoch 53/199
----------
train Loss: 0.3523 acc: 0.0000
val Loss: 0.3910 acc: 0.0000

Epoch 54/199
----------
train Loss: 0.3525 acc: 0.0000
val Loss: 0.3917 acc: 0.0000

Epoch 55/199
----------
train Loss: 0.3520 acc: 0.0000
val Loss: 0.3934 acc: 0.0000

Epoch 56/199
----------
train Loss: 0.3528 acc: 0.0000
val Loss: 0.3931 acc: 0.0000

Epoch 57/199
----------
train Loss: 0.3577 acc: 0.0000
val Loss: 0.3995 acc: 0.0000

Epoch 58/199
----------
train Loss: 0.3577 acc: 0.0000
val Loss: 0.3933 acc: 0.0000

Epoch 59/199
----------
train Loss: 0.3563 acc: 0.0000
val Loss: 0.3945 acc: 0.0000

Epoch 60/199
----------
train Loss: 0.3536 acc: 0.0000
val Loss: 0.3894 acc: 0.0000

Epoch 61/199
----------
train Loss: 0.3515 acc: 0.0000
val Loss: 0.3883 acc: 0.0000

Epoch 62/199
----------
train Loss: 0.3504 acc: 0.0000
val Loss: 0.3887 acc: 0.0000

Epoch 63/199
----------
train Loss: 0.3493 acc: 0.0000
val Loss: 0.3884 acc: 0.0000

Epoch 64/199
----------
train Loss: 0.3489 acc: 0.0000
val Loss: 0.3895 acc: 0.0000

Epoch 65/199
----------
train Loss: 0.3504 acc: 0.0000
val Loss: 0.3923 acc: 0.0000

Epoch 66/199
----------
train Loss: 0.3505 acc: 0.0000
val Loss: 0.3927 acc: 0.0000

Epoch 67/199
----------
train Loss: 0.3505 acc: 0.0000
val Loss: 0.3894 acc: 0.0000

Epoch 68/199
----------
train Loss: 0.3492 acc: 0.0000
val Loss: 0.3891 acc: 0.0000

Epoch 69/199
----------
train Loss: 0.3482 acc: 0.0000
val Loss: 0.3896 acc: 0.0000

Epoch 70/199
----------
train Loss: 0.3490 acc: 0.0000
val Loss: 0.3867 acc: 0.0000

Epoch 71/199
----------
train Loss: 0.3481 acc: 0.0000
val Loss: 0.3864 acc: 0.0000

Epoch 72/199
----------
train Loss: 0.3472 acc: 0.0000
val Loss: 0.3859 acc: 0.0000

Epoch 73/199
----------
train Loss: 0.3463 acc: 0.0000
val Loss: 0.3856 acc: 0.0000

Epoch 74/199
----------
train Loss: 0.3467 acc: 0.0000
val Loss: 0.3866 acc: 0.0000

Epoch 75/199
----------
train Loss: 0.3474 acc: 0.0000
val Loss: 0.3876 acc: 0.0000

Epoch 76/199
----------
train Loss: 0.3472 acc: 0.0000
val Loss: 0.3916 acc: 0.0000

Epoch 77/199
----------
train Loss: 0.3484 acc: 0.0000
val Loss: 0.3955 acc: 0.0000

Epoch 78/199
----------
train Loss: 0.3517 acc: 0.0000
val Loss: 0.4000 acc: 0.0000

Epoch 79/199
----------
train Loss: 0.3539 acc: 0.0000
val Loss: 0.3890 acc: 0.0001

Epoch 80/199
----------
train Loss: 0.3491 acc: 0.0000
val Loss: 0.3894 acc: 0.0000

Epoch 81/199
----------
train Loss: 0.3468 acc: 0.0000
val Loss: 0.3855 acc: 0.0000

Epoch 82/199
----------
train Loss: 0.3458 acc: 0.0000
val Loss: 0.3870 acc: 0.0000

Epoch 83/199
----------
train Loss: 0.3452 acc: 0.0000
val Loss: 0.3859 acc: 0.0000

Epoch 84/199
----------
train Loss: 0.3452 acc: 0.0000
val Loss: 0.3845 acc: 0.0000

Epoch 85/199
----------
train Loss: 0.3450 acc: 0.0000
val Loss: 0.3849 acc: 0.0000

Epoch 86/199
----------
train Loss: 0.3458 acc: 0.0000
val Loss: 0.3855 acc: 0.0000

Epoch 87/199
----------
train Loss: 0.3471 acc: 0.0000
val Loss: 0.3915 acc: 0.0000

Epoch 88/199
----------
train Loss: 0.3473 acc: 0.0000
val Loss: 0.3878 acc: 0.0000

Epoch 89/199
----------
train Loss: 0.3458 acc: 0.0000
val Loss: 0.3853 acc: 0.0001

Epoch 90/199
----------
train Loss: 0.3470 acc: 0.0000
val Loss: 0.3847 acc: 0.0001

Epoch 91/199
----------
train Loss: 0.3459 acc: 0.0000
val Loss: 0.3842 acc: 0.0000

Epoch 92/199
----------
train Loss: 0.3460 acc: 0.0000
val Loss: 0.3888 acc: 0.0001

Epoch 93/199
----------
train Loss: 0.3478 acc: 0.0000
val Loss: 0.3890 acc: 0.0000

Epoch 94/199
----------
train Loss: 0.3463 acc: 0.0000
val Loss: 0.3897 acc: 0.0000

Epoch 95/199
----------
train Loss: 0.3466 acc: 0.0000
val Loss: 0.3884 acc: 0.0001

Epoch 96/199
----------
train Loss: 0.3459 acc: 0.0000
val Loss: 0.3846 acc: 0.0000

Epoch 97/199
----------
train Loss: 0.3444 acc: 0.0000
val Loss: 0.3832 acc: 0.0000

Epoch 98/199
----------
train Loss: 0.3439 acc: 0.0000
val Loss: 0.3851 acc: 0.0000

Epoch 99/199
----------
train Loss: 0.3433 acc: 0.0000
val Loss: 0.3831 acc: 0.0000

Epoch 100/199
----------
train Loss: 0.3425 acc: 0.0000
val Loss: 0.3837 acc: 0.0000

Epoch 101/199
----------
train Loss: 0.3420 acc: 0.0000
val Loss: 0.3831 acc: 0.0000

Epoch 102/199
----------
train Loss: 0.3415 acc: 0.0000
val Loss: 0.3832 acc: 0.0000

Epoch 103/199
----------
train Loss: 0.3414 acc: 0.0000
val Loss: 0.3823 acc: 0.0000

Epoch 104/199
----------
train Loss: 0.3418 acc: 0.0000
val Loss: 0.3850 acc: 0.0000

Epoch 105/199
----------
train Loss: 0.3420 acc: 0.0000
val Loss: 0.3836 acc: 0.0000

Epoch 106/199
----------
train Loss: 0.3421 acc: 0.0000
val Loss: 0.3846 acc: 0.0001

Epoch 107/199
----------
train Loss: 0.3413 acc: 0.0000
val Loss: 0.3824 acc: 0.0000

Epoch 108/199
----------
train Loss: 0.3415 acc: 0.0000
val Loss: 0.3836 acc: 0.0001

Epoch 109/199
----------
train Loss: 0.3408 acc: 0.0000
val Loss: 0.3828 acc: 0.0000

Epoch 110/199
----------
train Loss: 0.3412 acc: 0.0000
val Loss: 0.3834 acc: 0.0000

Epoch 111/199
----------
train Loss: 0.3406 acc: 0.0000
val Loss: 0.3831 acc: 0.0000

Epoch 112/199
----------
train Loss: 0.3413 acc: 0.0000
val Loss: 0.3850 acc: 0.0001

Epoch 113/199
----------
train Loss: 0.3411 acc: 0.0000
val Loss: 0.3821 acc: 0.0000

Epoch 114/199
----------
train Loss: 0.3406 acc: 0.0000
val Loss: 0.3856 acc: 0.0001

Epoch 115/199
----------
train Loss: 0.3409 acc: 0.0000
val Loss: 0.3844 acc: 0.0001

Epoch 116/199
----------
train Loss: 0.3411 acc: 0.0000
val Loss: 0.3822 acc: 0.0001

Epoch 117/199
----------
train Loss: 0.3404 acc: 0.0000
val Loss: 0.3821 acc: 0.0001

Epoch 118/199
----------
train Loss: 0.3404 acc: 0.0000
val Loss: 0.3852 acc: 0.0001

Epoch 119/199
----------
train Loss: 0.3402 acc: 0.0000
val Loss: 0.3818 acc: 0.0001

Epoch 120/199
----------
train Loss: 0.3401 acc: 0.0000
val Loss: 0.3825 acc: 0.0001

Epoch 121/199
----------
train Loss: 0.3419 acc: 0.0000
val Loss: 0.3891 acc: 0.0001

Epoch 122/199
----------
train Loss: 0.3454 acc: 0.0000
val Loss: 0.3855 acc: 0.0001

Epoch 123/199
----------
train Loss: 0.3430 acc: 0.0000
val Loss: 0.3861 acc: 0.0001

Epoch 124/199
----------
train Loss: 0.3425 acc: 0.0000
val Loss: 0.3841 acc: 0.0001

Epoch 125/199
----------
train Loss: 0.3414 acc: 0.0000
val Loss: 0.3833 acc: 0.0001

Epoch 126/199
----------
train Loss: 0.3414 acc: 0.0000
val Loss: 0.3816 acc: 0.0001

Epoch 127/199
----------
train Loss: 0.3414 acc: 0.0000
val Loss: 0.3841 acc: 0.0001

Epoch 128/199
----------
train Loss: 0.3409 acc: 0.0000
val Loss: 0.3847 acc: 0.0001

Epoch 129/199
----------
train Loss: 0.3405 acc: 0.0000
val Loss: 0.3830 acc: 0.0001

Epoch 130/199
----------
train Loss: 0.3396 acc: 0.0000
val Loss: 0.3827 acc: 0.0001

Epoch 131/199
----------
train Loss: 0.3387 acc: 0.0001
val Loss: 0.3813 acc: 0.0001

Epoch 132/199
----------
train Loss: 0.3390 acc: 0.0000
val Loss: 0.3816 acc: 0.0001

Epoch 133/199
----------
train Loss: 0.3387 acc: 0.0000
val Loss: 0.3818 acc: 0.0001

Epoch 134/199
----------
train Loss: 0.3385 acc: 0.0001
val Loss: 0.3813 acc: 0.0001

Epoch 135/199
----------
train Loss: 0.3383 acc: 0.0000
val Loss: 0.3820 acc: 0.0001

Epoch 136/199
----------
train Loss: 0.3380 acc: 0.0000
val Loss: 0.3803 acc: 0.0001

Epoch 137/199
----------
train Loss: 0.3380 acc: 0.0001
val Loss: 0.3830 acc: 0.0001

Epoch 138/199
----------
train Loss: 0.3384 acc: 0.0001
val Loss: 0.3803 acc: 0.0001

Epoch 139/199
----------
train Loss: 0.3382 acc: 0.0000
val Loss: 0.3810 acc: 0.0001

Epoch 140/199
----------
train Loss: 0.3381 acc: 0.0001
val Loss: 0.3816 acc: 0.0001

Epoch 141/199
----------
train Loss: 0.3384 acc: 0.0001
val Loss: 0.3803 acc: 0.0001

Epoch 142/199
----------
train Loss: 0.3387 acc: 0.0001
val Loss: 0.3839 acc: 0.0001

Epoch 143/199
----------
train Loss: 0.3407 acc: 0.0000
val Loss: 0.3834 acc: 0.0001

Epoch 144/199
----------
train Loss: 0.3401 acc: 0.0000
val Loss: 0.3803 acc: 0.0001

Epoch 145/199
----------
train Loss: 0.3392 acc: 0.0000
val Loss: 0.3822 acc: 0.0001

Epoch 146/199
----------
train Loss: 0.3385 acc: 0.0001
val Loss: 0.3814 acc: 0.0001

Epoch 147/199
----------
train Loss: 0.3384 acc: 0.0000
val Loss: 0.3798 acc: 0.0001

Epoch 148/199
----------
train Loss: 0.3380 acc: 0.0000
val Loss: 0.3812 acc: 0.0001

Epoch 149/199
----------
train Loss: 0.3376 acc: 0.0001
val Loss: 0.3815 acc: 0.0001

Epoch 150/199
----------
train Loss: 0.3372 acc: 0.0001
val Loss: 0.3810 acc: 0.0001

Epoch 151/199
----------
train Loss: 0.3371 acc: 0.0001
val Loss: 0.3812 acc: 0.0001

Epoch 152/199
----------
train Loss: 0.3375 acc: 0.0001
val Loss: 0.3810 acc: 0.0001

Epoch 153/199
----------
train Loss: 0.3371 acc: 0.0001
val Loss: 0.3812 acc: 0.0001

Epoch 154/199
----------
train Loss: 0.3372 acc: 0.0001
val Loss: 0.3819 acc: 0.0001

Epoch 155/199
----------
train Loss: 0.3372 acc: 0.0001
val Loss: 0.3817 acc: 0.0001

Epoch 156/199
----------
train Loss: 0.3374 acc: 0.0001
val Loss: 0.3791 acc: 0.0001

Epoch 157/199
----------
train Loss: 0.3377 acc: 0.0001
val Loss: 0.3803 acc: 0.0001

Epoch 158/199
----------
train Loss: 0.3376 acc: 0.0001
val Loss: 0.3789 acc: 0.0001

Epoch 159/199
----------
train Loss: 0.3383 acc: 0.0001
val Loss: 0.3810 acc: 0.0001

Epoch 160/199
----------
train Loss: 0.3379 acc: 0.0001
val Loss: 0.3805 acc: 0.0001

Epoch 161/199
----------
train Loss: 0.3373 acc: 0.0001
val Loss: 0.3808 acc: 0.0001

Epoch 162/199
----------
train Loss: 0.3367 acc: 0.0001
val Loss: 0.3803 acc: 0.0001

Epoch 163/199
----------
train Loss: 0.3371 acc: 0.0001
val Loss: 0.3817 acc: 0.0001

Epoch 164/199
----------
train Loss: 0.3366 acc: 0.0001
val Loss: 0.3803 acc: 0.0001

Epoch 165/199
----------
train Loss: 0.3368 acc: 0.0001
val Loss: 0.3801 acc: 0.0001

Epoch 166/199
----------
train Loss: 0.3367 acc: 0.0001
val Loss: 0.3812 acc: 0.0001

Epoch 167/199
----------
train Loss: 0.3363 acc: 0.0001
val Loss: 0.3801 acc: 0.0001

Epoch 168/199
----------
train Loss: 0.3357 acc: 0.0001
val Loss: 0.3809 acc: 0.0001

Epoch 169/199
----------
train Loss: 0.3355 acc: 0.0001
val Loss: 0.3796 acc: 0.0001

Epoch 170/199
----------
train Loss: 0.3357 acc: 0.0001
val Loss: 0.3802 acc: 0.0001

Epoch 171/199
----------
train Loss: 0.3359 acc: 0.0001
val Loss: 0.3833 acc: 0.0001

Epoch 172/199
----------
train Loss: 0.3386 acc: 0.0001
val Loss: 0.3801 acc: 0.0001

Epoch 173/199
----------
train Loss: 0.3370 acc: 0.0001
val Loss: 0.3814 acc: 0.0001

Epoch 174/199
----------
train Loss: 0.3362 acc: 0.0001
val Loss: 0.3793 acc: 0.0001

Epoch 175/199
----------
train Loss: 0.3365 acc: 0.0001
val Loss: 0.3798 acc: 0.0001

Epoch 176/199
----------
train Loss: 0.3361 acc: 0.0001
val Loss: 0.3799 acc: 0.0001

Epoch 177/199
----------
train Loss: 0.3358 acc: 0.0001
val Loss: 0.3820 acc: 0.0001

Epoch 178/199
----------
train Loss: 0.3357 acc: 0.0001
val Loss: 0.3786 acc: 0.0001

Epoch 179/199
----------
train Loss: 0.3357 acc: 0.0001
val Loss: 0.3806 acc: 0.0001

Epoch 180/199
----------
train Loss: 0.3358 acc: 0.0001
val Loss: 0.3799 acc: 0.0001

Epoch 181/199
----------
train Loss: 0.3360 acc: 0.0001
val Loss: 0.3798 acc: 0.0001

Epoch 182/199
----------
train Loss: 0.3361 acc: 0.0001
val Loss: 0.3805 acc: 0.0001

Epoch 183/199
----------
train Loss: 0.3355 acc: 0.0001
val Loss: 0.3807 acc: 0.0001

Epoch 184/199
----------
train Loss: 0.3352 acc: 0.0001
val Loss: 0.3789 acc: 0.0001

Epoch 185/199
----------
train Loss: 0.3352 acc: 0.0001
val Loss: 0.3812 acc: 0.0002

Epoch 186/199
----------
train Loss: 0.3362 acc: 0.0001
val Loss: 0.3780 acc: 0.0001

Epoch 187/199
----------
train Loss: 0.3380 acc: 0.0001
val Loss: 0.3834 acc: 0.0001

Epoch 188/199
----------
train Loss: 0.3385 acc: 0.0001
val Loss: 0.3796 acc: 0.0001

Epoch 189/199
----------
train Loss: 0.3368 acc: 0.0001
val Loss: 0.3809 acc: 0.0001

Epoch 190/199
----------
train Loss: 0.3372 acc: 0.0001
val Loss: 0.3795 acc: 0.0002

Epoch 191/199
----------
train Loss: 0.3366 acc: 0.0001
val Loss: 0.3807 acc: 0.0001

Epoch 192/199
----------
train Loss: 0.3363 acc: 0.0001
val Loss: 0.3804 acc: 0.0001

Epoch 193/199
----------
train Loss: 0.3362 acc: 0.0001
val Loss: 0.3792 acc: 0.0001

Epoch 194/199
----------
train Loss: 0.3356 acc: 0.0001
val Loss: 0.3801 acc: 0.0002

Epoch 195/199
----------
train Loss: 0.3350 acc: 0.0001
val Loss: 0.3802 acc: 0.0001

Epoch 196/199
----------
train Loss: 0.3346 acc: 0.0001
val Loss: 0.3802 acc: 0.0001

Epoch 197/199
----------
train Loss: 0.3348 acc: 0.0001
val Loss: 0.3803 acc: 0.0002

Epoch 198/199
----------
train Loss: 0.3349 acc: 0.0001
val Loss: 0.3787 acc: 0.0001

Epoch 199/199
----------
train Loss: 0.3358 acc: 0.0001
val Loss: 0.3806 acc: 0.0002

Training complete in 12m 37s
Best val Acc: 0.377970
Saving..
Successfully retrieved statistics for job: zhangz65-gpu01-2908781_. 
+------------------------------------------------------------------------------+
| GPU ID: 2                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Tue Feb 27 09:12:39 2024                |
| End Time                           | Tue Feb 27 09:25:31 2024                |
| Total Execution Time (sec)         | 772.33                                  |
| No. of Processes                   | 1                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 71537                                   |
| Power Usage (Watts)                | Avg: 98.9374, Max: 144.068, Min: 34.125 |
| Max GPU Memory Used (bytes)        | 5079302144                              |
| SM Clock (MHz)                     | Avg: 1374, Max: 1380, Min: 1230         |
| Memory Clock (MHz)                 | Avg: 877, Max: 877, Min: 877            |
| SM Utilization (%)                 | Avg: 78, Max: 96, Min: 0                |
| Memory Utilization (%)             | Avg: 32, Max: 43, Min: 0                |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 1137235                                 |
|     Avg SM Utilization (%)         | 65                                      |
|     Avg Memory Utilization (%)     | 26                                      |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+

