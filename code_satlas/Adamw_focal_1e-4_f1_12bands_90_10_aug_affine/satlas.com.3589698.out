2.2.0
test
DataLoaders created successfully!
Model(
  (backbone): SwinBackbone(
    (backbone): SwinTransformer(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(12, 128, kernel_size=(4, 4), stride=(4, 4))
          (1): Permute()
          (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (1): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=4, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=4, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): PatchMergingV2(
          (reduction): Linear(in_features=512, out_features=256, bias=False)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (4): PatchMergingV2(
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (6): PatchMergingV2(
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=32, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=32, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.5, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (permute): Permute()
      (avgpool): AdaptiveAvgPool2d(output_size=1)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (head): Linear(in_features=1024, out_features=1000, bias=True)
    )
  )
  (fpn): FPN(
    (fpn): FeaturePyramidNetwork(
      (inner_blocks): ModuleList(
        (0): Conv2dNormActivation(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Conv2dNormActivation(
          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Conv2dNormActivation(
          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Conv2dNormActivation(
          (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (layer_blocks): ModuleList(
        (0-3): 4 x Conv2dNormActivation(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
  )
  (head): SimpleHead(
    (layers): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (1): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
Epoch 0/399
----------
train Loss: 0.3439 Acc: 0.7935 F1: 0.7848
val Loss: 0.0823 Acc: 0.8589 F1: 0.8385

Epoch 1/399
----------
train Loss: 0.0766 Acc: 0.8900 F1: 0.8847
val Loss: 0.0729 Acc: 0.8630 F1: 0.8434

Epoch 2/399
----------
train Loss: 0.0664 Acc: 0.8983 F1: 0.8943
val Loss: 0.0545 Acc: 0.9273 F1: 0.9243

Epoch 3/399
----------
train Loss: 0.0704 Acc: 0.8988 F1: 0.8956
val Loss: 0.0563 Acc: 0.9237 F1: 0.9199

Epoch 4/399
----------
train Loss: 0.0633 Acc: 0.9103 F1: 0.9066
val Loss: 0.0473 Acc: 0.9306 F1: 0.9280

Epoch 5/399
----------
train Loss: 0.0723 Acc: 0.8941 F1: 0.8908
val Loss: 0.0473 Acc: 0.9406 F1: 0.9397

Epoch 6/399
----------
train Loss: 0.0584 Acc: 0.9127 F1: 0.9094
val Loss: 0.0598 Acc: 0.9239 F1: 0.9249

Epoch 7/399
----------
train Loss: 0.0564 Acc: 0.9159 F1: 0.9134
val Loss: 0.0453 Acc: 0.9420 F1: 0.9409

Epoch 8/399
----------
train Loss: 0.0552 Acc: 0.9152 F1: 0.9119
val Loss: 0.0504 Acc: 0.9265 F1: 0.9271

Epoch 9/399
----------
train Loss: 0.0561 Acc: 0.9099 F1: 0.9076
val Loss: 0.0463 Acc: 0.9391 F1: 0.9382

Epoch 10/399
----------
train Loss: 0.0550 Acc: 0.9153 F1: 0.9125
val Loss: 0.0463 Acc: 0.9358 F1: 0.9338

Epoch 11/399
----------
train Loss: 0.0531 Acc: 0.9165 F1: 0.9141
val Loss: 0.0468 Acc: 0.9328 F1: 0.9306

Epoch 12/399
----------
train Loss: 0.0520 Acc: 0.9183 F1: 0.9157
val Loss: 0.0449 Acc: 0.9364 F1: 0.9360

Epoch 13/399
----------
train Loss: 0.0495 Acc: 0.9228 F1: 0.9205
val Loss: 0.0484 Acc: 0.9292 F1: 0.9293

Epoch 14/399
----------
train Loss: 0.0512 Acc: 0.9199 F1: 0.9177
val Loss: 0.0460 Acc: 0.9376 F1: 0.9365

Epoch 15/399
----------
train Loss: 0.0492 Acc: 0.9224 F1: 0.9202
val Loss: 0.0429 Acc: 0.9378 F1: 0.9366

Epoch 16/399
----------
train Loss: 0.0480 Acc: 0.9232 F1: 0.9212
val Loss: 0.0403 Acc: 0.9442 F1: 0.9436

Epoch 17/399
----------
train Loss: 0.0468 Acc: 0.9264 F1: 0.9245
val Loss: 0.0407 Acc: 0.9435 F1: 0.9427

Epoch 18/399
----------
train Loss: 0.0485 Acc: 0.9241 F1: 0.9221
val Loss: 0.0402 Acc: 0.9457 F1: 0.9453

Epoch 19/399
----------
train Loss: 0.0442 Acc: 0.9284 F1: 0.9268
val Loss: 0.0482 Acc: 0.9257 F1: 0.9238

Epoch 20/399
----------
train Loss: 0.0442 Acc: 0.9275 F1: 0.9260
val Loss: 0.0399 Acc: 0.9391 F1: 0.9393

Epoch 21/399
----------
train Loss: 0.0439 Acc: 0.9312 F1: 0.9299
val Loss: 0.0413 Acc: 0.9423 F1: 0.9412

Epoch 22/399
----------
train Loss: 0.0443 Acc: 0.9323 F1: 0.9307
val Loss: 0.0457 Acc: 0.9353 F1: 0.9349

Epoch 23/399
----------
train Loss: 0.0433 Acc: 0.9307 F1: 0.9294
val Loss: 0.0429 Acc: 0.9339 F1: 0.9344

Epoch 24/399
----------
train Loss: 0.0399 Acc: 0.9352 F1: 0.9342
val Loss: 0.0395 Acc: 0.9414 F1: 0.9411

Epoch 25/399
----------
train Loss: 0.0403 Acc: 0.9360 F1: 0.9348
val Loss: 0.0489 Acc: 0.9205 F1: 0.9219

Epoch 26/399
----------
train Loss: 0.0389 Acc: 0.9391 F1: 0.9382
val Loss: 0.0460 Acc: 0.9285 F1: 0.9296

Epoch 27/399
----------
train Loss: 0.0379 Acc: 0.9378 F1: 0.9370
val Loss: 0.0421 Acc: 0.9287 F1: 0.9298

Epoch 28/399
----------
train Loss: 0.0371 Acc: 0.9397 F1: 0.9390
val Loss: 0.0380 Acc: 0.9400 F1: 0.9405

Epoch 29/399
----------
train Loss: 0.0369 Acc: 0.9391 F1: 0.9384
val Loss: 0.0352 Acc: 0.9489 F1: 0.9483

Epoch 30/399
----------
train Loss: 0.0360 Acc: 0.9405 F1: 0.9396
val Loss: 0.0367 Acc: 0.9427 F1: 0.9430

Epoch 31/399
----------
train Loss: 0.0343 Acc: 0.9446 F1: 0.9441
val Loss: 0.0397 Acc: 0.9418 F1: 0.9421

Epoch 32/399
----------
train Loss: 0.0370 Acc: 0.9408 F1: 0.9401
val Loss: 0.0360 Acc: 0.9447 F1: 0.9440

Epoch 33/399
----------
train Loss: 0.0369 Acc: 0.9375 F1: 0.9368
val Loss: 0.0390 Acc: 0.9441 F1: 0.9440

Epoch 34/399
----------
train Loss: 0.0394 Acc: 0.9395 F1: 0.9387
val Loss: 0.0371 Acc: 0.9453 F1: 0.9447

Epoch 35/399
----------
train Loss: 0.0361 Acc: 0.9414 F1: 0.9407
val Loss: 0.0401 Acc: 0.9354 F1: 0.9364

Epoch 36/399
----------
train Loss: 0.0335 Acc: 0.9449 F1: 0.9444
val Loss: 0.0366 Acc: 0.9387 F1: 0.9392

Epoch 37/399
----------
train Loss: 0.0347 Acc: 0.9428 F1: 0.9422
val Loss: 0.0374 Acc: 0.9401 F1: 0.9406

Epoch 38/399
----------
train Loss: 0.0337 Acc: 0.9452 F1: 0.9448
val Loss: 0.0329 Acc: 0.9449 F1: 0.9448

Epoch 39/399
----------
train Loss: 0.0331 Acc: 0.9470 F1: 0.9465
val Loss: 0.0388 Acc: 0.9375 F1: 0.9385

Epoch 40/399
----------
train Loss: 0.0329 Acc: 0.9453 F1: 0.9449
val Loss: 0.0312 Acc: 0.9500 F1: 0.9500

Epoch 41/399
----------
train Loss: 0.0314 Acc: 0.9476 F1: 0.9471
val Loss: 0.0345 Acc: 0.9426 F1: 0.9423

Epoch 42/399
----------
train Loss: 0.0318 Acc: 0.9474 F1: 0.9470
val Loss: 0.0352 Acc: 0.9402 F1: 0.9400

Epoch 43/399
----------
train Loss: 0.0313 Acc: 0.9485 F1: 0.9481
val Loss: 0.0353 Acc: 0.9438 F1: 0.9436

Epoch 44/399
----------
train Loss: 0.0322 Acc: 0.9462 F1: 0.9457
val Loss: 0.0383 Acc: 0.9378 F1: 0.9386

Epoch 45/399
----------
train Loss: 0.0304 Acc: 0.9500 F1: 0.9497
val Loss: 0.0351 Acc: 0.9411 F1: 0.9418

Epoch 46/399
----------
train Loss: 0.0303 Acc: 0.9492 F1: 0.9489
val Loss: 0.0325 Acc: 0.9446 F1: 0.9457

Epoch 47/399
----------
train Loss: 0.0292 Acc: 0.9516 F1: 0.9514
val Loss: 0.0318 Acc: 0.9469 F1: 0.9472

Epoch 48/399
----------
train Loss: 0.0292 Acc: 0.9521 F1: 0.9519
val Loss: 0.0314 Acc: 0.9483 F1: 0.9485

Epoch 49/399
----------
train Loss: 0.0271 Acc: 0.9554 F1: 0.9552
val Loss: 0.0358 Acc: 0.9411 F1: 0.9422

Epoch 50/399
----------
train Loss: 0.0267 Acc: 0.9560 F1: 0.9559
val Loss: 0.0313 Acc: 0.9484 F1: 0.9485

Epoch 51/399
----------
train Loss: 0.0262 Acc: 0.9569 F1: 0.9568
val Loss: 0.0309 Acc: 0.9495 F1: 0.9500

Epoch 52/399
----------
train Loss: 0.0281 Acc: 0.9538 F1: 0.9536
val Loss: 0.0348 Acc: 0.9460 F1: 0.9464

Epoch 53/399
----------
train Loss: 0.0260 Acc: 0.9563 F1: 0.9561
val Loss: 0.0376 Acc: 0.9451 F1: 0.9457

Epoch 54/399
----------
train Loss: 0.0277 Acc: 0.9554 F1: 0.9552
val Loss: 0.0358 Acc: 0.9364 F1: 0.9379

Epoch 55/399
----------
train Loss: 0.0285 Acc: 0.9538 F1: 0.9536
val Loss: 0.0321 Acc: 0.9478 F1: 0.9486

Epoch 56/399
----------
train Loss: 0.0262 Acc: 0.9573 F1: 0.9571
val Loss: 0.0323 Acc: 0.9432 F1: 0.9441

Epoch 57/399
----------
train Loss: 0.0253 Acc: 0.9591 F1: 0.9589
val Loss: 0.0336 Acc: 0.9483 F1: 0.9484

Epoch 58/399
----------
train Loss: 0.0249 Acc: 0.9590 F1: 0.9589
val Loss: 0.0314 Acc: 0.9498 F1: 0.9502

Epoch 59/399
----------
train Loss: 0.0242 Acc: 0.9598 F1: 0.9597
val Loss: 0.0305 Acc: 0.9494 F1: 0.9499

Epoch 60/399
----------
train Loss: 0.0236 Acc: 0.9612 F1: 0.9612
val Loss: 0.0294 Acc: 0.9498 F1: 0.9503

Epoch 61/399
----------
train Loss: 0.0282 Acc: 0.9537 F1: 0.9535
val Loss: 0.0332 Acc: 0.9441 F1: 0.9451

Epoch 62/399
----------
train Loss: 0.0256 Acc: 0.9585 F1: 0.9583
val Loss: 0.0347 Acc: 0.9443 F1: 0.9448

Epoch 63/399
----------
train Loss: 0.0250 Acc: 0.9580 F1: 0.9578
val Loss: 0.0338 Acc: 0.9527 F1: 0.9528

Epoch 64/399
----------
train Loss: 0.0241 Acc: 0.9606 F1: 0.9605
val Loss: 0.0270 Acc: 0.9569 F1: 0.9567

Epoch 65/399
----------
train Loss: 0.0257 Acc: 0.9587 F1: 0.9586
val Loss: 0.0311 Acc: 0.9532 F1: 0.9535

Epoch 66/399
----------
train Loss: 0.0253 Acc: 0.9594 F1: 0.9592
val Loss: 0.0294 Acc: 0.9515 F1: 0.9513

Epoch 67/399
----------
train Loss: 0.0244 Acc: 0.9609 F1: 0.9607
val Loss: 0.0278 Acc: 0.9549 F1: 0.9551

Epoch 68/399
----------
train Loss: 0.0233 Acc: 0.9618 F1: 0.9618
val Loss: 0.0307 Acc: 0.9517 F1: 0.9521

Epoch 69/399
----------
train Loss: 0.0224 Acc: 0.9634 F1: 0.9633
val Loss: 0.0289 Acc: 0.9528 F1: 0.9528

Epoch 70/399
----------
train Loss: 0.0252 Acc: 0.9586 F1: 0.9584
val Loss: 0.0496 Acc: 0.9237 F1: 0.9244

Epoch 71/399
----------
train Loss: 0.0431 Acc: 0.9294 F1: 0.9283
val Loss: 0.0373 Acc: 0.9360 F1: 0.9368

Epoch 72/399
----------
train Loss: 0.0466 Acc: 0.9315 F1: 0.9310
val Loss: 0.0412 Acc: 0.9364 F1: 0.9366

Epoch 73/399
----------
train Loss: 0.0350 Acc: 0.9453 F1: 0.9447
val Loss: 0.0316 Acc: 0.9516 F1: 0.9511

Epoch 74/399
----------
train Loss: 0.0291 Acc: 0.9535 F1: 0.9532
val Loss: 0.0328 Acc: 0.9453 F1: 0.9450

Epoch 75/399
----------
train Loss: 0.0256 Acc: 0.9591 F1: 0.9589
val Loss: 0.0297 Acc: 0.9529 F1: 0.9529

Epoch 76/399
----------
train Loss: 0.0241 Acc: 0.9609 F1: 0.9607
val Loss: 0.0293 Acc: 0.9542 F1: 0.9543

Epoch 77/399
----------
train Loss: 0.0234 Acc: 0.9617 F1: 0.9616
val Loss: 0.0282 Acc: 0.9578 F1: 0.9581

Epoch 78/399
----------
train Loss: 0.0221 Acc: 0.9639 F1: 0.9638
val Loss: 0.0313 Acc: 0.9511 F1: 0.9517

Epoch 79/399
----------
train Loss: 0.0224 Acc: 0.9627 F1: 0.9626
val Loss: 0.0285 Acc: 0.9558 F1: 0.9563

Epoch 80/399
----------
train Loss: 0.0287 Acc: 0.9539 F1: 0.9536
val Loss: 0.0340 Acc: 0.9504 F1: 0.9510

Epoch 81/399
----------
train Loss: 0.0243 Acc: 0.9602 F1: 0.9601
val Loss: 0.0279 Acc: 0.9578 F1: 0.9579

Epoch 82/399
----------
train Loss: 0.0225 Acc: 0.9638 F1: 0.9637
val Loss: 0.0292 Acc: 0.9528 F1: 0.9532

Epoch 83/399
----------
train Loss: 0.0215 Acc: 0.9653 F1: 0.9652
val Loss: 0.0332 Acc: 0.9467 F1: 0.9475

Epoch 84/399
----------
train Loss: 0.0211 Acc: 0.9650 F1: 0.9649
val Loss: 0.0300 Acc: 0.9507 F1: 0.9514

Epoch 85/399
----------
train Loss: 0.0203 Acc: 0.9667 F1: 0.9666
val Loss: 0.0287 Acc: 0.9543 F1: 0.9543

Epoch 86/399
----------
train Loss: 0.0200 Acc: 0.9675 F1: 0.9674
val Loss: 0.0291 Acc: 0.9540 F1: 0.9538

Epoch 87/399
----------
train Loss: 0.0209 Acc: 0.9654 F1: 0.9654
val Loss: 0.0260 Acc: 0.9576 F1: 0.9575

Epoch 88/399
----------
train Loss: 0.0208 Acc: 0.9657 F1: 0.9656
val Loss: 0.0318 Acc: 0.9545 F1: 0.9548

Epoch 89/399
----------
train Loss: 0.0223 Acc: 0.9634 F1: 0.9633
val Loss: 0.0293 Acc: 0.9518 F1: 0.9520

Epoch 90/399
----------
train Loss: 0.0198 Acc: 0.9671 F1: 0.9670
val Loss: 0.0269 Acc: 0.9575 F1: 0.9577

Epoch 91/399
----------
train Loss: 0.0189 Acc: 0.9690 F1: 0.9689
val Loss: 0.0265 Acc: 0.9583 F1: 0.9582

Epoch 92/399
----------
train Loss: 0.0190 Acc: 0.9690 F1: 0.9689
val Loss: 0.0316 Acc: 0.9535 F1: 0.9536

Epoch 93/399
----------
train Loss: 0.0190 Acc: 0.9687 F1: 0.9686
val Loss: 0.0322 Acc: 0.9558 F1: 0.9561

Epoch 94/399
----------
train Loss: 0.0187 Acc: 0.9689 F1: 0.9688
val Loss: 0.0305 Acc: 0.9546 F1: 0.9549

Epoch 95/399
----------
train Loss: 0.0201 Acc: 0.9673 F1: 0.9673
val Loss: 0.0297 Acc: 0.9544 F1: 0.9551

Epoch 96/399
----------
train Loss: 0.0186 Acc: 0.9690 F1: 0.9690
val Loss: 0.0276 Acc: 0.9579 F1: 0.9580

Epoch 97/399
----------
train Loss: 0.0184 Acc: 0.9699 F1: 0.9699
val Loss: 0.0285 Acc: 0.9569 F1: 0.9570

Epoch 98/399
----------
train Loss: 0.0182 Acc: 0.9699 F1: 0.9699
val Loss: 0.0249 Acc: 0.9592 F1: 0.9592

Epoch 99/399
----------
train Loss: 0.0181 Acc: 0.9701 F1: 0.9701
val Loss: 0.0351 Acc: 0.9526 F1: 0.9531

Epoch 100/399
----------
train Loss: 0.0188 Acc: 0.9689 F1: 0.9689
val Loss: 0.0260 Acc: 0.9600 F1: 0.9601

Epoch 101/399
----------
train Loss: 0.0182 Acc: 0.9701 F1: 0.9700
val Loss: 0.0279 Acc: 0.9586 F1: 0.9587

Epoch 102/399
----------
train Loss: 0.0180 Acc: 0.9701 F1: 0.9701
val Loss: 0.0315 Acc: 0.9527 F1: 0.9530

Epoch 103/399
----------
train Loss: 0.0181 Acc: 0.9700 F1: 0.9699
val Loss: 0.0301 Acc: 0.9539 F1: 0.9538

Epoch 104/399
----------
train Loss: 0.0181 Acc: 0.9708 F1: 0.9707
val Loss: 0.0299 Acc: 0.9566 F1: 0.9569

Epoch 105/399
----------
train Loss: 0.0178 Acc: 0.9712 F1: 0.9711
val Loss: 0.0307 Acc: 0.9526 F1: 0.9530

Epoch 106/399
----------
train Loss: 0.0184 Acc: 0.9705 F1: 0.9704
val Loss: 0.0262 Acc: 0.9587 F1: 0.9587

Epoch 107/399
----------
train Loss: 0.0259 Acc: 0.9622 F1: 0.9620
val Loss: 0.0331 Acc: 0.9497 F1: 0.9502

Epoch 108/399
----------
train Loss: 0.0207 Acc: 0.9664 F1: 0.9663
val Loss: 0.0254 Acc: 0.9605 F1: 0.9603

Epoch 109/399
----------
train Loss: 0.0192 Acc: 0.9693 F1: 0.9692
val Loss: 0.0285 Acc: 0.9556 F1: 0.9559

Epoch 110/399
----------
train Loss: 0.0191 Acc: 0.9692 F1: 0.9691
val Loss: 0.0274 Acc: 0.9553 F1: 0.9553

Epoch 111/399
----------
train Loss: 0.0219 Acc: 0.9652 F1: 0.9650
val Loss: 0.0502 Acc: 0.9372 F1: 0.9386

Epoch 112/399
----------
train Loss: 0.0239 Acc: 0.9628 F1: 0.9626
val Loss: 0.0319 Acc: 0.9525 F1: 0.9529

Epoch 113/399
----------
train Loss: 0.0195 Acc: 0.9688 F1: 0.9687
val Loss: 0.0431 Acc: 0.9317 F1: 0.9333

Epoch 114/399
----------
train Loss: 0.0213 Acc: 0.9656 F1: 0.9656
val Loss: 0.0335 Acc: 0.9467 F1: 0.9473

Epoch 115/399
----------
train Loss: 0.0189 Acc: 0.9694 F1: 0.9694
val Loss: 0.0341 Acc: 0.9501 F1: 0.9504

Epoch 116/399
----------
train Loss: 0.0178 Acc: 0.9714 F1: 0.9714
val Loss: 0.0321 Acc: 0.9537 F1: 0.9541

Epoch 117/399
----------
train Loss: 0.0168 Acc: 0.9723 F1: 0.9723
val Loss: 0.0350 Acc: 0.9507 F1: 0.9514

Epoch 118/399
----------
train Loss: 0.0168 Acc: 0.9728 F1: 0.9728
val Loss: 0.0324 Acc: 0.9513 F1: 0.9515

Epoch 119/399
----------
train Loss: 0.0168 Acc: 0.9725 F1: 0.9725
val Loss: 0.0303 Acc: 0.9524 F1: 0.9529

Epoch 120/399
----------
train Loss: 0.0162 Acc: 0.9736 F1: 0.9735
val Loss: 0.0323 Acc: 0.9596 F1: 0.9597

Epoch 121/399
----------
train Loss: 0.0164 Acc: 0.9730 F1: 0.9730
val Loss: 0.0331 Acc: 0.9556 F1: 0.9562

Epoch 122/399
----------
train Loss: 0.0273 Acc: 0.9606 F1: 0.9605
val Loss: 0.0368 Acc: 0.9497 F1: 0.9495

Epoch 123/399
----------
train Loss: 0.0245 Acc: 0.9623 F1: 0.9622
val Loss: 0.0331 Acc: 0.9492 F1: 0.9497

Epoch 124/399
----------
train Loss: 0.0197 Acc: 0.9684 F1: 0.9684
val Loss: 0.0293 Acc: 0.9532 F1: 0.9536

Epoch 125/399
----------
train Loss: 0.0173 Acc: 0.9723 F1: 0.9722
val Loss: 0.0360 Acc: 0.9500 F1: 0.9504

Epoch 126/399
----------
train Loss: 0.0173 Acc: 0.9716 F1: 0.9716
val Loss: 0.0341 Acc: 0.9508 F1: 0.9512

Epoch 127/399
----------
train Loss: 0.0168 Acc: 0.9726 F1: 0.9726
val Loss: 0.0288 Acc: 0.9599 F1: 0.9599

Epoch 128/399
----------
train Loss: 0.0160 Acc: 0.9740 F1: 0.9740
val Loss: 0.0338 Acc: 0.9523 F1: 0.9529

Epoch 129/399
----------
train Loss: 0.0166 Acc: 0.9730 F1: 0.9729
val Loss: 0.0323 Acc: 0.9571 F1: 0.9571

Epoch 130/399
----------
train Loss: 0.0153 Acc: 0.9751 F1: 0.9751
val Loss: 0.0323 Acc: 0.9543 F1: 0.9545

Epoch 131/399
----------
train Loss: 0.0153 Acc: 0.9754 F1: 0.9754
val Loss: 0.0305 Acc: 0.9576 F1: 0.9577

Epoch 132/399
----------
train Loss: 0.0156 Acc: 0.9746 F1: 0.9746
val Loss: 0.0272 Acc: 0.9582 F1: 0.9585

Epoch 133/399
----------
train Loss: 0.0159 Acc: 0.9743 F1: 0.9743
val Loss: 0.0288 Acc: 0.9589 F1: 0.9590

Epoch 134/399
----------
train Loss: 0.0150 Acc: 0.9757 F1: 0.9757
val Loss: 0.0306 Acc: 0.9577 F1: 0.9580

Epoch 135/399
----------
train Loss: 0.0158 Acc: 0.9741 F1: 0.9741
val Loss: 0.0256 Acc: 0.9643 F1: 0.9642

Epoch 136/399
----------
train Loss: 0.0157 Acc: 0.9743 F1: 0.9743
val Loss: 0.0270 Acc: 0.9598 F1: 0.9601

Epoch 137/399
----------
train Loss: 0.0181 Acc: 0.9707 F1: 0.9707
val Loss: 0.0242 Acc: 0.9662 F1: 0.9662

Epoch 138/399
----------
train Loss: 0.0155 Acc: 0.9748 F1: 0.9747
val Loss: 0.0262 Acc: 0.9634 F1: 0.9634

Epoch 139/399
----------
train Loss: 0.0154 Acc: 0.9750 F1: 0.9750
val Loss: 0.0290 Acc: 0.9551 F1: 0.9554

Epoch 140/399
----------
train Loss: 0.0159 Acc: 0.9741 F1: 0.9741
val Loss: 0.0309 Acc: 0.9574 F1: 0.9575

Epoch 141/399
----------
train Loss: 0.0153 Acc: 0.9753 F1: 0.9753
val Loss: 0.0308 Acc: 0.9589 F1: 0.9592

Epoch 142/399
----------
train Loss: 0.0158 Acc: 0.9750 F1: 0.9750
val Loss: 0.0301 Acc: 0.9577 F1: 0.9577

Epoch 143/399
----------
train Loss: 0.0151 Acc: 0.9757 F1: 0.9757
val Loss: 0.0290 Acc: 0.9571 F1: 0.9574

Epoch 144/399
----------
train Loss: 0.0148 Acc: 0.9758 F1: 0.9757
val Loss: 0.0268 Acc: 0.9594 F1: 0.9594

Epoch 145/399
----------
train Loss: 0.0143 Acc: 0.9768 F1: 0.9768
val Loss: 0.0354 Acc: 0.9535 F1: 0.9540

Epoch 146/399
----------
train Loss: 0.0143 Acc: 0.9767 F1: 0.9767
val Loss: 0.0275 Acc: 0.9608 F1: 0.9608

Epoch 147/399
----------
train Loss: 0.0145 Acc: 0.9760 F1: 0.9760
val Loss: 0.0263 Acc: 0.9618 F1: 0.9619

Epoch 148/399
----------
train Loss: 0.0147 Acc: 0.9761 F1: 0.9761
val Loss: 0.0305 Acc: 0.9586 F1: 0.9588

Epoch 149/399
----------
train Loss: 0.0147 Acc: 0.9762 F1: 0.9762
val Loss: 0.0290 Acc: 0.9603 F1: 0.9601

Epoch 150/399
----------
train Loss: 0.0144 Acc: 0.9770 F1: 0.9770
val Loss: 0.0289 Acc: 0.9591 F1: 0.9593

Epoch 151/399
----------
train Loss: 0.0139 Acc: 0.9774 F1: 0.9773
val Loss: 0.0329 Acc: 0.9542 F1: 0.9546

Epoch 152/399
----------
train Loss: 0.0144 Acc: 0.9769 F1: 0.9769
val Loss: 0.0344 Acc: 0.9537 F1: 0.9543

Epoch 153/399
----------
train Loss: 0.0145 Acc: 0.9765 F1: 0.9765
val Loss: 0.0312 Acc: 0.9577 F1: 0.9580

Epoch 154/399
----------
train Loss: 0.0139 Acc: 0.9775 F1: 0.9775
val Loss: 0.0308 Acc: 0.9590 F1: 0.9593

Epoch 155/399
----------
train Loss: 0.0138 Acc: 0.9773 F1: 0.9773
val Loss: 0.0284 Acc: 0.9602 F1: 0.9605

Epoch 156/399
----------
train Loss: 0.0132 Acc: 0.9783 F1: 0.9783
val Loss: 0.0301 Acc: 0.9597 F1: 0.9596

Epoch 157/399
----------
train Loss: 0.0137 Acc: 0.9776 F1: 0.9776
val Loss: 0.0311 Acc: 0.9610 F1: 0.9611

Epoch 158/399
----------
train Loss: 0.0127 Acc: 0.9793 F1: 0.9793
val Loss: 0.0343 Acc: 0.9571 F1: 0.9575

Epoch 159/399
----------
train Loss: 0.0145 Acc: 0.9767 F1: 0.9766
val Loss: 0.0271 Acc: 0.9639 F1: 0.9640

Epoch 160/399
----------
train Loss: 0.0139 Acc: 0.9776 F1: 0.9776
val Loss: 0.0290 Acc: 0.9598 F1: 0.9600

Epoch 161/399
----------
train Loss: 0.0140 Acc: 0.9773 F1: 0.9773
val Loss: 0.0320 Acc: 0.9569 F1: 0.9571

Epoch 162/399
----------
train Loss: 0.0156 Acc: 0.9751 F1: 0.9750
val Loss: 0.0328 Acc: 0.9532 F1: 0.9534

Epoch 163/399
----------
train Loss: 0.0144 Acc: 0.9767 F1: 0.9766
val Loss: 0.0317 Acc: 0.9582 F1: 0.9585

Epoch 164/399
----------
train Loss: 0.0186 Acc: 0.9700 F1: 0.9700
val Loss: 0.0336 Acc: 0.9556 F1: 0.9555

Epoch 165/399
----------
train Loss: 0.0190 Acc: 0.9693 F1: 0.9692
val Loss: 0.0383 Acc: 0.9484 F1: 0.9493

Epoch 166/399
----------
train Loss: 0.0198 Acc: 0.9690 F1: 0.9690
val Loss: 0.0323 Acc: 0.9499 F1: 0.9502

Epoch 167/399
----------
train Loss: 0.0196 Acc: 0.9703 F1: 0.9703
val Loss: 0.0350 Acc: 0.9510 F1: 0.9515

Epoch 168/399
----------
train Loss: 0.0158 Acc: 0.9746 F1: 0.9745
val Loss: 0.0331 Acc: 0.9587 F1: 0.9588

Epoch 169/399
----------
train Loss: 0.0150 Acc: 0.9762 F1: 0.9762
val Loss: 0.0305 Acc: 0.9578 F1: 0.9581

Epoch 170/399
----------
train Loss: 0.0138 Acc: 0.9779 F1: 0.9779
val Loss: 0.0314 Acc: 0.9583 F1: 0.9586

Epoch 171/399
----------
train Loss: 0.0141 Acc: 0.9774 F1: 0.9774
val Loss: 0.0271 Acc: 0.9592 F1: 0.9593

Epoch 172/399
----------
train Loss: 0.0139 Acc: 0.9776 F1: 0.9776
val Loss: 0.0231 Acc: 0.9637 F1: 0.9641

Epoch 173/399
----------
train Loss: 0.0135 Acc: 0.9782 F1: 0.9782
val Loss: 0.0251 Acc: 0.9617 F1: 0.9620

Epoch 174/399
----------
train Loss: 0.0131 Acc: 0.9786 F1: 0.9786
val Loss: 0.0274 Acc: 0.9645 F1: 0.9646

Epoch 175/399
----------
train Loss: 0.0125 Acc: 0.9797 F1: 0.9796
val Loss: 0.0334 Acc: 0.9584 F1: 0.9586

Epoch 176/399
----------
train Loss: 0.0131 Acc: 0.9788 F1: 0.9787
val Loss: 0.0311 Acc: 0.9588 F1: 0.9589

Epoch 177/399
----------
train Loss: 0.0129 Acc: 0.9791 F1: 0.9790
val Loss: 0.0360 Acc: 0.9579 F1: 0.9582

Epoch 178/399
----------
train Loss: 0.0127 Acc: 0.9796 F1: 0.9795
val Loss: 0.0323 Acc: 0.9600 F1: 0.9602

Epoch 179/399
----------
train Loss: 0.0123 Acc: 0.9801 F1: 0.9801
val Loss: 0.0292 Acc: 0.9603 F1: 0.9605

Epoch 180/399
----------
train Loss: 0.0123 Acc: 0.9801 F1: 0.9801
val Loss: 0.0317 Acc: 0.9613 F1: 0.9615

Epoch 181/399
----------
train Loss: 0.0124 Acc: 0.9796 F1: 0.9796
val Loss: 0.0308 Acc: 0.9595 F1: 0.9597

Epoch 182/399
----------
train Loss: 0.0119 Acc: 0.9807 F1: 0.9807
val Loss: 0.0253 Acc: 0.9605 F1: 0.9609

Epoch 183/399
----------
train Loss: 0.0139 Acc: 0.9776 F1: 0.9776
val Loss: 0.0319 Acc: 0.9574 F1: 0.9575

Epoch 184/399
----------
train Loss: 0.0131 Acc: 0.9790 F1: 0.9790
val Loss: 0.0374 Acc: 0.9593 F1: 0.9589

Epoch 185/399
----------
train Loss: 0.0126 Acc: 0.9794 F1: 0.9794
val Loss: 0.0258 Acc: 0.9643 F1: 0.9644

Epoch 186/399
----------
train Loss: 0.0120 Acc: 0.9805 F1: 0.9805
val Loss: 0.0319 Acc: 0.9632 F1: 0.9633

Epoch 187/399
----------
train Loss: 0.0130 Acc: 0.9791 F1: 0.9791
val Loss: 0.0328 Acc: 0.9631 F1: 0.9630

Epoch 188/399
----------
train Loss: 0.0122 Acc: 0.9804 F1: 0.9804
val Loss: 0.0343 Acc: 0.9587 F1: 0.9589

Epoch 189/399
----------
train Loss: 0.0116 Acc: 0.9811 F1: 0.9811
val Loss: 0.0346 Acc: 0.9578 F1: 0.9582

Epoch 190/399
----------
train Loss: 0.0127 Acc: 0.9796 F1: 0.9796
val Loss: 0.0322 Acc: 0.9594 F1: 0.9598

Epoch 191/399
----------
train Loss: 0.0117 Acc: 0.9810 F1: 0.9809
val Loss: 0.0270 Acc: 0.9680 F1: 0.9681

Epoch 192/399
----------
train Loss: 0.0118 Acc: 0.9808 F1: 0.9808
val Loss: 0.0320 Acc: 0.9605 F1: 0.9608

Epoch 193/399
----------
train Loss: 0.0116 Acc: 0.9811 F1: 0.9811
val Loss: 0.0265 Acc: 0.9672 F1: 0.9672

Epoch 194/399
----------
train Loss: 0.0116 Acc: 0.9811 F1: 0.9811
val Loss: 0.0281 Acc: 0.9616 F1: 0.9618

Epoch 195/399
----------
train Loss: 0.0116 Acc: 0.9812 F1: 0.9812
val Loss: 0.0418 Acc: 0.9542 F1: 0.9547

Epoch 196/399
----------
train Loss: 0.0147 Acc: 0.9762 F1: 0.9762
val Loss: 0.0299 Acc: 0.9582 F1: 0.9585

Epoch 197/399
----------
train Loss: 0.0130 Acc: 0.9790 F1: 0.9790
val Loss: 0.0247 Acc: 0.9657 F1: 0.9658

Epoch 198/399
----------
train Loss: 0.0117 Acc: 0.9812 F1: 0.9812
val Loss: 0.0289 Acc: 0.9613 F1: 0.9616

Epoch 199/399
----------
train Loss: 0.0120 Acc: 0.9806 F1: 0.9806
val Loss: 0.0256 Acc: 0.9634 F1: 0.9636

Epoch 200/399
----------
train Loss: 0.0118 Acc: 0.9808 F1: 0.9807
val Loss: 0.0311 Acc: 0.9615 F1: 0.9615

Epoch 201/399
----------
train Loss: 0.0116 Acc: 0.9814 F1: 0.9814
val Loss: 0.0269 Acc: 0.9640 F1: 0.9643

Epoch 202/399
----------
train Loss: 0.0118 Acc: 0.9808 F1: 0.9808
val Loss: 0.0269 Acc: 0.9656 F1: 0.9656

Epoch 203/399
----------
train Loss: 0.0120 Acc: 0.9807 F1: 0.9806
val Loss: 0.0352 Acc: 0.9606 F1: 0.9609

Epoch 204/399
----------
train Loss: 0.0115 Acc: 0.9815 F1: 0.9815
val Loss: 0.0334 Acc: 0.9618 F1: 0.9619

Epoch 205/399
----------
train Loss: 0.0111 Acc: 0.9822 F1: 0.9822
val Loss: 0.0249 Acc: 0.9661 F1: 0.9661

Epoch 206/399
----------
train Loss: 0.0116 Acc: 0.9811 F1: 0.9811
val Loss: 0.0290 Acc: 0.9613 F1: 0.9614

Epoch 207/399
----------
train Loss: 0.0113 Acc: 0.9814 F1: 0.9814
val Loss: 0.0250 Acc: 0.9691 F1: 0.9692

Epoch 208/399
----------
train Loss: 0.0112 Acc: 0.9819 F1: 0.9819
val Loss: 0.0238 Acc: 0.9657 F1: 0.9658

Epoch 209/399
----------
train Loss: 0.0111 Acc: 0.9820 F1: 0.9820
val Loss: 0.0264 Acc: 0.9654 F1: 0.9656

Epoch 210/399
----------
train Loss: 0.0109 Acc: 0.9824 F1: 0.9824
val Loss: 0.0363 Acc: 0.9624 F1: 0.9626

Epoch 211/399
----------
train Loss: 0.0106 Acc: 0.9827 F1: 0.9827
val Loss: 0.0271 Acc: 0.9654 F1: 0.9654

Epoch 212/399
----------
train Loss: 0.0111 Acc: 0.9818 F1: 0.9818
val Loss: 0.0360 Acc: 0.9577 F1: 0.9581

Epoch 213/399
----------
train Loss: 0.0121 Acc: 0.9804 F1: 0.9804
val Loss: 0.0284 Acc: 0.9664 F1: 0.9663

Epoch 214/399
----------
train Loss: 0.0116 Acc: 0.9814 F1: 0.9814
val Loss: 0.0269 Acc: 0.9660 F1: 0.9661

Epoch 215/399
----------
train Loss: 0.0110 Acc: 0.9823 F1: 0.9823
val Loss: 0.0399 Acc: 0.9556 F1: 0.9562

Epoch 216/399
----------
train Loss: 0.0112 Acc: 0.9818 F1: 0.9817
val Loss: 0.0372 Acc: 0.9573 F1: 0.9578

Epoch 217/399
----------
train Loss: 0.0115 Acc: 0.9812 F1: 0.9812
val Loss: 0.0321 Acc: 0.9626 F1: 0.9626

Epoch 218/399
----------
train Loss: 0.0109 Acc: 0.9822 F1: 0.9822
val Loss: 0.0278 Acc: 0.9666 F1: 0.9666

Epoch 219/399
----------
train Loss: 0.0105 Acc: 0.9829 F1: 0.9829
val Loss: 0.0381 Acc: 0.9580 F1: 0.9578

Epoch 220/399
----------
train Loss: 0.0103 Acc: 0.9831 F1: 0.9831
val Loss: 0.0339 Acc: 0.9592 F1: 0.9595

Epoch 221/399
----------
train Loss: 0.0107 Acc: 0.9826 F1: 0.9826
val Loss: 0.0260 Acc: 0.9689 F1: 0.9688

Epoch 222/399
----------
train Loss: 0.0108 Acc: 0.9825 F1: 0.9825
val Loss: 0.0355 Acc: 0.9609 F1: 0.9611

Epoch 223/399
----------
train Loss: 0.0108 Acc: 0.9825 F1: 0.9825
val Loss: 0.0272 Acc: 0.9666 F1: 0.9666

Epoch 224/399
----------
train Loss: 0.0107 Acc: 0.9826 F1: 0.9826
val Loss: 0.0308 Acc: 0.9594 F1: 0.9597

Epoch 225/399
----------
train Loss: 0.0114 Acc: 0.9816 F1: 0.9816
val Loss: 0.0294 Acc: 0.9672 F1: 0.9670

Epoch 226/399
----------
train Loss: 0.0110 Acc: 0.9823 F1: 0.9823
val Loss: 0.0382 Acc: 0.9575 F1: 0.9578

Epoch 227/399
----------
train Loss: 0.0106 Acc: 0.9831 F1: 0.9831
val Loss: 0.0303 Acc: 0.9612 F1: 0.9615

Epoch 228/399
----------
train Loss: 0.0110 Acc: 0.9821 F1: 0.9821
val Loss: 0.0357 Acc: 0.9598 F1: 0.9600

Epoch 229/399
----------
train Loss: 0.0101 Acc: 0.9837 F1: 0.9837
val Loss: 0.0345 Acc: 0.9578 F1: 0.9582

Epoch 230/399
----------
train Loss: 0.0104 Acc: 0.9832 F1: 0.9832
val Loss: 0.0319 Acc: 0.9602 F1: 0.9605

Epoch 231/399
----------
train Loss: 0.0104 Acc: 0.9832 F1: 0.9832
val Loss: 0.0272 Acc: 0.9660 F1: 0.9660

Epoch 232/399
----------
train Loss: 0.0102 Acc: 0.9834 F1: 0.9834
val Loss: 0.0236 Acc: 0.9695 F1: 0.9695

Epoch 233/399
----------
train Loss: 0.0100 Acc: 0.9837 F1: 0.9836
val Loss: 0.0395 Acc: 0.9571 F1: 0.9574

Epoch 234/399
----------
train Loss: 0.0109 Acc: 0.9828 F1: 0.9827
val Loss: 0.0368 Acc: 0.9595 F1: 0.9597

Epoch 235/399
----------
train Loss: 0.0113 Acc: 0.9820 F1: 0.9820
val Loss: 0.0409 Acc: 0.9588 F1: 0.9590

Epoch 236/399
----------
train Loss: 0.0101 Acc: 0.9839 F1: 0.9839
val Loss: 0.0343 Acc: 0.9588 F1: 0.9593

Epoch 237/399
----------
train Loss: 0.0105 Acc: 0.9830 F1: 0.9830
val Loss: 0.0339 Acc: 0.9637 F1: 0.9638

Epoch 238/399
----------
train Loss: 0.0106 Acc: 0.9829 F1: 0.9829
val Loss: 0.0339 Acc: 0.9587 F1: 0.9590

Epoch 239/399
----------
train Loss: 0.0184 Acc: 0.9750 F1: 0.9750
val Loss: 0.0451 Acc: 0.9140 F1: 0.9088

Epoch 240/399
----------
train Loss: 0.0220 Acc: 0.9663 F1: 0.9662
val Loss: 0.0252 Acc: 0.9617 F1: 0.9621

Epoch 241/399
----------
train Loss: 0.0164 Acc: 0.9748 F1: 0.9747
val Loss: 0.0330 Acc: 0.9544 F1: 0.9548

Epoch 242/399
----------
train Loss: 0.0160 Acc: 0.9748 F1: 0.9748
val Loss: 0.0332 Acc: 0.9564 F1: 0.9565

Epoch 243/399
----------
train Loss: 0.0135 Acc: 0.9786 F1: 0.9786
val Loss: 0.0315 Acc: 0.9565 F1: 0.9570

Epoch 244/399
----------
train Loss: 0.0118 Acc: 0.9812 F1: 0.9811
val Loss: 0.0320 Acc: 0.9571 F1: 0.9577

Epoch 245/399
----------
train Loss: 0.0111 Acc: 0.9823 F1: 0.9823
val Loss: 0.0330 Acc: 0.9584 F1: 0.9587

Epoch 246/399
----------
train Loss: 0.0107 Acc: 0.9827 F1: 0.9827
val Loss: 0.0319 Acc: 0.9586 F1: 0.9589

Epoch 247/399
----------
train Loss: 0.0106 Acc: 0.9829 F1: 0.9829
val Loss: 0.0334 Acc: 0.9584 F1: 0.9585

Epoch 248/399
----------
train Loss: 0.0105 Acc: 0.9829 F1: 0.9829
val Loss: 0.0317 Acc: 0.9611 F1: 0.9612

Epoch 249/399
----------
train Loss: 0.0104 Acc: 0.9831 F1: 0.9831
val Loss: 0.0314 Acc: 0.9611 F1: 0.9613

Epoch 250/399
----------
train Loss: 0.0096 Acc: 0.9845 F1: 0.9845
val Loss: 0.0367 Acc: 0.9606 F1: 0.9610

Epoch 251/399
----------
train Loss: 0.0098 Acc: 0.9842 F1: 0.9842
val Loss: 0.0388 Acc: 0.9590 F1: 0.9595

Epoch 252/399
----------
train Loss: 0.0100 Acc: 0.9839 F1: 0.9839
val Loss: 0.0285 Acc: 0.9651 F1: 0.9652

Epoch 253/399
----------
train Loss: 0.0099 Acc: 0.9840 F1: 0.9840
val Loss: 0.0270 Acc: 0.9681 F1: 0.9681

Epoch 254/399
----------
train Loss: 0.0098 Acc: 0.9841 F1: 0.9841
val Loss: 0.0367 Acc: 0.9595 F1: 0.9596

Epoch 255/399
----------
train Loss: 0.0093 Acc: 0.9848 F1: 0.9848
val Loss: 0.0335 Acc: 0.9635 F1: 0.9636

Epoch 256/399
----------
train Loss: 0.0096 Acc: 0.9843 F1: 0.9843
val Loss: 0.0328 Acc: 0.9634 F1: 0.9637

Epoch 257/399
----------
train Loss: 0.0090 Acc: 0.9855 F1: 0.9855
val Loss: 0.0360 Acc: 0.9634 F1: 0.9637

Epoch 258/399
----------
train Loss: 0.0092 Acc: 0.9849 F1: 0.9849
val Loss: 0.0343 Acc: 0.9644 F1: 0.9647

Epoch 259/399
----------
train Loss: 0.0092 Acc: 0.9848 F1: 0.9848
val Loss: 0.0400 Acc: 0.9604 F1: 0.9607

Epoch 260/399
----------
train Loss: 0.0091 Acc: 0.9852 F1: 0.9852
val Loss: 0.0332 Acc: 0.9621 F1: 0.9624

Epoch 261/399
----------
train Loss: 0.0093 Acc: 0.9849 F1: 0.9849
val Loss: 0.0451 Acc: 0.9601 F1: 0.9604

Epoch 262/399
----------
train Loss: 0.0090 Acc: 0.9854 F1: 0.9854
val Loss: 0.0382 Acc: 0.9622 F1: 0.9625

Epoch 263/399
----------
train Loss: 0.0087 Acc: 0.9860 F1: 0.9860
val Loss: 0.0444 Acc: 0.9625 F1: 0.9627

Epoch 264/399
----------
train Loss: 0.0093 Acc: 0.9850 F1: 0.9850
val Loss: 0.0241 Acc: 0.9706 F1: 0.9707

Epoch 265/399
----------
train Loss: 0.0089 Acc: 0.9856 F1: 0.9856
val Loss: 0.0277 Acc: 0.9674 F1: 0.9674

Epoch 266/399
----------
train Loss: 0.0095 Acc: 0.9846 F1: 0.9846
val Loss: 0.0332 Acc: 0.9633 F1: 0.9634

Epoch 267/399
----------
train Loss: 0.0095 Acc: 0.9847 F1: 0.9847
val Loss: 0.0323 Acc: 0.9627 F1: 0.9627

Epoch 268/399
----------
train Loss: 0.0095 Acc: 0.9848 F1: 0.9848
val Loss: 0.0386 Acc: 0.9618 F1: 0.9622

Epoch 269/399
----------
train Loss: 0.0094 Acc: 0.9847 F1: 0.9847
val Loss: 0.0290 Acc: 0.9662 F1: 0.9662

Epoch 270/399
----------
train Loss: 0.0096 Acc: 0.9843 F1: 0.9843
val Loss: 0.0286 Acc: 0.9654 F1: 0.9655

Epoch 271/399
----------
train Loss: 0.0089 Acc: 0.9855 F1: 0.9855
val Loss: 0.0322 Acc: 0.9665 F1: 0.9665

Epoch 272/399
----------
train Loss: 0.0090 Acc: 0.9854 F1: 0.9854
val Loss: 0.0303 Acc: 0.9600 F1: 0.9604

Epoch 273/399
----------
train Loss: 0.0090 Acc: 0.9855 F1: 0.9855
val Loss: 0.0292 Acc: 0.9674 F1: 0.9675

Epoch 274/399
----------
train Loss: 0.0089 Acc: 0.9856 F1: 0.9856
val Loss: 0.0295 Acc: 0.9666 F1: 0.9667

Epoch 275/399
----------
train Loss: 0.0090 Acc: 0.9853 F1: 0.9853
val Loss: 0.0260 Acc: 0.9700 F1: 0.9701

Epoch 276/399
----------
train Loss: 0.0090 Acc: 0.9855 F1: 0.9855
val Loss: 0.0361 Acc: 0.9640 F1: 0.9642

Epoch 277/399
----------
train Loss: 0.0085 Acc: 0.9863 F1: 0.9863
val Loss: 0.0364 Acc: 0.9626 F1: 0.9628

Epoch 278/399
----------
train Loss: 0.0086 Acc: 0.9860 F1: 0.9860
val Loss: 0.0385 Acc: 0.9631 F1: 0.9633

Epoch 279/399
----------
train Loss: 0.0086 Acc: 0.9862 F1: 0.9862
val Loss: 0.0423 Acc: 0.9608 F1: 0.9612

Epoch 280/399
----------
train Loss: 0.0087 Acc: 0.9858 F1: 0.9858
val Loss: 0.0395 Acc: 0.9635 F1: 0.9637

Epoch 281/399
----------
train Loss: 0.0084 Acc: 0.9863 F1: 0.9863
val Loss: 0.0313 Acc: 0.9669 F1: 0.9671

Epoch 282/399
----------
train Loss: 0.0086 Acc: 0.9859 F1: 0.9859
val Loss: 0.0347 Acc: 0.9612 F1: 0.9613

Epoch 283/399
----------
train Loss: 0.0087 Acc: 0.9858 F1: 0.9858
val Loss: 0.0316 Acc: 0.9654 F1: 0.9654

Epoch 284/399
----------
train Loss: 0.0083 Acc: 0.9865 F1: 0.9865
val Loss: 0.0304 Acc: 0.9679 F1: 0.9681

Epoch 285/399
----------
train Loss: 0.0088 Acc: 0.9859 F1: 0.9859
val Loss: 0.0410 Acc: 0.9618 F1: 0.9621

Epoch 286/399
----------
train Loss: 0.0089 Acc: 0.9857 F1: 0.9857
val Loss: 0.0423 Acc: 0.9605 F1: 0.9608

Epoch 287/399
----------
train Loss: 0.0089 Acc: 0.9856 F1: 0.9856
val Loss: 0.0306 Acc: 0.9620 F1: 0.9624

Epoch 288/399
----------
train Loss: 0.0085 Acc: 0.9864 F1: 0.9864
val Loss: 0.0371 Acc: 0.9640 F1: 0.9642

Epoch 289/399
----------
train Loss: 0.0089 Acc: 0.9856 F1: 0.9856
val Loss: 0.0267 Acc: 0.9677 F1: 0.9677

Epoch 290/399
----------
train Loss: 0.0089 Acc: 0.9858 F1: 0.9858
val Loss: 0.0381 Acc: 0.9657 F1: 0.9658

Epoch 291/399
----------
train Loss: 0.0092 Acc: 0.9851 F1: 0.9851
val Loss: 0.0305 Acc: 0.9673 F1: 0.9673

Epoch 292/399
----------
train Loss: 0.0088 Acc: 0.9859 F1: 0.9859
val Loss: 0.0344 Acc: 0.9658 F1: 0.9659

Epoch 293/399
----------
train Loss: 0.0091 Acc: 0.9854 F1: 0.9854
val Loss: 0.0317 Acc: 0.9644 F1: 0.9644

Epoch 294/399
----------
train Loss: 0.0091 Acc: 0.9854 F1: 0.9854
val Loss: 0.0278 Acc: 0.9681 F1: 0.9682

Epoch 295/399
----------
train Loss: 0.0090 Acc: 0.9855 F1: 0.9855
val Loss: 0.0393 Acc: 0.9640 F1: 0.9640

Epoch 296/399
----------
train Loss: 0.0088 Acc: 0.9859 F1: 0.9859
val Loss: 0.0396 Acc: 0.9635 F1: 0.9636

Epoch 297/399
----------
train Loss: 0.0090 Acc: 0.9854 F1: 0.9854
val Loss: 0.0402 Acc: 0.9592 F1: 0.9596

Epoch 298/399
----------
train Loss: 0.0083 Acc: 0.9866 F1: 0.9866
val Loss: 0.0421 Acc: 0.9599 F1: 0.9602

Epoch 299/399
----------
train Loss: 0.0089 Acc: 0.9857 F1: 0.9857
val Loss: 0.0390 Acc: 0.9601 F1: 0.9602

Epoch 300/399
----------
train Loss: 0.0085 Acc: 0.9862 F1: 0.9862
val Loss: 0.0324 Acc: 0.9698 F1: 0.9697

Epoch 301/399
----------
train Loss: 0.0086 Acc: 0.9862 F1: 0.9861
val Loss: 0.0253 Acc: 0.9708 F1: 0.9709

Epoch 302/399
----------
train Loss: 0.0085 Acc: 0.9862 F1: 0.9862
val Loss: 0.0408 Acc: 0.9635 F1: 0.9637

Epoch 303/399
----------
train Loss: 0.0085 Acc: 0.9864 F1: 0.9864
val Loss: 0.0270 Acc: 0.9709 F1: 0.9709

Epoch 304/399
----------
train Loss: 0.0083 Acc: 0.9864 F1: 0.9864
val Loss: 0.0369 Acc: 0.9657 F1: 0.9659

Epoch 305/399
----------
train Loss: 0.0081 Acc: 0.9867 F1: 0.9867
val Loss: 0.0372 Acc: 0.9658 F1: 0.9660

Epoch 306/399
----------
train Loss: 0.0080 Acc: 0.9870 F1: 0.9870
val Loss: 0.0386 Acc: 0.9617 F1: 0.9619

Epoch 307/399
----------
train Loss: 0.0104 Acc: 0.9837 F1: 0.9836
val Loss: 0.0433 Acc: 0.9587 F1: 0.9591

Epoch 308/399
----------
train Loss: 0.0141 Acc: 0.9792 F1: 0.9792
val Loss: 0.0371 Acc: 0.9578 F1: 0.9582

Epoch 309/399
----------
train Loss: 0.0182 Acc: 0.9744 F1: 0.9742
val Loss: 0.0592 Acc: 0.8990 F1: 0.9032

Epoch 310/399
----------
train Loss: 0.0299 Acc: 0.9571 F1: 0.9569
val Loss: 0.0330 Acc: 0.9538 F1: 0.9535

Epoch 311/399
----------
train Loss: 0.0147 Acc: 0.9773 F1: 0.9772
val Loss: 0.0425 Acc: 0.9589 F1: 0.9593

Epoch 312/399
----------
train Loss: 0.0118 Acc: 0.9816 F1: 0.9815
val Loss: 0.0369 Acc: 0.9566 F1: 0.9570

Epoch 313/399
----------
train Loss: 0.0101 Acc: 0.9841 F1: 0.9841
val Loss: 0.0455 Acc: 0.9571 F1: 0.9575

Epoch 314/399
----------
train Loss: 0.0098 Acc: 0.9844 F1: 0.9844
val Loss: 0.0411 Acc: 0.9584 F1: 0.9587

Epoch 315/399
----------
train Loss: 0.0111 Acc: 0.9825 F1: 0.9825
val Loss: 0.0267 Acc: 0.9630 F1: 0.9631

Epoch 316/399
----------
train Loss: 0.0127 Acc: 0.9809 F1: 0.9809
val Loss: 0.0332 Acc: 0.9628 F1: 0.9630

Epoch 317/399
----------
train Loss: 0.0098 Acc: 0.9845 F1: 0.9845
val Loss: 0.0319 Acc: 0.9634 F1: 0.9636

Epoch 318/399
----------
train Loss: 0.0093 Acc: 0.9849 F1: 0.9849
val Loss: 0.0432 Acc: 0.9614 F1: 0.9616

Epoch 319/399
----------
train Loss: 0.0085 Acc: 0.9864 F1: 0.9864
val Loss: 0.0369 Acc: 0.9635 F1: 0.9637

Epoch 320/399
----------
train Loss: 0.0086 Acc: 0.9862 F1: 0.9862
val Loss: 0.0465 Acc: 0.9617 F1: 0.9619

Epoch 321/399
----------
train Loss: 0.0084 Acc: 0.9864 F1: 0.9864
val Loss: 0.0342 Acc: 0.9643 F1: 0.9645

Epoch 322/399
----------
train Loss: 0.0081 Acc: 0.9870 F1: 0.9870
val Loss: 0.0395 Acc: 0.9624 F1: 0.9628

Epoch 323/399
----------
train Loss: 0.0087 Acc: 0.9859 F1: 0.9859
val Loss: 0.0462 Acc: 0.9601 F1: 0.9603

Epoch 324/399
----------
train Loss: 0.0091 Acc: 0.9855 F1: 0.9855
val Loss: 0.0374 Acc: 0.9619 F1: 0.9620

Epoch 325/399
----------
train Loss: 0.0084 Acc: 0.9866 F1: 0.9865
val Loss: 0.0466 Acc: 0.9601 F1: 0.9604

Epoch 326/399
----------
train Loss: 0.0082 Acc: 0.9868 F1: 0.9868
val Loss: 0.0362 Acc: 0.9638 F1: 0.9639

Epoch 327/399
----------
train Loss: 0.0083 Acc: 0.9865 F1: 0.9865
val Loss: 0.0480 Acc: 0.9617 F1: 0.9619

Epoch 328/399
----------
train Loss: 0.0086 Acc: 0.9865 F1: 0.9865
val Loss: 0.0355 Acc: 0.9619 F1: 0.9620

Epoch 329/399
----------
train Loss: 0.0113 Acc: 0.9822 F1: 0.9822
val Loss: 0.0420 Acc: 0.9607 F1: 0.9610

Epoch 330/399
----------
train Loss: 0.0086 Acc: 0.9862 F1: 0.9862
val Loss: 0.0458 Acc: 0.9584 F1: 0.9587

Epoch 331/399
----------
train Loss: 0.0081 Acc: 0.9870 F1: 0.9870
val Loss: 0.0484 Acc: 0.9617 F1: 0.9619

Epoch 332/399
----------
train Loss: 0.0080 Acc: 0.9869 F1: 0.9869
val Loss: 0.0440 Acc: 0.9623 F1: 0.9626

Epoch 333/399
----------
train Loss: 0.0080 Acc: 0.9871 F1: 0.9871
val Loss: 0.0472 Acc: 0.9606 F1: 0.9610

Epoch 334/399
----------
train Loss: 0.0078 Acc: 0.9873 F1: 0.9873
val Loss: 0.0504 Acc: 0.9621 F1: 0.9624

Epoch 335/399
----------
train Loss: 0.0079 Acc: 0.9873 F1: 0.9873
val Loss: 0.0494 Acc: 0.9611 F1: 0.9614

Epoch 336/399
----------
train Loss: 0.0084 Acc: 0.9864 F1: 0.9864
val Loss: 0.0460 Acc: 0.9624 F1: 0.9626

Epoch 337/399
----------
train Loss: 0.0077 Acc: 0.9873 F1: 0.9873
val Loss: 0.0421 Acc: 0.9635 F1: 0.9636

Epoch 338/399
----------
train Loss: 0.0077 Acc: 0.9874 F1: 0.9874
val Loss: 0.0491 Acc: 0.9611 F1: 0.9612

Epoch 339/399
----------
train Loss: 0.0078 Acc: 0.9872 F1: 0.9872
val Loss: 0.0382 Acc: 0.9644 F1: 0.9645

Epoch 340/399
----------
train Loss: 0.0075 Acc: 0.9878 F1: 0.9878
val Loss: 0.0381 Acc: 0.9622 F1: 0.9625

Epoch 341/399
----------
train Loss: 0.0073 Acc: 0.9883 F1: 0.9883
val Loss: 0.0408 Acc: 0.9628 F1: 0.9630

Epoch 342/399
----------
train Loss: 0.0075 Acc: 0.9878 F1: 0.9878
val Loss: 0.0453 Acc: 0.9632 F1: 0.9635

Epoch 343/399
----------
train Loss: 0.0075 Acc: 0.9879 F1: 0.9879
val Loss: 0.0406 Acc: 0.9631 F1: 0.9633

Epoch 344/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884
val Loss: 0.0409 Acc: 0.9632 F1: 0.9634

Epoch 345/399
----------
train Loss: 0.0076 Acc: 0.9879 F1: 0.9879
val Loss: 0.0447 Acc: 0.9632 F1: 0.9634

Epoch 346/399
----------
train Loss: 0.0074 Acc: 0.9879 F1: 0.9879
val Loss: 0.0369 Acc: 0.9647 F1: 0.9650

Epoch 347/399
----------
train Loss: 0.0072 Acc: 0.9883 F1: 0.9883
val Loss: 0.0460 Acc: 0.9629 F1: 0.9631

Epoch 348/399
----------
train Loss: 0.0071 Acc: 0.9884 F1: 0.9884
val Loss: 0.0357 Acc: 0.9648 F1: 0.9649

Epoch 349/399
----------
train Loss: 0.0076 Acc: 0.9875 F1: 0.9875
val Loss: 0.0416 Acc: 0.9601 F1: 0.9604

Epoch 350/399
----------
train Loss: 0.0077 Acc: 0.9876 F1: 0.9876
val Loss: 0.0364 Acc: 0.9611 F1: 0.9614

Epoch 351/399
----------
train Loss: 0.0075 Acc: 0.9878 F1: 0.9878
val Loss: 0.0502 Acc: 0.9629 F1: 0.9630

Epoch 352/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884
val Loss: 0.0428 Acc: 0.9608 F1: 0.9610

Epoch 353/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885
val Loss: 0.0414 Acc: 0.9639 F1: 0.9641

Epoch 354/399
----------
train Loss: 0.0070 Acc: 0.9886 F1: 0.9886
val Loss: 0.0393 Acc: 0.9649 F1: 0.9652

Epoch 355/399
----------
train Loss: 0.0071 Acc: 0.9886 F1: 0.9886
val Loss: 0.0478 Acc: 0.9614 F1: 0.9618

Epoch 356/399
----------
train Loss: 0.0076 Acc: 0.9877 F1: 0.9877
val Loss: 0.0569 Acc: 0.9595 F1: 0.9598

Epoch 357/399
----------
train Loss: 0.0093 Acc: 0.9854 F1: 0.9854
val Loss: 0.0346 Acc: 0.9632 F1: 0.9633

Epoch 358/399
----------
train Loss: 0.0080 Acc: 0.9869 F1: 0.9869
val Loss: 0.0364 Acc: 0.9642 F1: 0.9645

Epoch 359/399
----------
train Loss: 0.0075 Acc: 0.9880 F1: 0.9880
val Loss: 0.0424 Acc: 0.9630 F1: 0.9632

Epoch 360/399
----------
train Loss: 0.0073 Acc: 0.9882 F1: 0.9882
val Loss: 0.0390 Acc: 0.9607 F1: 0.9610

Epoch 361/399
----------
train Loss: 0.0074 Acc: 0.9880 F1: 0.9880
val Loss: 0.0489 Acc: 0.9622 F1: 0.9625

Epoch 362/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885
val Loss: 0.0397 Acc: 0.9622 F1: 0.9625

Epoch 363/399
----------
train Loss: 0.0068 Acc: 0.9890 F1: 0.9890
val Loss: 0.0488 Acc: 0.9618 F1: 0.9621

Epoch 364/399
----------
train Loss: 0.0070 Acc: 0.9886 F1: 0.9886
val Loss: 0.0519 Acc: 0.9628 F1: 0.9629

Epoch 365/399
----------
train Loss: 0.0073 Acc: 0.9882 F1: 0.9882
val Loss: 0.0508 Acc: 0.9622 F1: 0.9623

Epoch 366/399
----------
train Loss: 0.0075 Acc: 0.9879 F1: 0.9879
val Loss: 0.0413 Acc: 0.9612 F1: 0.9614

Epoch 367/399
----------
train Loss: 0.0074 Acc: 0.9880 F1: 0.9880
val Loss: 0.0498 Acc: 0.9628 F1: 0.9629

Epoch 368/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885
val Loss: 0.0514 Acc: 0.9623 F1: 0.9626

Epoch 369/399
----------
train Loss: 0.0072 Acc: 0.9882 F1: 0.9882
val Loss: 0.0521 Acc: 0.9626 F1: 0.9627

Epoch 370/399
----------
train Loss: 0.0074 Acc: 0.9881 F1: 0.9881
val Loss: 0.0349 Acc: 0.9644 F1: 0.9646

Epoch 371/399
----------
train Loss: 0.0071 Acc: 0.9884 F1: 0.9884
val Loss: 0.0509 Acc: 0.9599 F1: 0.9604

Epoch 372/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884
val Loss: 0.0340 Acc: 0.9651 F1: 0.9652

Epoch 373/399
----------
train Loss: 0.0071 Acc: 0.9886 F1: 0.9886
val Loss: 0.0505 Acc: 0.9596 F1: 0.9599

Epoch 374/399
----------
train Loss: 0.0074 Acc: 0.9879 F1: 0.9879
val Loss: 0.0424 Acc: 0.9614 F1: 0.9615

Epoch 375/399
----------
train Loss: 0.0070 Acc: 0.9887 F1: 0.9887
val Loss: 0.0387 Acc: 0.9643 F1: 0.9645

Epoch 376/399
----------
train Loss: 0.0069 Acc: 0.9887 F1: 0.9887
val Loss: 0.0427 Acc: 0.9617 F1: 0.9619

Epoch 377/399
----------
train Loss: 0.0068 Acc: 0.9891 F1: 0.9891
val Loss: 0.0519 Acc: 0.9622 F1: 0.9624

Epoch 378/399
----------
train Loss: 0.0070 Acc: 0.9887 F1: 0.9887
val Loss: 0.0386 Acc: 0.9623 F1: 0.9624

Epoch 379/399
----------
train Loss: 0.0068 Acc: 0.9891 F1: 0.9890
val Loss: 0.0520 Acc: 0.9641 F1: 0.9644

Epoch 380/399
----------
train Loss: 0.0070 Acc: 0.9886 F1: 0.9886
val Loss: 0.0395 Acc: 0.9627 F1: 0.9629

Epoch 381/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884
val Loss: 0.0367 Acc: 0.9612 F1: 0.9612

Epoch 382/399
----------
train Loss: 0.0072 Acc: 0.9885 F1: 0.9885
val Loss: 0.0443 Acc: 0.9609 F1: 0.9610

Epoch 383/399
----------
train Loss: 0.0071 Acc: 0.9886 F1: 0.9886
val Loss: 0.0554 Acc: 0.9588 F1: 0.9592

Epoch 384/399
----------
train Loss: 0.0069 Acc: 0.9889 F1: 0.9889
val Loss: 0.0323 Acc: 0.9661 F1: 0.9661

Epoch 385/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885
val Loss: 0.0442 Acc: 0.9630 F1: 0.9634

Epoch 386/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885
val Loss: 0.0446 Acc: 0.9635 F1: 0.9637

Epoch 387/399
----------
train Loss: 0.0068 Acc: 0.9891 F1: 0.9891
val Loss: 0.0448 Acc: 0.9630 F1: 0.9633

Epoch 388/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885
val Loss: 0.0411 Acc: 0.9627 F1: 0.9629

Epoch 389/399
----------
train Loss: 0.0070 Acc: 0.9888 F1: 0.9888
val Loss: 0.0395 Acc: 0.9622 F1: 0.9625

Epoch 390/399
----------
train Loss: 0.0064 Acc: 0.9896 F1: 0.9896
val Loss: 0.0335 Acc: 0.9626 F1: 0.9630

Epoch 391/399
----------
train Loss: 0.0067 Acc: 0.9893 F1: 0.9893
val Loss: 0.0491 Acc: 0.9634 F1: 0.9637

Epoch 392/399
----------
train Loss: 0.0063 Acc: 0.9898 F1: 0.9898
val Loss: 0.0495 Acc: 0.9656 F1: 0.9659

Epoch 393/399
----------
train Loss: 0.0066 Acc: 0.9894 F1: 0.9894
val Loss: 0.0499 Acc: 0.9636 F1: 0.9638

Epoch 394/399
----------
train Loss: 0.0104 Acc: 0.9852 F1: 0.9852
val Loss: 0.0360 Acc: 0.9432 F1: 0.9443

Epoch 395/399
----------
train Loss: 0.0256 Acc: 0.9643 F1: 0.9642
val Loss: 0.0346 Acc: 0.9428 F1: 0.9435

Epoch 396/399
----------
train Loss: 0.0144 Acc: 0.9780 F1: 0.9780
val Loss: 0.0326 Acc: 0.9584 F1: 0.9586

Epoch 397/399
----------
train Loss: 0.0097 Acc: 0.9849 F1: 0.9849
val Loss: 0.0424 Acc: 0.9575 F1: 0.9578

Epoch 398/399
----------
train Loss: 0.0086 Acc: 0.9863 F1: 0.9863
val Loss: 0.0299 Acc: 0.9636 F1: 0.9639

Epoch 399/399
----------
train Loss: 0.0076 Acc: 0.9880 F1: 0.9880
val Loss: 0.0413 Acc: 0.9606 F1: 0.9609

Training complete in 169m 23s
Best val F1: 0.9709
Saving..
Successfully retrieved statistics for job: zhangz65-gpu02-3589698_. 
+------------------------------------------------------------------------------+
| GPU ID: 2                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Fri Mar  8 10:37:58 2024                |
| End Time                           | Fri Mar  8 13:28:04 2024                |
| Total Execution Time (sec)         | 10206.1                                 |
| No. of Processes                   | 1                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 634317                                  |
| Power Usage (Watts)                | Avg: 176.714, Max: 243.42, Min: 47.253  |
| Max GPU Memory Used (bytes)        | 8990490624                              |
| SM Clock (MHz)                     | Avg: 1380, Max: 1380, Min: 1380         |
| Memory Clock (MHz)                 | Avg: 877, Max: 877, Min: 877            |
| SM Utilization (%)                 | Avg: 79, Max: 100, Min: 0               |
| Memory Utilization (%)             | Avg: 41, Max: 58, Min: 0                |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 84982                                   |
|     Avg SM Utilization (%)         | 82                                      |
|     Avg Memory Utilization (%)     | 41                                      |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+

