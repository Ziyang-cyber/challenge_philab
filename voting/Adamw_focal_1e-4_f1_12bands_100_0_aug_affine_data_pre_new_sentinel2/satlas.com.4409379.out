 INiotted
DataLoaders created successfully!
Model(
  (backbone): SwinBackbone(
    (backbone): SwinTransformer(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(22, 128, kernel_size=(4, 4), stride=(4, 4))
          (1): Permute()
          (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (1): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=4, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=4, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): PatchMergingV2(
          (reduction): Linear(in_features=512, out_features=256, bias=False)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (4): PatchMergingV2(
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (6): PatchMergingV2(
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=32, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=32, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.5, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (permute): Permute()
      (avgpool): AdaptiveAvgPool2d(output_size=1)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (head): Linear(in_features=1024, out_features=1000, bias=True)
    )
  )
  (fpn): FPN(
    (fpn): FeaturePyramidNetwork(
      (inner_blocks): ModuleList(
        (0): Conv2dNormActivation(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Conv2dNormActivation(
          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Conv2dNormActivation(
          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Conv2dNormActivation(
          (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (layer_blocks): ModuleList(
        (0-3): 4 x Conv2dNormActivation(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
  )
  (head): SimpleHead(
    (layers): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (1): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
Epoch 0/399
----------
train Loss: 0.1535 Acc: 0.8364 F1: 0.8322

Epoch 1/399
----------
train Loss: 0.0665 Acc: 0.8983 F1: 0.8954

Epoch 2/399
----------
train Loss: 0.0670 Acc: 0.8936 F1: 0.8895

Epoch 3/399
----------
train Loss: 0.0587 Acc: 0.9112 F1: 0.9086

Epoch 4/399
----------
train Loss: 0.0599 Acc: 0.9105 F1: 0.9075

Epoch 5/399
----------
train Loss: 0.0562 Acc: 0.9114 F1: 0.9087

Epoch 6/399
----------
train Loss: 0.0538 Acc: 0.9158 F1: 0.9129

Epoch 7/399
----------
train Loss: 0.0517 Acc: 0.9166 F1: 0.9138

Epoch 8/399
----------
train Loss: 0.0515 Acc: 0.9169 F1: 0.9141

Epoch 9/399
----------
train Loss: 0.0496 Acc: 0.9185 F1: 0.9158

Epoch 10/399
----------
train Loss: 0.0477 Acc: 0.9200 F1: 0.9172

Epoch 11/399
----------
train Loss: 0.0482 Acc: 0.9189 F1: 0.9169

Epoch 12/399
----------
train Loss: 0.0487 Acc: 0.9195 F1: 0.9166

Epoch 13/399
----------
train Loss: 0.0460 Acc: 0.9223 F1: 0.9198

Epoch 14/399
----------
train Loss: 0.0457 Acc: 0.9246 F1: 0.9224

Epoch 15/399
----------
train Loss: 0.0461 Acc: 0.9236 F1: 0.9214

Epoch 16/399
----------
train Loss: 0.0434 Acc: 0.9271 F1: 0.9251

Epoch 17/399
----------
train Loss: 0.0441 Acc: 0.9259 F1: 0.9239

Epoch 18/399
----------
train Loss: 0.0420 Acc: 0.9293 F1: 0.9278

Epoch 19/399
----------
train Loss: 0.0412 Acc: 0.9299 F1: 0.9285

Epoch 20/399
----------
train Loss: 0.0415 Acc: 0.9319 F1: 0.9304

Epoch 21/399
----------
train Loss: 0.0403 Acc: 0.9322 F1: 0.9309

Epoch 22/399
----------
train Loss: 0.0383 Acc: 0.9369 F1: 0.9361

Epoch 23/399
----------
train Loss: 0.0385 Acc: 0.9360 F1: 0.9348

Epoch 24/399
----------
train Loss: 0.0422 Acc: 0.9303 F1: 0.9291

Epoch 25/399
----------
train Loss: 0.0400 Acc: 0.9341 F1: 0.9331

Epoch 26/399
----------
train Loss: 0.0389 Acc: 0.9329 F1: 0.9319

Epoch 27/399
----------
train Loss: 0.0372 Acc: 0.9378 F1: 0.9369

Epoch 28/399
----------
train Loss: 0.0351 Acc: 0.9413 F1: 0.9405

Epoch 29/399
----------
train Loss: 0.0360 Acc: 0.9407 F1: 0.9401

Epoch 30/399
----------
train Loss: 0.0333 Acc: 0.9439 F1: 0.9433

Epoch 31/399
----------
train Loss: 0.0330 Acc: 0.9450 F1: 0.9445

Epoch 32/399
----------
train Loss: 0.0356 Acc: 0.9404 F1: 0.9398

Epoch 33/399
----------
train Loss: 0.0350 Acc: 0.9412 F1: 0.9405

Epoch 34/399
----------
train Loss: 0.0314 Acc: 0.9476 F1: 0.9471

Epoch 35/399
----------
train Loss: 0.0317 Acc: 0.9478 F1: 0.9473

Epoch 36/399
----------
train Loss: 0.0369 Acc: 0.9405 F1: 0.9397

Epoch 37/399
----------
train Loss: 0.0324 Acc: 0.9457 F1: 0.9452

Epoch 38/399
----------
train Loss: 0.0319 Acc: 0.9465 F1: 0.9461

Epoch 39/399
----------
train Loss: 0.0312 Acc: 0.9475 F1: 0.9471

Epoch 40/399
----------
train Loss: 0.0286 Acc: 0.9520 F1: 0.9517

Epoch 41/399
----------
train Loss: 0.0323 Acc: 0.9475 F1: 0.9471

Epoch 42/399
----------
train Loss: 0.0299 Acc: 0.9495 F1: 0.9492

Epoch 43/399
----------
train Loss: 0.0285 Acc: 0.9520 F1: 0.9517

Epoch 44/399
----------
train Loss: 0.0268 Acc: 0.9546 F1: 0.9544

Epoch 45/399
----------
train Loss: 0.0269 Acc: 0.9556 F1: 0.9555

Epoch 46/399
----------
train Loss: 0.0261 Acc: 0.9558 F1: 0.9556

Epoch 47/399
----------
train Loss: 0.0264 Acc: 0.9565 F1: 0.9564

Epoch 48/399
----------
train Loss: 0.0267 Acc: 0.9557 F1: 0.9555

Epoch 49/399
----------
train Loss: 0.0271 Acc: 0.9543 F1: 0.9541

Epoch 50/399
----------
train Loss: 0.0254 Acc: 0.9574 F1: 0.9572

Epoch 51/399
----------
train Loss: 0.0243 Acc: 0.9588 F1: 0.9587

Epoch 52/399
----------
train Loss: 0.0263 Acc: 0.9575 F1: 0.9573

Epoch 53/399
----------
train Loss: 0.0266 Acc: 0.9563 F1: 0.9561

Epoch 54/399
----------
train Loss: 0.0260 Acc: 0.9571 F1: 0.9569

Epoch 55/399
----------
train Loss: 0.0245 Acc: 0.9592 F1: 0.9591

Epoch 56/399
----------
train Loss: 0.0234 Acc: 0.9610 F1: 0.9609

Epoch 57/399
----------
train Loss: 0.0250 Acc: 0.9592 F1: 0.9590

Epoch 58/399
----------
train Loss: 0.0289 Acc: 0.9534 F1: 0.9531

Epoch 59/399
----------
train Loss: 0.0259 Acc: 0.9575 F1: 0.9573

Epoch 60/399
----------
train Loss: 0.0232 Acc: 0.9620 F1: 0.9618

Epoch 61/399
----------
train Loss: 0.0256 Acc: 0.9579 F1: 0.9577

Epoch 62/399
----------
train Loss: 0.0245 Acc: 0.9594 F1: 0.9592

Epoch 63/399
----------
train Loss: 0.0242 Acc: 0.9603 F1: 0.9601

Epoch 64/399
----------
train Loss: 0.0235 Acc: 0.9615 F1: 0.9614

Epoch 65/399
----------
train Loss: 0.0227 Acc: 0.9624 F1: 0.9622

Epoch 66/399
----------
train Loss: 0.0227 Acc: 0.9620 F1: 0.9619

Epoch 67/399
----------
train Loss: 0.0224 Acc: 0.9625 F1: 0.9624

Epoch 68/399
----------
train Loss: 0.0216 Acc: 0.9641 F1: 0.9640

Epoch 69/399
----------
train Loss: 0.0212 Acc: 0.9651 F1: 0.9650

Epoch 70/399
----------
train Loss: 0.0221 Acc: 0.9636 F1: 0.9635

Epoch 71/399
----------
train Loss: 0.0249 Acc: 0.9602 F1: 0.9600

Epoch 72/399
----------
train Loss: 0.0277 Acc: 0.9542 F1: 0.9539

Epoch 73/399
----------
train Loss: 0.0269 Acc: 0.9559 F1: 0.9556

Epoch 74/399
----------
train Loss: 0.0256 Acc: 0.9585 F1: 0.9583

Epoch 75/399
----------
train Loss: 0.0334 Acc: 0.9466 F1: 0.9461

Epoch 76/399
----------
train Loss: 0.0305 Acc: 0.9519 F1: 0.9515

Epoch 77/399
----------
train Loss: 0.0231 Acc: 0.9624 F1: 0.9623

Epoch 78/399
----------
train Loss: 0.0212 Acc: 0.9658 F1: 0.9657

Epoch 79/399
----------
train Loss: 0.0212 Acc: 0.9647 F1: 0.9647

Epoch 80/399
----------
train Loss: 0.0212 Acc: 0.9653 F1: 0.9652

Epoch 81/399
----------
train Loss: 0.0215 Acc: 0.9651 F1: 0.9650

Epoch 82/399
----------
train Loss: 0.0204 Acc: 0.9666 F1: 0.9665

Epoch 83/399
----------
train Loss: 0.0195 Acc: 0.9678 F1: 0.9677

Epoch 84/399
----------
train Loss: 0.0187 Acc: 0.9691 F1: 0.9690

Epoch 85/399
----------
train Loss: 0.0195 Acc: 0.9680 F1: 0.9680

Epoch 86/399
----------
train Loss: 0.0203 Acc: 0.9670 F1: 0.9670

Epoch 87/399
----------
train Loss: 0.0195 Acc: 0.9676 F1: 0.9675

Epoch 88/399
----------
train Loss: 0.0198 Acc: 0.9673 F1: 0.9672

Epoch 89/399
----------
train Loss: 0.0209 Acc: 0.9657 F1: 0.9656

Epoch 90/399
----------
train Loss: 0.0224 Acc: 0.9627 F1: 0.9626

Epoch 91/399
----------
train Loss: 0.0204 Acc: 0.9664 F1: 0.9664

Epoch 92/399
----------
train Loss: 0.0215 Acc: 0.9645 F1: 0.9645

Epoch 93/399
----------
train Loss: 0.0208 Acc: 0.9662 F1: 0.9661

Epoch 94/399
----------
train Loss: 0.0188 Acc: 0.9689 F1: 0.9689

Epoch 95/399
----------
train Loss: 0.0186 Acc: 0.9697 F1: 0.9696

Epoch 96/399
----------
train Loss: 0.0234 Acc: 0.9636 F1: 0.9635

Epoch 97/399
----------
train Loss: 0.0212 Acc: 0.9658 F1: 0.9656

Epoch 98/399
----------
train Loss: 0.0188 Acc: 0.9690 F1: 0.9690

Epoch 99/399
----------
train Loss: 0.0176 Acc: 0.9712 F1: 0.9711

Epoch 100/399
----------
train Loss: 0.0176 Acc: 0.9710 F1: 0.9710

Epoch 101/399
----------
train Loss: 0.0176 Acc: 0.9708 F1: 0.9707

Epoch 102/399
----------
train Loss: 0.0173 Acc: 0.9715 F1: 0.9714

Epoch 103/399
----------
train Loss: 0.0169 Acc: 0.9724 F1: 0.9723

Epoch 104/399
----------
train Loss: 0.0170 Acc: 0.9719 F1: 0.9719

Epoch 105/399
----------
train Loss: 0.0174 Acc: 0.9713 F1: 0.9712

Epoch 106/399
----------
train Loss: 0.0183 Acc: 0.9700 F1: 0.9699

Epoch 107/399
----------
train Loss: 0.0172 Acc: 0.9719 F1: 0.9719

Epoch 108/399
----------
train Loss: 0.0177 Acc: 0.9711 F1: 0.9711

Epoch 109/399
----------
train Loss: 0.0172 Acc: 0.9719 F1: 0.9718

Epoch 110/399
----------
train Loss: 0.0177 Acc: 0.9710 F1: 0.9709

Epoch 111/399
----------
train Loss: 0.0170 Acc: 0.9725 F1: 0.9724

Epoch 112/399
----------
train Loss: 0.0173 Acc: 0.9718 F1: 0.9718

Epoch 113/399
----------
train Loss: 0.0166 Acc: 0.9729 F1: 0.9729

Epoch 114/399
----------
train Loss: 0.0161 Acc: 0.9736 F1: 0.9735

Epoch 115/399
----------
train Loss: 0.0157 Acc: 0.9746 F1: 0.9746

Epoch 116/399
----------
train Loss: 0.0157 Acc: 0.9744 F1: 0.9744

Epoch 117/399
----------
train Loss: 0.0155 Acc: 0.9747 F1: 0.9746

Epoch 118/399
----------
train Loss: 0.0171 Acc: 0.9723 F1: 0.9722

Epoch 119/399
----------
train Loss: 0.0156 Acc: 0.9745 F1: 0.9744

Epoch 120/399
----------
train Loss: 0.0151 Acc: 0.9753 F1: 0.9753

Epoch 121/399
----------
train Loss: 0.0155 Acc: 0.9747 F1: 0.9746

Epoch 122/399
----------
train Loss: 0.0156 Acc: 0.9744 F1: 0.9744

Epoch 123/399
----------
train Loss: 0.0152 Acc: 0.9752 F1: 0.9752

Epoch 124/399
----------
train Loss: 0.0151 Acc: 0.9755 F1: 0.9755

Epoch 125/399
----------
train Loss: 0.0147 Acc: 0.9762 F1: 0.9762

Epoch 126/399
----------
train Loss: 0.0149 Acc: 0.9755 F1: 0.9755

Epoch 127/399
----------
train Loss: 0.0175 Acc: 0.9717 F1: 0.9716

Epoch 128/399
----------
train Loss: 0.0169 Acc: 0.9727 F1: 0.9727

Epoch 129/399
----------
train Loss: 0.0155 Acc: 0.9748 F1: 0.9748

Epoch 130/399
----------
train Loss: 0.0147 Acc: 0.9757 F1: 0.9757

Epoch 131/399
----------
train Loss: 0.0141 Acc: 0.9769 F1: 0.9769

Epoch 132/399
----------
train Loss: 0.0139 Acc: 0.9773 F1: 0.9773

Epoch 133/399
----------
train Loss: 0.0138 Acc: 0.9780 F1: 0.9780

Epoch 134/399
----------
train Loss: 0.0153 Acc: 0.9752 F1: 0.9752

Epoch 135/399
----------
train Loss: 0.0150 Acc: 0.9760 F1: 0.9759

Epoch 136/399
----------
train Loss: 0.0149 Acc: 0.9760 F1: 0.9760

Epoch 137/399
----------
train Loss: 0.0146 Acc: 0.9763 F1: 0.9763

Epoch 138/399
----------
train Loss: 0.0144 Acc: 0.9763 F1: 0.9763

Epoch 139/399
----------
train Loss: 0.0141 Acc: 0.9771 F1: 0.9770

Epoch 140/399
----------
train Loss: 0.0142 Acc: 0.9770 F1: 0.9770

Epoch 141/399
----------
train Loss: 0.0141 Acc: 0.9770 F1: 0.9770

Epoch 142/399
----------
train Loss: 0.0148 Acc: 0.9763 F1: 0.9763

Epoch 143/399
----------
train Loss: 0.0137 Acc: 0.9777 F1: 0.9777

Epoch 144/399
----------
train Loss: 0.0138 Acc: 0.9777 F1: 0.9777

Epoch 145/399
----------
train Loss: 0.0131 Acc: 0.9789 F1: 0.9789

Epoch 146/399
----------
train Loss: 0.0132 Acc: 0.9786 F1: 0.9786

Epoch 147/399
----------
train Loss: 0.0135 Acc: 0.9785 F1: 0.9784

Epoch 148/399
----------
train Loss: 0.0131 Acc: 0.9787 F1: 0.9787

Epoch 149/399
----------
train Loss: 0.0126 Acc: 0.9795 F1: 0.9795

Epoch 150/399
----------
train Loss: 0.0128 Acc: 0.9791 F1: 0.9791

Epoch 151/399
----------
train Loss: 0.0124 Acc: 0.9800 F1: 0.9800

Epoch 152/399
----------
train Loss: 0.0145 Acc: 0.9768 F1: 0.9768

Epoch 153/399
----------
train Loss: 0.0149 Acc: 0.9768 F1: 0.9768

Epoch 154/399
----------
train Loss: 0.0142 Acc: 0.9775 F1: 0.9775

Epoch 155/399
----------
train Loss: 0.0165 Acc: 0.9736 F1: 0.9736

Epoch 156/399
----------
train Loss: 0.0139 Acc: 0.9779 F1: 0.9779

Epoch 157/399
----------
train Loss: 0.0133 Acc: 0.9784 F1: 0.9784

Epoch 158/399
----------
train Loss: 0.0127 Acc: 0.9795 F1: 0.9794

Epoch 159/399
----------
train Loss: 0.0133 Acc: 0.9785 F1: 0.9785

Epoch 160/399
----------
train Loss: 0.0134 Acc: 0.9782 F1: 0.9782

Epoch 161/399
----------
train Loss: 0.0215 Acc: 0.9687 F1: 0.9686

Epoch 162/399
----------
train Loss: 0.0151 Acc: 0.9761 F1: 0.9760

Epoch 163/399
----------
train Loss: 0.0137 Acc: 0.9784 F1: 0.9783

Epoch 164/399
----------
train Loss: 0.0127 Acc: 0.9796 F1: 0.9796

Epoch 165/399
----------
train Loss: 0.0126 Acc: 0.9794 F1: 0.9794

Epoch 166/399
----------
train Loss: 0.0122 Acc: 0.9803 F1: 0.9803

Epoch 167/399
----------
train Loss: 0.0124 Acc: 0.9800 F1: 0.9800

Epoch 168/399
----------
train Loss: 0.0124 Acc: 0.9798 F1: 0.9798

Epoch 169/399
----------
train Loss: 0.0119 Acc: 0.9806 F1: 0.9806

Epoch 170/399
----------
train Loss: 0.0125 Acc: 0.9801 F1: 0.9800

Epoch 171/399
----------
train Loss: 0.0124 Acc: 0.9795 F1: 0.9795

Epoch 172/399
----------
train Loss: 0.0115 Acc: 0.9813 F1: 0.9813

Epoch 173/399
----------
train Loss: 0.0189 Acc: 0.9709 F1: 0.9708

Epoch 174/399
----------
train Loss: 0.0175 Acc: 0.9723 F1: 0.9723

Epoch 175/399
----------
train Loss: 0.0137 Acc: 0.9778 F1: 0.9778

Epoch 176/399
----------
train Loss: 0.0138 Acc: 0.9780 F1: 0.9780

Epoch 177/399
----------
train Loss: 0.0121 Acc: 0.9808 F1: 0.9808

Epoch 178/399
----------
train Loss: 0.0121 Acc: 0.9805 F1: 0.9805

Epoch 179/399
----------
train Loss: 0.0121 Acc: 0.9803 F1: 0.9803

Epoch 180/399
----------
train Loss: 0.0113 Acc: 0.9815 F1: 0.9815

Epoch 181/399
----------
train Loss: 0.0120 Acc: 0.9804 F1: 0.9804

Epoch 182/399
----------
train Loss: 0.0114 Acc: 0.9815 F1: 0.9815

Epoch 183/399
----------
train Loss: 0.0112 Acc: 0.9817 F1: 0.9817

Epoch 184/399
----------
train Loss: 0.0111 Acc: 0.9822 F1: 0.9822

Epoch 185/399
----------
train Loss: 0.0108 Acc: 0.9825 F1: 0.9825

Epoch 186/399
----------
train Loss: 0.0113 Acc: 0.9818 F1: 0.9818

Epoch 187/399
----------
train Loss: 0.0111 Acc: 0.9820 F1: 0.9820

Epoch 188/399
----------
train Loss: 0.0112 Acc: 0.9818 F1: 0.9818

Epoch 189/399
----------
train Loss: 0.0109 Acc: 0.9822 F1: 0.9822

Epoch 190/399
----------
train Loss: 0.0108 Acc: 0.9824 F1: 0.9824

Epoch 191/399
----------
train Loss: 0.0106 Acc: 0.9829 F1: 0.9829

Epoch 192/399
----------
train Loss: 0.0107 Acc: 0.9825 F1: 0.9825

Epoch 193/399
----------
train Loss: 0.0102 Acc: 0.9836 F1: 0.9835

Epoch 194/399
----------
train Loss: 0.0110 Acc: 0.9822 F1: 0.9822

Epoch 195/399
----------
train Loss: 0.0108 Acc: 0.9824 F1: 0.9824

Epoch 196/399
----------
train Loss: 0.0106 Acc: 0.9828 F1: 0.9828

Epoch 197/399
----------
train Loss: 0.0104 Acc: 0.9831 F1: 0.9831

Epoch 198/399
----------
train Loss: 0.0114 Acc: 0.9815 F1: 0.9814

Epoch 199/399
----------
train Loss: 0.0109 Acc: 0.9825 F1: 0.9825

Epoch 200/399
----------
train Loss: 0.0108 Acc: 0.9825 F1: 0.9825

Epoch 201/399
----------
train Loss: 0.0104 Acc: 0.9832 F1: 0.9832

Epoch 202/399
----------
train Loss: 0.0377 Acc: 0.9476 F1: 0.9472

Epoch 203/399
----------
train Loss: 0.0252 Acc: 0.9621 F1: 0.9619

Epoch 204/399
----------
train Loss: 0.0155 Acc: 0.9758 F1: 0.9757

Epoch 205/399
----------
train Loss: 0.0201 Acc: 0.9705 F1: 0.9704

Epoch 206/399
----------
train Loss: 0.0161 Acc: 0.9751 F1: 0.9750

Epoch 207/399
----------
train Loss: 0.0163 Acc: 0.9737 F1: 0.9736

Epoch 208/399
----------
train Loss: 0.0138 Acc: 0.9785 F1: 0.9784

Epoch 209/399
----------
train Loss: 0.0120 Acc: 0.9808 F1: 0.9808

Epoch 210/399
----------
train Loss: 0.0126 Acc: 0.9799 F1: 0.9799

Epoch 211/399
----------
train Loss: 0.0113 Acc: 0.9819 F1: 0.9819

Epoch 212/399
----------
train Loss: 0.0110 Acc: 0.9823 F1: 0.9823

Epoch 213/399
----------
train Loss: 0.0110 Acc: 0.9824 F1: 0.9824

Epoch 214/399
----------
train Loss: 0.0127 Acc: 0.9796 F1: 0.9795

Epoch 215/399
----------
train Loss: 0.0113 Acc: 0.9818 F1: 0.9818

Epoch 216/399
----------
train Loss: 0.0108 Acc: 0.9824 F1: 0.9824

Epoch 217/399
----------
train Loss: 0.0109 Acc: 0.9825 F1: 0.9825

Epoch 218/399
----------
train Loss: 0.0108 Acc: 0.9826 F1: 0.9825

Epoch 219/399
----------
train Loss: 0.0103 Acc: 0.9832 F1: 0.9832

Epoch 220/399
----------
train Loss: 0.0102 Acc: 0.9835 F1: 0.9835

Epoch 221/399
----------
train Loss: 0.0099 Acc: 0.9839 F1: 0.9839

Epoch 222/399
----------
train Loss: 0.0100 Acc: 0.9838 F1: 0.9838

Epoch 223/399
----------
train Loss: 0.0099 Acc: 0.9840 F1: 0.9840

Epoch 224/399
----------
train Loss: 0.0098 Acc: 0.9840 F1: 0.9840

Epoch 225/399
----------
train Loss: 0.0097 Acc: 0.9843 F1: 0.9843

Epoch 226/399
----------
train Loss: 0.0097 Acc: 0.9842 F1: 0.9842

Epoch 227/399
----------
train Loss: 0.0097 Acc: 0.9844 F1: 0.9844

Epoch 228/399
----------
train Loss: 0.0097 Acc: 0.9843 F1: 0.9843

Epoch 229/399
----------
train Loss: 0.0104 Acc: 0.9832 F1: 0.9832

Epoch 230/399
----------
train Loss: 0.0099 Acc: 0.9839 F1: 0.9838

Epoch 231/399
----------
train Loss: 0.0095 Acc: 0.9846 F1: 0.9846

Epoch 232/399
----------
train Loss: 0.0094 Acc: 0.9847 F1: 0.9847

Epoch 233/399
----------
train Loss: 0.0092 Acc: 0.9849 F1: 0.9849

Epoch 234/399
----------
train Loss: 0.0091 Acc: 0.9853 F1: 0.9853

Epoch 235/399
----------
train Loss: 0.0092 Acc: 0.9851 F1: 0.9851

Epoch 236/399
----------
train Loss: 0.0094 Acc: 0.9847 F1: 0.9847

Epoch 237/399
----------
train Loss: 0.0094 Acc: 0.9846 F1: 0.9846

Epoch 238/399
----------
train Loss: 0.0098 Acc: 0.9839 F1: 0.9839

Epoch 239/399
----------
train Loss: 0.0094 Acc: 0.9848 F1: 0.9848

Epoch 240/399
----------
train Loss: 0.0094 Acc: 0.9847 F1: 0.9847

Epoch 241/399
----------
train Loss: 0.0095 Acc: 0.9846 F1: 0.9846

Epoch 242/399
----------
train Loss: 0.0097 Acc: 0.9843 F1: 0.9843

Epoch 243/399
----------
train Loss: 0.0090 Acc: 0.9854 F1: 0.9854

Epoch 244/399
----------
train Loss: 0.0092 Acc: 0.9851 F1: 0.9851

Epoch 245/399
----------
train Loss: 0.0092 Acc: 0.9850 F1: 0.9850

Epoch 246/399
----------
train Loss: 0.0091 Acc: 0.9852 F1: 0.9852

Epoch 247/399
----------
train Loss: 0.0108 Acc: 0.9831 F1: 0.9831

Epoch 248/399
----------
train Loss: 0.0100 Acc: 0.9843 F1: 0.9843

Epoch 249/399
----------
train Loss: 0.0094 Acc: 0.9848 F1: 0.9848

Epoch 250/399
----------
train Loss: 0.0094 Acc: 0.9847 F1: 0.9847

Epoch 251/399
----------
train Loss: 0.0091 Acc: 0.9854 F1: 0.9854

Epoch 252/399
----------
train Loss: 0.0088 Acc: 0.9860 F1: 0.9860

Epoch 253/399
----------
train Loss: 0.0094 Acc: 0.9849 F1: 0.9849

Epoch 254/399
----------
train Loss: 0.0125 Acc: 0.9811 F1: 0.9811

Epoch 255/399
----------
train Loss: 0.0138 Acc: 0.9801 F1: 0.9801

Epoch 256/399
----------
train Loss: 0.0120 Acc: 0.9811 F1: 0.9811

Epoch 257/399
----------
train Loss: 0.0136 Acc: 0.9825 F1: 0.9825

Epoch 258/399
----------
train Loss: 0.0124 Acc: 0.9811 F1: 0.9811

Epoch 259/399
----------
train Loss: 0.0099 Acc: 0.9840 F1: 0.9840

Epoch 260/399
----------
train Loss: 0.0091 Acc: 0.9853 F1: 0.9853

Epoch 261/399
----------
train Loss: 0.0092 Acc: 0.9854 F1: 0.9854

Epoch 262/399
----------
train Loss: 0.0087 Acc: 0.9861 F1: 0.9861

Epoch 263/399
----------
train Loss: 0.0090 Acc: 0.9856 F1: 0.9856

Epoch 264/399
----------
train Loss: 0.0090 Acc: 0.9855 F1: 0.9855

Epoch 265/399
----------
train Loss: 0.0093 Acc: 0.9851 F1: 0.9851

Epoch 266/399
----------
train Loss: 0.0087 Acc: 0.9860 F1: 0.9860

Epoch 267/399
----------
train Loss: 0.0088 Acc: 0.9857 F1: 0.9857

Epoch 268/399
----------
train Loss: 0.0088 Acc: 0.9858 F1: 0.9858

Epoch 269/399
----------
train Loss: 0.0089 Acc: 0.9856 F1: 0.9856

Epoch 270/399
----------
train Loss: 0.0086 Acc: 0.9861 F1: 0.9861

Epoch 271/399
----------
train Loss: 0.0086 Acc: 0.9862 F1: 0.9861

Epoch 272/399
----------
train Loss: 0.0083 Acc: 0.9868 F1: 0.9868

Epoch 273/399
----------
train Loss: 0.0081 Acc: 0.9868 F1: 0.9868

Epoch 274/399
----------
train Loss: 0.0085 Acc: 0.9863 F1: 0.9863

Epoch 275/399
----------
train Loss: 0.0081 Acc: 0.9869 F1: 0.9869

Epoch 276/399
----------
train Loss: 0.0085 Acc: 0.9864 F1: 0.9864

Epoch 277/399
----------
train Loss: 0.0089 Acc: 0.9857 F1: 0.9857

Epoch 278/399
----------
train Loss: 0.0088 Acc: 0.9860 F1: 0.9859

Epoch 279/399
----------
train Loss: 0.0086 Acc: 0.9862 F1: 0.9862

Epoch 280/399
----------
train Loss: 0.0087 Acc: 0.9861 F1: 0.9861

Epoch 281/399
----------
train Loss: 0.0085 Acc: 0.9862 F1: 0.9862

Epoch 282/399
----------
train Loss: 0.0081 Acc: 0.9869 F1: 0.9869

Epoch 283/399
----------
train Loss: 0.0083 Acc: 0.9866 F1: 0.9866

Epoch 284/399
----------
train Loss: 0.0081 Acc: 0.9869 F1: 0.9869

Epoch 285/399
----------
train Loss: 0.0088 Acc: 0.9859 F1: 0.9859

Epoch 286/399
----------
train Loss: 0.0085 Acc: 0.9864 F1: 0.9864

Epoch 287/399
----------
train Loss: 0.0083 Acc: 0.9868 F1: 0.9868

Epoch 288/399
----------
train Loss: 0.0082 Acc: 0.9868 F1: 0.9868

Epoch 289/399
----------
train Loss: 0.0085 Acc: 0.9862 F1: 0.9862

Epoch 290/399
----------
train Loss: 0.0083 Acc: 0.9867 F1: 0.9867

Epoch 291/399
----------
train Loss: 0.0085 Acc: 0.9862 F1: 0.9862

Epoch 292/399
----------
train Loss: 0.0112 Acc: 0.9820 F1: 0.9820

Epoch 293/399
----------
train Loss: 0.0107 Acc: 0.9831 F1: 0.9830

Epoch 294/399
----------
train Loss: 0.0277 Acc: 0.9614 F1: 0.9612

Epoch 295/399
----------
train Loss: 0.0155 Acc: 0.9762 F1: 0.9761

Epoch 296/399
----------
train Loss: 0.0108 Acc: 0.9829 F1: 0.9829

Epoch 297/399
----------
train Loss: 0.0093 Acc: 0.9852 F1: 0.9852

Epoch 298/399
----------
train Loss: 0.0094 Acc: 0.9848 F1: 0.9848

Epoch 299/399
----------
train Loss: 0.0090 Acc: 0.9857 F1: 0.9857

Epoch 300/399
----------
train Loss: 0.0086 Acc: 0.9863 F1: 0.9863

Epoch 301/399
----------
train Loss: 0.0082 Acc: 0.9869 F1: 0.9869

Epoch 302/399
----------
train Loss: 0.0077 Acc: 0.9876 F1: 0.9876

Epoch 303/399
----------
train Loss: 0.0083 Acc: 0.9867 F1: 0.9867

Epoch 304/399
----------
train Loss: 0.0078 Acc: 0.9876 F1: 0.9876

Epoch 305/399
----------
train Loss: 0.0083 Acc: 0.9866 F1: 0.9866

Epoch 306/399
----------
train Loss: 0.0080 Acc: 0.9870 F1: 0.9870

Epoch 307/399
----------
train Loss: 0.0077 Acc: 0.9877 F1: 0.9876

Epoch 308/399
----------
train Loss: 0.0080 Acc: 0.9872 F1: 0.9872

Epoch 309/399
----------
train Loss: 0.0077 Acc: 0.9877 F1: 0.9877

Epoch 310/399
----------
train Loss: 0.0078 Acc: 0.9874 F1: 0.9874

Epoch 311/399
----------
train Loss: 0.0074 Acc: 0.9880 F1: 0.9880

Epoch 312/399
----------
train Loss: 0.0077 Acc: 0.9876 F1: 0.9876

Epoch 313/399
----------
train Loss: 0.0081 Acc: 0.9868 F1: 0.9868

Epoch 314/399
----------
train Loss: 0.0076 Acc: 0.9878 F1: 0.9878

Epoch 315/399
----------
train Loss: 0.0073 Acc: 0.9882 F1: 0.9882

Epoch 316/399
----------
train Loss: 0.0074 Acc: 0.9882 F1: 0.9882

Epoch 317/399
----------
train Loss: 0.0072 Acc: 0.9885 F1: 0.9885

Epoch 318/399
----------
train Loss: 0.0076 Acc: 0.9877 F1: 0.9877

Epoch 319/399
----------
train Loss: 0.0077 Acc: 0.9875 F1: 0.9875

Epoch 320/399
----------
train Loss: 0.0076 Acc: 0.9878 F1: 0.9878

Epoch 321/399
----------
train Loss: 0.0075 Acc: 0.9879 F1: 0.9879

Epoch 322/399
----------
train Loss: 0.0072 Acc: 0.9883 F1: 0.9883

Epoch 323/399
----------
train Loss: 0.0073 Acc: 0.9883 F1: 0.9883

Epoch 324/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885

Epoch 325/399
----------
train Loss: 0.0070 Acc: 0.9887 F1: 0.9887

Epoch 326/399
----------
train Loss: 0.0105 Acc: 0.9835 F1: 0.9835

Epoch 327/399
----------
train Loss: 0.0088 Acc: 0.9861 F1: 0.9861

Epoch 328/399
----------
train Loss: 0.0074 Acc: 0.9883 F1: 0.9883

Epoch 329/399
----------
train Loss: 0.0073 Acc: 0.9882 F1: 0.9882

Epoch 330/399
----------
train Loss: 0.0071 Acc: 0.9883 F1: 0.9883

Epoch 331/399
----------
train Loss: 0.0071 Acc: 0.9886 F1: 0.9886

Epoch 332/399
----------
train Loss: 0.0070 Acc: 0.9887 F1: 0.9887

Epoch 333/399
----------
train Loss: 0.0069 Acc: 0.9889 F1: 0.9889

Epoch 334/399
----------
train Loss: 0.0076 Acc: 0.9879 F1: 0.9879

Epoch 335/399
----------
train Loss: 0.0071 Acc: 0.9887 F1: 0.9887

Epoch 336/399
----------
train Loss: 0.0072 Acc: 0.9885 F1: 0.9885

Epoch 337/399
----------
train Loss: 0.0071 Acc: 0.9887 F1: 0.9887

Epoch 338/399
----------
train Loss: 0.0067 Acc: 0.9893 F1: 0.9893

Epoch 339/399
----------
train Loss: 0.0066 Acc: 0.9894 F1: 0.9894

Epoch 340/399
----------
train Loss: 0.0067 Acc: 0.9892 F1: 0.9892

Epoch 341/399
----------
train Loss: 0.0069 Acc: 0.9888 F1: 0.9888

Epoch 342/399
----------
train Loss: 0.0070 Acc: 0.9886 F1: 0.9886

Epoch 343/399
----------
train Loss: 0.0071 Acc: 0.9885 F1: 0.9885

Epoch 344/399
----------
train Loss: 0.0070 Acc: 0.9888 F1: 0.9888

Epoch 345/399
----------
train Loss: 0.0067 Acc: 0.9893 F1: 0.9893

Epoch 346/399
----------
train Loss: 0.0066 Acc: 0.9895 F1: 0.9895

Epoch 347/399
----------
train Loss: 0.0140 Acc: 0.9794 F1: 0.9794

Epoch 348/399
----------
train Loss: 0.0115 Acc: 0.9826 F1: 0.9826

Epoch 349/399
----------
train Loss: 0.0088 Acc: 0.9861 F1: 0.9861

Epoch 350/399
----------
train Loss: 0.0079 Acc: 0.9875 F1: 0.9875

Epoch 351/399
----------
train Loss: 0.0078 Acc: 0.9876 F1: 0.9876

Epoch 352/399
----------
train Loss: 0.0078 Acc: 0.9876 F1: 0.9876

Epoch 353/399
----------
train Loss: 0.0074 Acc: 0.9881 F1: 0.9881

Epoch 354/399
----------
train Loss: 0.0073 Acc: 0.9883 F1: 0.9883

Epoch 355/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884

Epoch 356/399
----------
train Loss: 0.0071 Acc: 0.9886 F1: 0.9886

Epoch 357/399
----------
train Loss: 0.0074 Acc: 0.9884 F1: 0.9884

Epoch 358/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884

Epoch 359/399
----------
train Loss: 0.0079 Acc: 0.9873 F1: 0.9873

Epoch 360/399
----------
train Loss: 0.0074 Acc: 0.9883 F1: 0.9883

Epoch 361/399
----------
train Loss: 0.0070 Acc: 0.9887 F1: 0.9887

Epoch 362/399
----------
train Loss: 0.0072 Acc: 0.9884 F1: 0.9884

Epoch 363/399
----------
train Loss: 0.0063 Acc: 0.9900 F1: 0.9900

Epoch 364/399
----------
train Loss: 0.0066 Acc: 0.9894 F1: 0.9894

Epoch 365/399
----------
train Loss: 0.0068 Acc: 0.9890 F1: 0.9890

Epoch 366/399
----------
train Loss: 0.0068 Acc: 0.9891 F1: 0.9891

Epoch 367/399
----------
train Loss: 0.0068 Acc: 0.9890 F1: 0.9890

Epoch 368/399
----------
train Loss: 0.0066 Acc: 0.9895 F1: 0.9895

Epoch 369/399
----------
train Loss: 0.0065 Acc: 0.9896 F1: 0.9896

Epoch 370/399
----------
train Loss: 0.0065 Acc: 0.9895 F1: 0.9895

Epoch 371/399
----------
train Loss: 0.0062 Acc: 0.9899 F1: 0.9899

Epoch 372/399
----------
train Loss: 0.0066 Acc: 0.9895 F1: 0.9895

Epoch 373/399
----------
train Loss: 0.0061 Acc: 0.9903 F1: 0.9903

Epoch 374/399
----------
train Loss: 0.0066 Acc: 0.9895 F1: 0.9895

Epoch 375/399
----------
train Loss: 0.0067 Acc: 0.9893 F1: 0.9893

Epoch 376/399
----------
train Loss: 0.0067 Acc: 0.9890 F1: 0.9890

Epoch 377/399
----------
train Loss: 0.0066 Acc: 0.9895 F1: 0.9895

Epoch 378/399
----------
train Loss: 0.0066 Acc: 0.9895 F1: 0.9895

Epoch 379/399
----------
train Loss: 0.0065 Acc: 0.9895 F1: 0.9895

Epoch 380/399
----------
train Loss: 0.0065 Acc: 0.9896 F1: 0.9896

Epoch 381/399
----------
train Loss: 0.0066 Acc: 0.9893 F1: 0.9893

Epoch 382/399
----------
train Loss: 0.0066 Acc: 0.9894 F1: 0.9894

Epoch 383/399
----------
train Loss: 0.0065 Acc: 0.9897 F1: 0.9897

Epoch 384/399
----------
train Loss: 0.0064 Acc: 0.9898 F1: 0.9898

Epoch 385/399
----------
train Loss: 0.0064 Acc: 0.9897 F1: 0.9897

Epoch 386/399
----------
train Loss: 0.0062 Acc: 0.9900 F1: 0.9900

Epoch 387/399
----------
train Loss: 0.0064 Acc: 0.9896 F1: 0.9896

Epoch 388/399
----------
train Loss: 0.0061 Acc: 0.9903 F1: 0.9903

Epoch 389/399
----------
train Loss: 0.0066 Acc: 0.9893 F1: 0.9893

Epoch 390/399
----------
train Loss: 0.0062 Acc: 0.9901 F1: 0.9901

Epoch 391/399
----------
train Loss: 0.0071 Acc: 0.9886 F1: 0.9886

Epoch 392/399
----------
train Loss: 0.0060 Acc: 0.9904 F1: 0.9904

Epoch 393/399
----------
train Loss: 0.0061 Acc: 0.9903 F1: 0.9903

Epoch 394/399
----------
train Loss: 0.0059 Acc: 0.9904 F1: 0.9904

Epoch 395/399
----------
train Loss: 0.0065 Acc: 0.9896 F1: 0.9896

Epoch 396/399
----------
train Loss: 0.0061 Acc: 0.9903 F1: 0.9903

Epoch 397/399
----------
train Loss: 0.0063 Acc: 0.9899 F1: 0.9899

Epoch 398/399
----------
train Loss: 0.0062 Acc: 0.9901 F1: 0.9901

Epoch 399/399
----------
train Loss: 0.0063 Acc: 0.9898 F1: 0.9898

Training complete in 641m 49s
Best train F1: 0.9904
Saving..
Successfully retrieved statistics for job: zhangz65-gpu05-4409379_. 
+------------------------------------------------------------------------------+
| GPU ID: 2                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Sun Mar 17 19:02:33 2024                |
| End Time                           | Mon Mar 18 05:46:25 2024                |
| Total Execution Time (sec)         | 38632.3                                 |
| No. of Processes                   | 16                                      |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 258126                                  |
| Power Usage (Watts)                | Avg: 69.9863, Max: 229.186, Min: 35.142 |
| Max GPU Memory Used (bytes)        | 9034530816                              |
| SM Clock (MHz)                     | Avg: 1272, Max: 1380, Min: 1230         |
| Memory Clock (MHz)                 | Avg: 877, Max: 877, Min: 877            |
| SM Utilization (%)                 | Avg: 21, Max: 100, Min: 0               |
| Memory Utilization (%)             | Avg: 11, Max: 55, Min: 0                |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 3765226                                 |
|     Avg SM Utilization (%)         | 11                                      |
|     Avg Memory Utilization (%)     | 5                                       |
| PID                                | 3841793                                 |
|     Avg SM Utilization (%)         | Not Found                               |
|     Avg Memory Utilization (%)     | Not Found                               |
| PID                                | 3841794                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3841791                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3841792                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3841998                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3841996                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3841995                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3841997                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3842204                                 |
|     Avg SM Utilization (%)         | Not Found                               |
|     Avg Memory Utilization (%)     | Not Found                               |
| PID                                | 3842202                                 |
|     Avg SM Utilization (%)         | Not Found                               |
|     Avg Memory Utilization (%)     | Not Found                               |
| PID                                | 3842418                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3842420                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
| PID                                | 3842419                                 |
|     Avg SM Utilization (%)         | Not Found                               |
|     Avg Memory Utilization (%)     | Not Found                               |
| PID                                | 3842421                                 |
|     Avg SM Utilization (%)         | Not Found                               |
|     Avg Memory Utilization (%)     | Not Found                               |
| PID                                | 3842627                                 |
|     Avg SM Utilization (%)         | 0                                       |
|     Avg Memory Utilization (%)     | 0                                       |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+

