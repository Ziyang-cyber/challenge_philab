2.2.0
test
DataLoaders created successfully!
Model(
  (backbone): SwinBackbone(
    (backbone): SwinTransformer(
      (features): Sequential(
        (0): Sequential(
          (0): Conv2d(7, 128, kernel_size=(4, 4), stride=(4, 4))
          (1): Permute()
          (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (1): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=4, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=4, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): PatchMergingV2(
          (reduction): Linear(in_features=512, out_features=256, bias=False)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=8, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (4): PatchMergingV2(
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (5): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlockV2(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=16, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (6): PatchMergingV2(
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (7): Sequential(
          (0): SwinTransformerBlockV2(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=32, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlockV2(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttentionV2(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (cpb_mlp): Sequential(
                (0): Linear(in_features=2, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=512, out_features=32, bias=False)
              )
            )
            (stochastic_depth): StochasticDepth(p=0.5, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate='none')
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (permute): Permute()
      (avgpool): AdaptiveAvgPool2d(output_size=1)
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (head): Linear(in_features=1024, out_features=1000, bias=True)
    )
  )
  (fpn): FPN(
    (fpn): FeaturePyramidNetwork(
      (inner_blocks): ModuleList(
        (0): Conv2dNormActivation(
          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Conv2dNormActivation(
          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Conv2dNormActivation(
          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Conv2dNormActivation(
          (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (layer_blocks): ModuleList(
        (0-3): 4 x Conv2dNormActivation(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
  )
  (head): SimpleHead(
    (layers): Sequential(
      (0): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
      (1): Conv2d(128, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
)
Epoch 0/199
----------
train Loss: 0.5233 Acc: 0.0503
val Loss: 0.4685 Acc: 0.0554

Epoch 1/199
----------
train Loss: 0.4625 Acc: 0.0560
val Loss: 0.4533 Acc: 0.0550

Epoch 2/199
----------
train Loss: 0.4513 Acc: 0.0562
val Loss: 0.4490 Acc: 0.0544

Epoch 3/199
----------
train Loss: 0.4516 Acc: 0.0552
val Loss: 0.4543 Acc: 0.0566

Epoch 4/199
----------
train Loss: 0.4471 Acc: 0.0557
val Loss: 0.4456 Acc: 0.0557

Epoch 5/199
----------
train Loss: 0.4433 Acc: 0.0556
val Loss: 0.4429 Acc: 0.0547

Epoch 6/199
----------
train Loss: 0.4413 Acc: 0.0558
val Loss: 0.4399 Acc: 0.0545

Epoch 7/199
----------
train Loss: 0.4395 Acc: 0.0556
val Loss: 0.4386 Acc: 0.0555

Epoch 8/199
----------
train Loss: 0.4398 Acc: 0.0555
val Loss: 0.4386 Acc: 0.0559

Epoch 9/199
----------
train Loss: 0.4384 Acc: 0.0555
val Loss: 0.4389 Acc: 0.0558

Epoch 10/199
----------
train Loss: 0.4378 Acc: 0.0554
val Loss: 0.4431 Acc: 0.0561

Epoch 11/199
----------
train Loss: 0.4386 Acc: 0.0558
val Loss: 0.4359 Acc: 0.0553

Epoch 12/199
----------
train Loss: 0.4361 Acc: 0.0563
val Loss: 0.4415 Acc: 0.0523

Epoch 13/199
----------
train Loss: 0.4372 Acc: 0.0554
val Loss: 0.4292 Acc: 0.0552

Epoch 14/199
----------
train Loss: 0.4338 Acc: 0.0557
val Loss: 0.4291 Acc: 0.0556

Epoch 15/199
----------
train Loss: 0.4318 Acc: 0.0556
val Loss: 0.4286 Acc: 0.0557

Epoch 16/199
----------
train Loss: 0.4275 Acc: 0.0561
val Loss: 0.4250 Acc: 0.0546

Epoch 17/199
----------
train Loss: 0.4278 Acc: 0.0562
val Loss: 0.4243 Acc: 0.0545

Epoch 18/199
----------
train Loss: 0.4303 Acc: 0.0558
val Loss: 0.4242 Acc: 0.0539

Epoch 19/199
----------
train Loss: 0.4298 Acc: 0.0561
val Loss: 0.4246 Acc: 0.0561

Epoch 20/199
----------
train Loss: 0.4256 Acc: 0.0558
val Loss: 0.4336 Acc: 0.0564

Epoch 21/199
----------
train Loss: 0.4282 Acc: 0.0559
val Loss: 0.4361 Acc: 0.0564

Epoch 22/199
----------
train Loss: 0.4315 Acc: 0.0557
val Loss: 0.4313 Acc: 0.0562

Epoch 23/199
----------
train Loss: 0.4242 Acc: 0.0560
val Loss: 0.4302 Acc: 0.0562

Epoch 24/199
----------
train Loss: 0.4244 Acc: 0.0558
val Loss: 0.4241 Acc: 0.0558

Epoch 25/199
----------
train Loss: 0.4239 Acc: 0.0560
val Loss: 0.4219 Acc: 0.0561

Epoch 26/199
----------
train Loss: 0.4207 Acc: 0.0563
val Loss: 0.4240 Acc: 0.0560

Epoch 27/199
----------
train Loss: 0.4211 Acc: 0.0562
val Loss: 0.4181 Acc: 0.0555

Epoch 28/199
----------
train Loss: 0.4320 Acc: 0.0557
val Loss: 0.4216 Acc: 0.0555

Epoch 29/199
----------
train Loss: 0.4277 Acc: 0.0556
val Loss: 0.4214 Acc: 0.0557

Epoch 30/199
----------
train Loss: 0.4233 Acc: 0.0558
val Loss: 0.4193 Acc: 0.0558

Epoch 31/199
----------
train Loss: 0.4210 Acc: 0.0564
val Loss: 0.4201 Acc: 0.0560

Epoch 32/199
----------
train Loss: 0.4230 Acc: 0.0563
val Loss: 0.4196 Acc: 0.0558

Epoch 33/199
----------
train Loss: 0.4203 Acc: 0.0562
val Loss: 0.4191 Acc: 0.0561

Epoch 34/199
----------
train Loss: 0.4171 Acc: 0.0564
val Loss: 0.4129 Acc: 0.0554

Epoch 35/199
----------
train Loss: 0.4136 Acc: 0.0566
val Loss: 0.4128 Acc: 0.0554

Epoch 36/199
----------
train Loss: 0.4196 Acc: 0.0563
val Loss: 0.4199 Acc: 0.0565

Epoch 37/199
----------
train Loss: 0.4120 Acc: 0.0564
val Loss: 0.4157 Acc: 0.0557

Epoch 38/199
----------
train Loss: 0.4143 Acc: 0.0564
val Loss: 0.4141 Acc: 0.0557

Epoch 39/199
----------
train Loss: 0.4170 Acc: 0.0564
val Loss: 0.4153 Acc: 0.0562

Epoch 40/199
----------
train Loss: 0.4167 Acc: 0.0561
val Loss: 0.4171 Acc: 0.0563

Epoch 41/199
----------
train Loss: 0.4129 Acc: 0.0565
val Loss: 0.4155 Acc: 0.0566

Epoch 42/199
----------
train Loss: 0.4171 Acc: 0.0563
val Loss: 0.4162 Acc: 0.0566

Epoch 43/199
----------
train Loss: 0.4114 Acc: 0.0567
val Loss: 0.4119 Acc: 0.0561

Epoch 44/199
----------
train Loss: 0.4124 Acc: 0.0564
val Loss: 0.4127 Acc: 0.0564

Epoch 45/199
----------
train Loss: 0.4089 Acc: 0.0566
val Loss: 0.4116 Acc: 0.0564

Epoch 46/199
----------
train Loss: 0.4089 Acc: 0.0567
val Loss: 0.4120 Acc: 0.0560

Epoch 47/199
----------
train Loss: 0.4087 Acc: 0.0566
val Loss: 0.4084 Acc: 0.0559

Epoch 48/199
----------
train Loss: 0.4128 Acc: 0.0565
val Loss: 0.4098 Acc: 0.0563

Epoch 49/199
----------
train Loss: 0.4087 Acc: 0.0567
val Loss: 0.4145 Acc: 0.0569

Epoch 50/199
----------
train Loss: 0.4074 Acc: 0.0567
val Loss: 0.4122 Acc: 0.0568

Epoch 51/199
----------
train Loss: 0.4081 Acc: 0.0567
val Loss: 0.4091 Acc: 0.0567

Epoch 52/199
----------
train Loss: 0.4051 Acc: 0.0568
val Loss: 0.4076 Acc: 0.0562

Epoch 53/199
----------
train Loss: 0.4149 Acc: 0.0566
val Loss: 0.4175 Acc: 0.0540

Epoch 54/199
----------
train Loss: 0.4128 Acc: 0.0564
val Loss: 0.4076 Acc: 0.0552

Epoch 55/199
----------
train Loss: 0.4097 Acc: 0.0564
val Loss: 0.4123 Acc: 0.0563

Epoch 56/199
----------
train Loss: 0.4087 Acc: 0.0567
val Loss: 0.4137 Acc: 0.0571

Epoch 57/199
----------
train Loss: 0.4066 Acc: 0.0570
val Loss: 0.4064 Acc: 0.0564

Epoch 58/199
----------
train Loss: 0.4049 Acc: 0.0568
val Loss: 0.4061 Acc: 0.0571

Epoch 59/199
----------
train Loss: 0.4085 Acc: 0.0565
val Loss: 0.4066 Acc: 0.0566

Epoch 60/199
----------
train Loss: 0.4044 Acc: 0.0569
val Loss: 0.4156 Acc: 0.0569

Epoch 61/199
----------
train Loss: 0.4056 Acc: 0.0567
val Loss: 0.4059 Acc: 0.0569

Epoch 62/199
----------
train Loss: 0.4031 Acc: 0.0569
val Loss: 0.4157 Acc: 0.0571

Epoch 63/199
----------
train Loss: 0.4055 Acc: 0.0570
val Loss: 0.4030 Acc: 0.0566

Epoch 64/199
----------
train Loss: 0.4026 Acc: 0.0569
val Loss: 0.4067 Acc: 0.0569

Epoch 65/199
----------
train Loss: 0.4080 Acc: 0.0570
val Loss: 0.4061 Acc: 0.0568

Epoch 66/199
----------
train Loss: 0.4036 Acc: 0.0566
val Loss: 0.4067 Acc: 0.0571

Epoch 67/199
----------
train Loss: 0.4118 Acc: 0.0566
val Loss: 0.4032 Acc: 0.0570

Epoch 68/199
----------
train Loss: 0.4040 Acc: 0.0569
val Loss: 0.4014 Acc: 0.0564

Epoch 69/199
----------
train Loss: 0.4074 Acc: 0.0567
val Loss: 0.4097 Acc: 0.0562

Epoch 70/199
----------
train Loss: 0.4029 Acc: 0.0569
val Loss: 0.4032 Acc: 0.0554

Epoch 71/199
----------
train Loss: 0.4040 Acc: 0.0567
val Loss: 0.4111 Acc: 0.0550

Epoch 72/199
----------
train Loss: 0.4046 Acc: 0.0566
val Loss: 0.4013 Acc: 0.0570

Epoch 73/199
----------
train Loss: 0.4039 Acc: 0.0571
val Loss: 0.3993 Acc: 0.0567

Epoch 74/199
----------
train Loss: 0.4015 Acc: 0.0571
val Loss: 0.4005 Acc: 0.0562

Epoch 75/199
----------
train Loss: 0.4003 Acc: 0.0569
val Loss: 0.3991 Acc: 0.0566

Epoch 76/199
----------
train Loss: 0.4001 Acc: 0.0570
val Loss: 0.3984 Acc: 0.0567

Epoch 77/199
----------
train Loss: 0.3992 Acc: 0.0573
val Loss: 0.4000 Acc: 0.0561

Epoch 78/199
----------
train Loss: 0.4025 Acc: 0.0567
val Loss: 0.4015 Acc: 0.0571

Epoch 79/199
----------
train Loss: 0.4002 Acc: 0.0571
val Loss: 0.3974 Acc: 0.0571

Epoch 80/199
----------
train Loss: 0.3999 Acc: 0.0570
val Loss: 0.4017 Acc: 0.0572

Epoch 81/199
----------
train Loss: 0.4050 Acc: 0.0566
val Loss: 0.4057 Acc: 0.0572

Epoch 82/199
----------
train Loss: 0.4046 Acc: 0.0571
val Loss: 0.3998 Acc: 0.0571

Epoch 83/199
----------
train Loss: 0.4027 Acc: 0.0570
val Loss: 0.4052 Acc: 0.0555

Epoch 84/199
----------
train Loss: 0.4014 Acc: 0.0567
val Loss: 0.3986 Acc: 0.0571

Epoch 85/199
----------
train Loss: 0.3983 Acc: 0.0569
val Loss: 0.3991 Acc: 0.0568

Epoch 86/199
----------
train Loss: 0.4002 Acc: 0.0571
val Loss: 0.4032 Acc: 0.0574

Epoch 87/199
----------
train Loss: 0.3991 Acc: 0.0572
val Loss: 0.3980 Acc: 0.0570

Epoch 88/199
----------
train Loss: 0.3935 Acc: 0.0574
val Loss: 0.3986 Acc: 0.0569

Epoch 89/199
----------
train Loss: 0.3951 Acc: 0.0573
val Loss: 0.3987 Acc: 0.0569

Epoch 90/199
----------
train Loss: 0.3970 Acc: 0.0574
val Loss: 0.3988 Acc: 0.0565

Epoch 91/199
----------
train Loss: 0.4005 Acc: 0.0568
val Loss: 0.3972 Acc: 0.0573

Epoch 92/199
----------
train Loss: 0.3962 Acc: 0.0572
val Loss: 0.3974 Acc: 0.0570

Epoch 93/199
----------
train Loss: 0.3943 Acc: 0.0572
val Loss: 0.4015 Acc: 0.0573

Epoch 94/199
----------
train Loss: 0.3945 Acc: 0.0572
val Loss: 0.4010 Acc: 0.0574

Epoch 95/199
----------
train Loss: 0.3948 Acc: 0.0574
val Loss: 0.3977 Acc: 0.0569

Epoch 96/199
----------
train Loss: 0.3951 Acc: 0.0571
val Loss: 0.4008 Acc: 0.0573

Epoch 97/199
----------
train Loss: 0.3946 Acc: 0.0572
val Loss: 0.3999 Acc: 0.0574

Epoch 98/199
----------
train Loss: 0.3961 Acc: 0.0574
val Loss: 0.3956 Acc: 0.0571

Epoch 99/199
----------
train Loss: 0.3962 Acc: 0.0571
val Loss: 0.3957 Acc: 0.0572

Epoch 100/199
----------
train Loss: 0.3955 Acc: 0.0571
val Loss: 0.3959 Acc: 0.0571

Epoch 101/199
----------
train Loss: 0.3955 Acc: 0.0569
val Loss: 0.4026 Acc: 0.0576

Epoch 102/199
----------
train Loss: 0.3949 Acc: 0.0572
val Loss: 0.3998 Acc: 0.0574

Epoch 103/199
----------
train Loss: 0.3956 Acc: 0.0574
val Loss: 0.3918 Acc: 0.0571

Epoch 104/199
----------
train Loss: 0.3922 Acc: 0.0573
val Loss: 0.3918 Acc: 0.0570

Epoch 105/199
----------
train Loss: 0.3969 Acc: 0.0571
val Loss: 0.3974 Acc: 0.0575

Epoch 106/199
----------
train Loss: 0.3935 Acc: 0.0574
val Loss: 0.3988 Acc: 0.0575

Epoch 107/199
----------
train Loss: 0.3915 Acc: 0.0574
val Loss: 0.3969 Acc: 0.0575

Epoch 108/199
----------
train Loss: 0.3915 Acc: 0.0575
val Loss: 0.3952 Acc: 0.0574

Epoch 109/199
----------
train Loss: 0.3923 Acc: 0.0574
val Loss: 0.3954 Acc: 0.0573

Epoch 110/199
----------
train Loss: 0.3930 Acc: 0.0574
val Loss: 0.3920 Acc: 0.0573

Epoch 111/199
----------
train Loss: 0.3918 Acc: 0.0573
val Loss: 0.3948 Acc: 0.0576

Epoch 112/199
----------
train Loss: 0.3906 Acc: 0.0573
val Loss: 0.3957 Acc: 0.0573

Epoch 113/199
----------
train Loss: 0.3889 Acc: 0.0577
val Loss: 0.3946 Acc: 0.0565

Epoch 114/199
----------
train Loss: 0.3934 Acc: 0.0572
val Loss: 0.3958 Acc: 0.0576

Epoch 115/199
----------
train Loss: 0.3912 Acc: 0.0574
val Loss: 0.4026 Acc: 0.0576

Epoch 116/199
----------
train Loss: 0.3906 Acc: 0.0574
val Loss: 0.3949 Acc: 0.0574

Epoch 117/199
----------
train Loss: 0.3914 Acc: 0.0573
val Loss: 0.4049 Acc: 0.0576

Epoch 118/199
----------
train Loss: 0.3916 Acc: 0.0575
val Loss: 0.3968 Acc: 0.0576

Epoch 119/199
----------
train Loss: 0.3896 Acc: 0.0575
val Loss: 0.3917 Acc: 0.0573

Epoch 120/199
----------
train Loss: 0.3876 Acc: 0.0576
val Loss: 0.3904 Acc: 0.0572

Epoch 121/199
----------
train Loss: 0.3941 Acc: 0.0572
val Loss: 0.3978 Acc: 0.0573

Epoch 122/199
----------
train Loss: 0.3918 Acc: 0.0572
val Loss: 0.3930 Acc: 0.0569

Epoch 123/199
----------
train Loss: 0.3921 Acc: 0.0574
val Loss: 0.3949 Acc: 0.0572

Epoch 124/199
----------
train Loss: 0.3913 Acc: 0.0575
val Loss: 0.3929 Acc: 0.0575

Epoch 125/199
----------
train Loss: 0.3925 Acc: 0.0575
val Loss: 0.3928 Acc: 0.0571

Epoch 126/199
----------
train Loss: 0.3888 Acc: 0.0576
val Loss: 0.3940 Acc: 0.0570

Epoch 127/199
----------
train Loss: 0.3888 Acc: 0.0575
val Loss: 0.3911 Acc: 0.0571

Epoch 128/199
----------
train Loss: 0.3877 Acc: 0.0576
val Loss: 0.3950 Acc: 0.0572

Epoch 129/199
----------
train Loss: 0.3893 Acc: 0.0574
val Loss: 0.3947 Acc: 0.0575

Epoch 130/199
----------
train Loss: 0.3883 Acc: 0.0576
val Loss: 0.3922 Acc: 0.0573

Epoch 131/199
----------
train Loss: 0.3869 Acc: 0.0575
val Loss: 0.3949 Acc: 0.0576

Epoch 132/199
----------
train Loss: 0.3907 Acc: 0.0576
val Loss: 0.3917 Acc: 0.0569

Epoch 133/199
----------
train Loss: 0.3900 Acc: 0.0574
val Loss: 0.3919 Acc: 0.0567

Epoch 134/199
----------
train Loss: 0.3893 Acc: 0.0576
val Loss: 0.3924 Acc: 0.0572

Epoch 135/199
----------
train Loss: 0.3917 Acc: 0.0574
val Loss: 0.3916 Acc: 0.0570

Epoch 136/199
----------
train Loss: 0.3881 Acc: 0.0575
val Loss: 0.3937 Acc: 0.0572

Epoch 137/199
----------
train Loss: 0.3886 Acc: 0.0576
val Loss: 0.3934 Acc: 0.0569

Epoch 138/199
----------
train Loss: 0.3943 Acc: 0.0573
val Loss: 0.3966 Acc: 0.0569

Epoch 139/199
----------
train Loss: 0.3878 Acc: 0.0575
val Loss: 0.3956 Acc: 0.0576

Epoch 140/199
----------
train Loss: 0.3880 Acc: 0.0576
val Loss: 0.3948 Acc: 0.0577

Epoch 141/199
----------
train Loss: 0.3883 Acc: 0.0576
val Loss: 0.3954 Acc: 0.0576

Epoch 142/199
----------
train Loss: 0.3889 Acc: 0.0574
val Loss: 0.3959 Acc: 0.0576

Epoch 143/199
----------
train Loss: 0.3911 Acc: 0.0576
val Loss: 0.3904 Acc: 0.0574

Epoch 144/199
----------
train Loss: 0.3878 Acc: 0.0575
val Loss: 0.3920 Acc: 0.0571

Epoch 145/199
----------
train Loss: 0.3876 Acc: 0.0576
val Loss: 0.3910 Acc: 0.0575

Epoch 146/199
----------
train Loss: 0.3863 Acc: 0.0576
val Loss: 0.3902 Acc: 0.0574

Epoch 147/199
----------
train Loss: 0.3855 Acc: 0.0577
val Loss: 0.3924 Acc: 0.0574

Epoch 148/199
----------
train Loss: 0.3861 Acc: 0.0575
val Loss: 0.3932 Acc: 0.0576

Epoch 149/199
----------
train Loss: 0.3897 Acc: 0.0575
val Loss: 0.3976 Acc: 0.0577

Epoch 150/199
----------
train Loss: 0.3866 Acc: 0.0577
val Loss: 0.3910 Acc: 0.0575

Epoch 151/199
----------
train Loss: 0.3860 Acc: 0.0576
val Loss: 0.3919 Acc: 0.0572

Epoch 152/199
----------
train Loss: 0.3880 Acc: 0.0575
val Loss: 0.3913 Acc: 0.0575

Epoch 153/199
----------
train Loss: 0.3848 Acc: 0.0576
val Loss: 0.3935 Acc: 0.0573

Epoch 154/199
----------
train Loss: 0.3852 Acc: 0.0577
val Loss: 0.3924 Acc: 0.0575

Epoch 155/199
----------
train Loss: 0.3859 Acc: 0.0576
val Loss: 0.3905 Acc: 0.0571

Epoch 156/199
----------
train Loss: 0.3857 Acc: 0.0576
val Loss: 0.3905 Acc: 0.0573

Epoch 157/199
----------
train Loss: 0.3854 Acc: 0.0577
val Loss: 0.3917 Acc: 0.0575

Epoch 158/199
----------
train Loss: 0.3857 Acc: 0.0577
val Loss: 0.3953 Acc: 0.0575

Epoch 159/199
----------
train Loss: 0.3833 Acc: 0.0579
val Loss: 0.3955 Acc: 0.0576

Epoch 160/199
----------
train Loss: 0.3844 Acc: 0.0576
val Loss: 0.3951 Acc: 0.0576

Epoch 161/199
----------
train Loss: 0.3872 Acc: 0.0575
val Loss: 0.3953 Acc: 0.0578

Epoch 162/199
----------
train Loss: 0.3845 Acc: 0.0578
val Loss: 0.3917 Acc: 0.0576

Epoch 163/199
----------
train Loss: 0.3853 Acc: 0.0577
val Loss: 0.3924 Acc: 0.0573

Epoch 164/199
----------
train Loss: 0.3843 Acc: 0.0578
val Loss: 0.3919 Acc: 0.0573

Epoch 165/199
----------
train Loss: 0.3870 Acc: 0.0575
val Loss: 0.3920 Acc: 0.0577

Epoch 166/199
----------
train Loss: 0.3854 Acc: 0.0576
val Loss: 0.3951 Acc: 0.0577

Epoch 167/199
----------
train Loss: 0.3837 Acc: 0.0579
val Loss: 0.3924 Acc: 0.0577

Epoch 168/199
----------
train Loss: 0.3824 Acc: 0.0579
val Loss: 0.3898 Acc: 0.0573

Epoch 169/199
----------
train Loss: 0.3840 Acc: 0.0578
val Loss: 0.3894 Acc: 0.0573

Epoch 170/199
----------
train Loss: 0.3844 Acc: 0.0577
val Loss: 0.3905 Acc: 0.0576

Epoch 171/199
----------
train Loss: 0.3862 Acc: 0.0577
val Loss: 0.3923 Acc: 0.0577

Epoch 172/199
----------
train Loss: 0.3890 Acc: 0.0576
val Loss: 0.3928 Acc: 0.0578

Epoch 173/199
----------
train Loss: 0.3838 Acc: 0.0578
val Loss: 0.3927 Acc: 0.0576

Epoch 174/199
----------
train Loss: 0.3856 Acc: 0.0578
val Loss: 0.3888 Acc: 0.0575

Epoch 175/199
----------
train Loss: 0.3838 Acc: 0.0578
val Loss: 0.3883 Acc: 0.0575

Epoch 176/199
----------
train Loss: 0.3873 Acc: 0.0575
val Loss: 0.3888 Acc: 0.0577

Epoch 177/199
----------
train Loss: 0.3849 Acc: 0.0577
val Loss: 0.3864 Acc: 0.0574

Epoch 178/199
----------
train Loss: 0.3874 Acc: 0.0575
val Loss: 0.3922 Acc: 0.0575

Epoch 179/199
----------
train Loss: 0.3847 Acc: 0.0577
val Loss: 0.3883 Acc: 0.0579

Epoch 180/199
----------
train Loss: 0.3857 Acc: 0.0576
val Loss: 0.3898 Acc: 0.0575

Epoch 181/199
----------
train Loss: 0.3839 Acc: 0.0577
val Loss: 0.3907 Acc: 0.0575

Epoch 182/199
----------
train Loss: 0.3845 Acc: 0.0578
val Loss: 0.3886 Acc: 0.0578

Epoch 183/199
----------
train Loss: 0.3839 Acc: 0.0577
val Loss: 0.3884 Acc: 0.0574

Epoch 184/199
----------
train Loss: 0.3823 Acc: 0.0579
val Loss: 0.3876 Acc: 0.0577

Epoch 185/199
----------
train Loss: 0.3824 Acc: 0.0579
val Loss: 0.3859 Acc: 0.0577

Epoch 186/199
----------
train Loss: 0.3813 Acc: 0.0579
val Loss: 0.3863 Acc: 0.0577

Epoch 187/199
----------
train Loss: 0.3817 Acc: 0.0578
val Loss: 0.3866 Acc: 0.0577

Epoch 188/199
----------
train Loss: 0.3871 Acc: 0.0574
val Loss: 0.3857 Acc: 0.0577

Epoch 189/199
----------
train Loss: 0.3886 Acc: 0.0572
val Loss: 0.3887 Acc: 0.0572

Epoch 190/199
----------
train Loss: 0.3878 Acc: 0.0576
val Loss: 0.3868 Acc: 0.0581

Epoch 191/199
----------
train Loss: 0.3838 Acc: 0.0579
val Loss: 0.3837 Acc: 0.0575

Epoch 192/199
----------
train Loss: 0.3822 Acc: 0.0578
val Loss: 0.3864 Acc: 0.0577

Epoch 193/199
----------
train Loss: 0.3820 Acc: 0.0578
val Loss: 0.3864 Acc: 0.0576

Epoch 194/199
----------
train Loss: 0.3824 Acc: 0.0579
val Loss: 0.3887 Acc: 0.0574

Epoch 195/199
----------
train Loss: 0.3839 Acc: 0.0577
val Loss: 0.3872 Acc: 0.0580

Epoch 196/199
----------
train Loss: 0.3805 Acc: 0.0580
val Loss: 0.3889 Acc: 0.0577

Epoch 197/199
----------
train Loss: 0.3813 Acc: 0.0579
val Loss: 0.3915 Acc: 0.0577

Epoch 198/199
----------
train Loss: 0.3804 Acc: 0.0579
val Loss: 0.3915 Acc: 0.0580

Epoch 199/199
----------
train Loss: 0.3794 Acc: 0.0579
val Loss: 0.3933 Acc: 0.0579

Training complete in 64m 51s
Best val Acc: 0.383715
Saving..
Successfully retrieved statistics for job: zhangz65-gpu03-2908650_. 
+------------------------------------------------------------------------------+
| GPU ID: 2                                                                    |
+====================================+=========================================+
|-----  Execution Stats  ------------+-----------------------------------------|
| Start Time                         | Tue Feb 27 09:04:05 2024                |
| End Time                           | Tue Feb 27 10:10:08 2024                |
| Total Execution Time (sec)         | 3963.53                                 |
| No. of Processes                   | 1                                       |
+-----  Performance Stats  ----------+-----------------------------------------+
| Energy Consumed (Joules)           | 730055                                  |
| Power Usage (Watts)                | Avg: 205.767, Max: 261.349, Min: 46.564 |
| Max GPU Memory Used (bytes)        | 28997320704                             |
| SM Clock (MHz)                     | Avg: 1372, Max: 1380, Min: 1335         |
| Memory Clock (MHz)                 | Avg: 877, Max: 877, Min: 877            |
| SM Utilization (%)                 | Avg: 89, Max: 100, Min: 0               |
| Memory Utilization (%)             | Avg: 46, Max: 79, Min: 0                |
| PCIe Rx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
| PCIe Tx Bandwidth (megabytes)      | Avg: N/A, Max: N/A, Min: N/A            |
+-----  Event Stats  ----------------+-----------------------------------------+
| Single Bit ECC Errors              | 0                                       |
| Double Bit ECC Errors              | 0                                       |
| PCIe Replay Warnings               | 0                                       |
| Critical XID Errors                | 0                                       |
+-----  Slowdown Stats  -------------+-----------------------------------------+
| Due to - Power (%)                 | 0                                       |
|        - Thermal (%)               | 0                                       |
|        - Reliability (%)           | Not Supported                           |
|        - Board Limit (%)           | Not Supported                           |
|        - Low Utilization (%)       | Not Supported                           |
|        - Sync Boost (%)            | 0                                       |
+--  Compute Process Utilization  ---+-----------------------------------------+
| PID                                | 2719717                                 |
|     Avg SM Utilization (%)         | 88                                      |
|     Avg Memory Utilization (%)     | 45                                      |
+-----  Overall Health  -------------+-----------------------------------------+
| Overall Health                     | Healthy                                 |
+------------------------------------+-----------------------------------------+

